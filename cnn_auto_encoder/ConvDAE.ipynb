{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/kevin/dataset/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting /home/kevin/dataset/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/kevin/dataset/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/kevin/dataset/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/home/kevin/dataset/MNIST/', validation_size=0, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1bd249978>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRF+ICjCDwRF+IGg/h+AcVQ40gTKpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1beac6a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[20]\n",
    "plt.imshow(img.reshape([28, 28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name='inputs_')\n",
    "targets_ = tf.placeholder(dtype=tf.float32, shape=[None ,28, 28, 1], name='targets_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "三层卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1\n",
    "conv1 = tf.layers.conv2d(inputs=inputs_, filters=64, kernel_size=(3,3), \n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "\n",
    "# conv2\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=(3,3), \n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "\n",
    "# conv3\n",
    "conv3 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=(3,3),\n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)\n",
    "conv3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(2,2), strides=(2,2), padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv4 = tf.image.resize_nearest_neighbor(images=conv3, size=(7,7))\n",
    "conv4 = tf.layers.conv2d(inputs=conv4, filters=32, kernel_size=(3,3), \n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "conv5 = tf.image.resize_nearest_neighbor(images=conv3, size=(14,14))\n",
    "conv5 = tf.layers.conv2d(inputs=conv5, filters=64, kernel_size=(3,3),\n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "conv6 = tf.image.resize_nearest_neighbor(images=conv5, size=(28,28))\n",
    "conv6 = tf.layers.conv2d(inputs=conv6, filters=64, kernel_size=(3,3),\n",
    "                         strides=(1,1), padding='same', activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logits and ouputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits_ = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), \n",
    "                           strides=(1,1), padding='same', activation=None)\n",
    "\n",
    "outputs_ = tf.nn.sigmoid(logits_, name='outputs_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_, labels=targets_)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20; Batch:1/468; Training loss:0.686069\n",
      "Epoch: 1/20; Batch:2/468; Training loss:0.614573\n",
      "Epoch: 1/20; Batch:3/468; Training loss:0.524235\n",
      "Epoch: 1/20; Batch:4/468; Training loss:0.487001\n",
      "Epoch: 1/20; Batch:5/468; Training loss:0.528825\n",
      "Epoch: 1/20; Batch:6/468; Training loss:0.475946\n",
      "Epoch: 1/20; Batch:7/468; Training loss:0.462307\n",
      "Epoch: 1/20; Batch:8/468; Training loss:0.457507\n",
      "Epoch: 1/20; Batch:9/468; Training loss:0.441077\n",
      "Epoch: 1/20; Batch:10/468; Training loss:0.423748\n",
      "Epoch: 1/20; Batch:11/468; Training loss:0.425047\n",
      "Epoch: 1/20; Batch:12/468; Training loss:0.404334\n",
      "Epoch: 1/20; Batch:13/468; Training loss:0.38443\n",
      "Epoch: 1/20; Batch:14/468; Training loss:0.373014\n",
      "Epoch: 1/20; Batch:15/468; Training loss:0.358164\n",
      "Epoch: 1/20; Batch:16/468; Training loss:0.346013\n",
      "Epoch: 1/20; Batch:17/468; Training loss:0.32784\n",
      "Epoch: 1/20; Batch:18/468; Training loss:0.303415\n",
      "Epoch: 1/20; Batch:19/468; Training loss:0.296547\n",
      "Epoch: 1/20; Batch:20/468; Training loss:0.291313\n",
      "Epoch: 1/20; Batch:21/468; Training loss:0.278131\n",
      "Epoch: 1/20; Batch:22/468; Training loss:0.276839\n",
      "Epoch: 1/20; Batch:23/468; Training loss:0.27567\n",
      "Epoch: 1/20; Batch:24/468; Training loss:0.279247\n",
      "Epoch: 1/20; Batch:25/468; Training loss:0.280876\n",
      "Epoch: 1/20; Batch:26/468; Training loss:0.281094\n",
      "Epoch: 1/20; Batch:27/468; Training loss:0.267397\n",
      "Epoch: 1/20; Batch:28/468; Training loss:0.269635\n",
      "Epoch: 1/20; Batch:29/468; Training loss:0.25537\n",
      "Epoch: 1/20; Batch:30/468; Training loss:0.257886\n",
      "Epoch: 1/20; Batch:31/468; Training loss:0.258228\n",
      "Epoch: 1/20; Batch:32/468; Training loss:0.249448\n",
      "Epoch: 1/20; Batch:33/468; Training loss:0.251013\n",
      "Epoch: 1/20; Batch:34/468; Training loss:0.251821\n",
      "Epoch: 1/20; Batch:35/468; Training loss:0.246177\n",
      "Epoch: 1/20; Batch:36/468; Training loss:0.256668\n",
      "Epoch: 1/20; Batch:37/468; Training loss:0.243331\n",
      "Epoch: 1/20; Batch:38/468; Training loss:0.255773\n",
      "Epoch: 1/20; Batch:39/468; Training loss:0.249913\n",
      "Epoch: 1/20; Batch:40/468; Training loss:0.239936\n",
      "Epoch: 1/20; Batch:41/468; Training loss:0.249864\n",
      "Epoch: 1/20; Batch:42/468; Training loss:0.240772\n",
      "Epoch: 1/20; Batch:43/468; Training loss:0.255092\n",
      "Epoch: 1/20; Batch:44/468; Training loss:0.229147\n",
      "Epoch: 1/20; Batch:45/468; Training loss:0.253497\n",
      "Epoch: 1/20; Batch:46/468; Training loss:0.230259\n",
      "Epoch: 1/20; Batch:47/468; Training loss:0.253975\n",
      "Epoch: 1/20; Batch:48/468; Training loss:0.233431\n",
      "Epoch: 1/20; Batch:49/468; Training loss:0.255231\n",
      "Epoch: 1/20; Batch:50/468; Training loss:0.230574\n",
      "Epoch: 1/20; Batch:51/468; Training loss:0.233549\n",
      "Epoch: 1/20; Batch:52/468; Training loss:0.232364\n",
      "Epoch: 1/20; Batch:53/468; Training loss:0.232737\n",
      "Epoch: 1/20; Batch:54/468; Training loss:0.239543\n",
      "Epoch: 1/20; Batch:55/468; Training loss:0.23008\n",
      "Epoch: 1/20; Batch:56/468; Training loss:0.241935\n",
      "Epoch: 1/20; Batch:57/468; Training loss:0.228167\n",
      "Epoch: 1/20; Batch:58/468; Training loss:0.229637\n",
      "Epoch: 1/20; Batch:59/468; Training loss:0.228465\n",
      "Epoch: 1/20; Batch:60/468; Training loss:0.228237\n",
      "Epoch: 1/20; Batch:61/468; Training loss:0.228334\n",
      "Epoch: 1/20; Batch:62/468; Training loss:0.231095\n",
      "Epoch: 1/20; Batch:63/468; Training loss:0.229831\n",
      "Epoch: 1/20; Batch:64/468; Training loss:0.233215\n",
      "Epoch: 1/20; Batch:65/468; Training loss:0.227295\n",
      "Epoch: 1/20; Batch:66/468; Training loss:0.221157\n",
      "Epoch: 1/20; Batch:67/468; Training loss:0.217872\n",
      "Epoch: 1/20; Batch:68/468; Training loss:0.228802\n",
      "Epoch: 1/20; Batch:69/468; Training loss:0.222515\n",
      "Epoch: 1/20; Batch:70/468; Training loss:0.219384\n",
      "Epoch: 1/20; Batch:71/468; Training loss:0.220282\n",
      "Epoch: 1/20; Batch:72/468; Training loss:0.216733\n",
      "Epoch: 1/20; Batch:73/468; Training loss:0.211935\n",
      "Epoch: 1/20; Batch:74/468; Training loss:0.215646\n",
      "Epoch: 1/20; Batch:75/468; Training loss:0.212994\n",
      "Epoch: 1/20; Batch:76/468; Training loss:0.211806\n",
      "Epoch: 1/20; Batch:77/468; Training loss:0.215977\n",
      "Epoch: 1/20; Batch:78/468; Training loss:0.214175\n",
      "Epoch: 1/20; Batch:79/468; Training loss:0.20839\n",
      "Epoch: 1/20; Batch:80/468; Training loss:0.214799\n",
      "Epoch: 1/20; Batch:81/468; Training loss:0.210435\n",
      "Epoch: 1/20; Batch:82/468; Training loss:0.211887\n",
      "Epoch: 1/20; Batch:83/468; Training loss:0.214518\n",
      "Epoch: 1/20; Batch:84/468; Training loss:0.208316\n",
      "Epoch: 1/20; Batch:85/468; Training loss:0.209979\n",
      "Epoch: 1/20; Batch:86/468; Training loss:0.21294\n",
      "Epoch: 1/20; Batch:87/468; Training loss:0.206263\n",
      "Epoch: 1/20; Batch:88/468; Training loss:0.209682\n",
      "Epoch: 1/20; Batch:89/468; Training loss:0.215461\n",
      "Epoch: 1/20; Batch:90/468; Training loss:0.199412\n",
      "Epoch: 1/20; Batch:91/468; Training loss:0.197986\n",
      "Epoch: 1/20; Batch:92/468; Training loss:0.203303\n",
      "Epoch: 1/20; Batch:93/468; Training loss:0.200251\n",
      "Epoch: 1/20; Batch:94/468; Training loss:0.197959\n",
      "Epoch: 1/20; Batch:95/468; Training loss:0.199935\n",
      "Epoch: 1/20; Batch:96/468; Training loss:0.201655\n",
      "Epoch: 1/20; Batch:97/468; Training loss:0.193944\n",
      "Epoch: 1/20; Batch:98/468; Training loss:0.193577\n",
      "Epoch: 1/20; Batch:99/468; Training loss:0.205257\n",
      "Epoch: 1/20; Batch:100/468; Training loss:0.194732\n",
      "Epoch: 1/20; Batch:101/468; Training loss:0.208415\n",
      "Epoch: 1/20; Batch:102/468; Training loss:0.206403\n",
      "Epoch: 1/20; Batch:103/468; Training loss:0.203105\n",
      "Epoch: 1/20; Batch:104/468; Training loss:0.200727\n",
      "Epoch: 1/20; Batch:105/468; Training loss:0.198875\n",
      "Epoch: 1/20; Batch:106/468; Training loss:0.199211\n",
      "Epoch: 1/20; Batch:107/468; Training loss:0.197467\n",
      "Epoch: 1/20; Batch:108/468; Training loss:0.192584\n",
      "Epoch: 1/20; Batch:109/468; Training loss:0.196343\n",
      "Epoch: 1/20; Batch:110/468; Training loss:0.200137\n",
      "Epoch: 1/20; Batch:111/468; Training loss:0.194888\n",
      "Epoch: 1/20; Batch:112/468; Training loss:0.189272\n",
      "Epoch: 1/20; Batch:113/468; Training loss:0.195792\n",
      "Epoch: 1/20; Batch:114/468; Training loss:0.190523\n",
      "Epoch: 1/20; Batch:115/468; Training loss:0.192536\n",
      "Epoch: 1/20; Batch:116/468; Training loss:0.193853\n",
      "Epoch: 1/20; Batch:117/468; Training loss:0.193928\n",
      "Epoch: 1/20; Batch:118/468; Training loss:0.198357\n",
      "Epoch: 1/20; Batch:119/468; Training loss:0.196187\n",
      "Epoch: 1/20; Batch:120/468; Training loss:0.194783\n",
      "Epoch: 1/20; Batch:121/468; Training loss:0.191753\n",
      "Epoch: 1/20; Batch:122/468; Training loss:0.19241\n",
      "Epoch: 1/20; Batch:123/468; Training loss:0.197633\n",
      "Epoch: 1/20; Batch:124/468; Training loss:0.196082\n",
      "Epoch: 1/20; Batch:125/468; Training loss:0.189752\n",
      "Epoch: 1/20; Batch:126/468; Training loss:0.188537\n",
      "Epoch: 1/20; Batch:127/468; Training loss:0.188328\n",
      "Epoch: 1/20; Batch:128/468; Training loss:0.186235\n",
      "Epoch: 1/20; Batch:129/468; Training loss:0.192113\n",
      "Epoch: 1/20; Batch:130/468; Training loss:0.184381\n",
      "Epoch: 1/20; Batch:131/468; Training loss:0.181245\n",
      "Epoch: 1/20; Batch:132/468; Training loss:0.1907\n",
      "Epoch: 1/20; Batch:133/468; Training loss:0.185234\n",
      "Epoch: 1/20; Batch:134/468; Training loss:0.189068\n",
      "Epoch: 1/20; Batch:135/468; Training loss:0.182202\n",
      "Epoch: 1/20; Batch:136/468; Training loss:0.187314\n",
      "Epoch: 1/20; Batch:137/468; Training loss:0.187186\n",
      "Epoch: 1/20; Batch:138/468; Training loss:0.180073\n",
      "Epoch: 1/20; Batch:139/468; Training loss:0.189314\n",
      "Epoch: 1/20; Batch:140/468; Training loss:0.179082\n",
      "Epoch: 1/20; Batch:141/468; Training loss:0.185582\n",
      "Epoch: 1/20; Batch:142/468; Training loss:0.184611\n",
      "Epoch: 1/20; Batch:143/468; Training loss:0.184659\n",
      "Epoch: 1/20; Batch:144/468; Training loss:0.180294\n",
      "Epoch: 1/20; Batch:145/468; Training loss:0.187793\n",
      "Epoch: 1/20; Batch:146/468; Training loss:0.183842\n",
      "Epoch: 1/20; Batch:147/468; Training loss:0.188512\n",
      "Epoch: 1/20; Batch:148/468; Training loss:0.180768\n",
      "Epoch: 1/20; Batch:149/468; Training loss:0.185801\n",
      "Epoch: 1/20; Batch:150/468; Training loss:0.179925\n",
      "Epoch: 1/20; Batch:151/468; Training loss:0.183789\n",
      "Epoch: 1/20; Batch:152/468; Training loss:0.185444\n",
      "Epoch: 1/20; Batch:153/468; Training loss:0.183041\n",
      "Epoch: 1/20; Batch:154/468; Training loss:0.180659\n",
      "Epoch: 1/20; Batch:155/468; Training loss:0.187943\n",
      "Epoch: 1/20; Batch:156/468; Training loss:0.186154\n",
      "Epoch: 1/20; Batch:157/468; Training loss:0.183911\n",
      "Epoch: 1/20; Batch:158/468; Training loss:0.178605\n",
      "Epoch: 1/20; Batch:159/468; Training loss:0.178468\n",
      "Epoch: 1/20; Batch:160/468; Training loss:0.17969\n",
      "Epoch: 1/20; Batch:161/468; Training loss:0.179685\n",
      "Epoch: 1/20; Batch:162/468; Training loss:0.18079\n",
      "Epoch: 1/20; Batch:163/468; Training loss:0.18086\n",
      "Epoch: 1/20; Batch:164/468; Training loss:0.177267\n",
      "Epoch: 1/20; Batch:165/468; Training loss:0.181471\n",
      "Epoch: 1/20; Batch:166/468; Training loss:0.180549\n",
      "Epoch: 1/20; Batch:167/468; Training loss:0.179259\n",
      "Epoch: 1/20; Batch:168/468; Training loss:0.178912\n",
      "Epoch: 1/20; Batch:169/468; Training loss:0.178368\n",
      "Epoch: 1/20; Batch:170/468; Training loss:0.184674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20; Batch:171/468; Training loss:0.178974\n",
      "Epoch: 1/20; Batch:172/468; Training loss:0.186155\n",
      "Epoch: 1/20; Batch:173/468; Training loss:0.181398\n",
      "Epoch: 1/20; Batch:174/468; Training loss:0.173418\n",
      "Epoch: 1/20; Batch:175/468; Training loss:0.169563\n",
      "Epoch: 1/20; Batch:176/468; Training loss:0.180724\n",
      "Epoch: 1/20; Batch:177/468; Training loss:0.186191\n",
      "Epoch: 1/20; Batch:178/468; Training loss:0.178929\n",
      "Epoch: 1/20; Batch:179/468; Training loss:0.177625\n",
      "Epoch: 1/20; Batch:180/468; Training loss:0.194045\n",
      "Epoch: 1/20; Batch:181/468; Training loss:0.181352\n",
      "Epoch: 1/20; Batch:182/468; Training loss:0.173751\n",
      "Epoch: 1/20; Batch:183/468; Training loss:0.177099\n",
      "Epoch: 1/20; Batch:184/468; Training loss:0.178904\n",
      "Epoch: 1/20; Batch:185/468; Training loss:0.185373\n",
      "Epoch: 1/20; Batch:186/468; Training loss:0.179678\n",
      "Epoch: 1/20; Batch:187/468; Training loss:0.178515\n",
      "Epoch: 1/20; Batch:188/468; Training loss:0.177773\n",
      "Epoch: 1/20; Batch:189/468; Training loss:0.180043\n",
      "Epoch: 1/20; Batch:190/468; Training loss:0.182086\n",
      "Epoch: 1/20; Batch:191/468; Training loss:0.173245\n",
      "Epoch: 1/20; Batch:192/468; Training loss:0.173444\n",
      "Epoch: 1/20; Batch:193/468; Training loss:0.178979\n",
      "Epoch: 1/20; Batch:194/468; Training loss:0.181276\n",
      "Epoch: 1/20; Batch:195/468; Training loss:0.172268\n",
      "Epoch: 1/20; Batch:196/468; Training loss:0.173969\n",
      "Epoch: 1/20; Batch:197/468; Training loss:0.167002\n",
      "Epoch: 1/20; Batch:198/468; Training loss:0.170472\n",
      "Epoch: 1/20; Batch:199/468; Training loss:0.176828\n",
      "Epoch: 1/20; Batch:200/468; Training loss:0.17116\n",
      "Epoch: 1/20; Batch:201/468; Training loss:0.169774\n",
      "Epoch: 1/20; Batch:202/468; Training loss:0.169151\n",
      "Epoch: 1/20; Batch:203/468; Training loss:0.177993\n",
      "Epoch: 1/20; Batch:204/468; Training loss:0.168825\n",
      "Epoch: 1/20; Batch:205/468; Training loss:0.167376\n",
      "Epoch: 1/20; Batch:206/468; Training loss:0.167869\n",
      "Epoch: 1/20; Batch:207/468; Training loss:0.164063\n",
      "Epoch: 1/20; Batch:208/468; Training loss:0.171281\n",
      "Epoch: 1/20; Batch:209/468; Training loss:0.166417\n",
      "Epoch: 1/20; Batch:210/468; Training loss:0.166955\n",
      "Epoch: 1/20; Batch:211/468; Training loss:0.173939\n",
      "Epoch: 1/20; Batch:212/468; Training loss:0.170954\n",
      "Epoch: 1/20; Batch:213/468; Training loss:0.165623\n",
      "Epoch: 1/20; Batch:214/468; Training loss:0.173242\n",
      "Epoch: 1/20; Batch:215/468; Training loss:0.170266\n",
      "Epoch: 1/20; Batch:216/468; Training loss:0.169387\n",
      "Epoch: 1/20; Batch:217/468; Training loss:0.16572\n",
      "Epoch: 1/20; Batch:218/468; Training loss:0.168065\n",
      "Epoch: 1/20; Batch:219/468; Training loss:0.167623\n",
      "Epoch: 1/20; Batch:220/468; Training loss:0.168858\n",
      "Epoch: 1/20; Batch:221/468; Training loss:0.167001\n",
      "Epoch: 1/20; Batch:222/468; Training loss:0.167109\n",
      "Epoch: 1/20; Batch:223/468; Training loss:0.164712\n",
      "Epoch: 1/20; Batch:224/468; Training loss:0.166787\n",
      "Epoch: 1/20; Batch:225/468; Training loss:0.166024\n",
      "Epoch: 1/20; Batch:226/468; Training loss:0.159954\n",
      "Epoch: 1/20; Batch:227/468; Training loss:0.159671\n",
      "Epoch: 1/20; Batch:228/468; Training loss:0.160945\n",
      "Epoch: 1/20; Batch:229/468; Training loss:0.163397\n",
      "Epoch: 1/20; Batch:230/468; Training loss:0.168082\n",
      "Epoch: 1/20; Batch:231/468; Training loss:0.171508\n",
      "Epoch: 1/20; Batch:232/468; Training loss:0.171656\n",
      "Epoch: 1/20; Batch:233/468; Training loss:0.170161\n",
      "Epoch: 1/20; Batch:234/468; Training loss:0.164616\n",
      "Epoch: 1/20; Batch:235/468; Training loss:0.16637\n",
      "Epoch: 1/20; Batch:236/468; Training loss:0.161876\n",
      "Epoch: 1/20; Batch:237/468; Training loss:0.170236\n",
      "Epoch: 1/20; Batch:238/468; Training loss:0.16078\n",
      "Epoch: 1/20; Batch:239/468; Training loss:0.165913\n",
      "Epoch: 1/20; Batch:240/468; Training loss:0.163221\n",
      "Epoch: 1/20; Batch:241/468; Training loss:0.157482\n",
      "Epoch: 1/20; Batch:242/468; Training loss:0.16479\n",
      "Epoch: 1/20; Batch:243/468; Training loss:0.163488\n",
      "Epoch: 1/20; Batch:244/468; Training loss:0.167838\n",
      "Epoch: 1/20; Batch:245/468; Training loss:0.165001\n",
      "Epoch: 1/20; Batch:246/468; Training loss:0.16034\n",
      "Epoch: 1/20; Batch:247/468; Training loss:0.159324\n",
      "Epoch: 1/20; Batch:248/468; Training loss:0.162636\n",
      "Epoch: 1/20; Batch:249/468; Training loss:0.17038\n",
      "Epoch: 1/20; Batch:250/468; Training loss:0.170202\n",
      "Epoch: 1/20; Batch:251/468; Training loss:0.163888\n",
      "Epoch: 1/20; Batch:252/468; Training loss:0.162997\n",
      "Epoch: 1/20; Batch:253/468; Training loss:0.160844\n",
      "Epoch: 1/20; Batch:254/468; Training loss:0.164378\n",
      "Epoch: 1/20; Batch:255/468; Training loss:0.169465\n",
      "Epoch: 1/20; Batch:256/468; Training loss:0.16616\n",
      "Epoch: 1/20; Batch:257/468; Training loss:0.163215\n",
      "Epoch: 1/20; Batch:258/468; Training loss:0.164234\n",
      "Epoch: 1/20; Batch:259/468; Training loss:0.16045\n",
      "Epoch: 1/20; Batch:260/468; Training loss:0.158364\n",
      "Epoch: 1/20; Batch:261/468; Training loss:0.158849\n",
      "Epoch: 1/20; Batch:262/468; Training loss:0.163414\n",
      "Epoch: 1/20; Batch:263/468; Training loss:0.159263\n",
      "Epoch: 1/20; Batch:264/468; Training loss:0.164471\n",
      "Epoch: 1/20; Batch:265/468; Training loss:0.15708\n",
      "Epoch: 1/20; Batch:266/468; Training loss:0.161324\n",
      "Epoch: 1/20; Batch:267/468; Training loss:0.151184\n",
      "Epoch: 1/20; Batch:268/468; Training loss:0.165641\n",
      "Epoch: 1/20; Batch:269/468; Training loss:0.161586\n",
      "Epoch: 1/20; Batch:270/468; Training loss:0.164573\n",
      "Epoch: 1/20; Batch:271/468; Training loss:0.163876\n",
      "Epoch: 1/20; Batch:272/468; Training loss:0.159477\n",
      "Epoch: 1/20; Batch:273/468; Training loss:0.163279\n",
      "Epoch: 1/20; Batch:274/468; Training loss:0.164791\n",
      "Epoch: 1/20; Batch:275/468; Training loss:0.160608\n",
      "Epoch: 1/20; Batch:276/468; Training loss:0.158559\n",
      "Epoch: 1/20; Batch:277/468; Training loss:0.154069\n",
      "Epoch: 1/20; Batch:278/468; Training loss:0.159277\n",
      "Epoch: 1/20; Batch:279/468; Training loss:0.162591\n",
      "Epoch: 1/20; Batch:280/468; Training loss:0.168173\n",
      "Epoch: 1/20; Batch:281/468; Training loss:0.153053\n",
      "Epoch: 1/20; Batch:282/468; Training loss:0.161282\n",
      "Epoch: 1/20; Batch:283/468; Training loss:0.159953\n",
      "Epoch: 1/20; Batch:284/468; Training loss:0.160265\n",
      "Epoch: 1/20; Batch:285/468; Training loss:0.154913\n",
      "Epoch: 1/20; Batch:286/468; Training loss:0.150761\n",
      "Epoch: 1/20; Batch:287/468; Training loss:0.159044\n",
      "Epoch: 1/20; Batch:288/468; Training loss:0.162985\n",
      "Epoch: 1/20; Batch:289/468; Training loss:0.155055\n",
      "Epoch: 1/20; Batch:290/468; Training loss:0.156014\n",
      "Epoch: 1/20; Batch:291/468; Training loss:0.158798\n",
      "Epoch: 1/20; Batch:292/468; Training loss:0.154263\n",
      "Epoch: 1/20; Batch:293/468; Training loss:0.155425\n",
      "Epoch: 1/20; Batch:294/468; Training loss:0.15664\n",
      "Epoch: 1/20; Batch:295/468; Training loss:0.14822\n",
      "Epoch: 1/20; Batch:296/468; Training loss:0.150298\n",
      "Epoch: 1/20; Batch:297/468; Training loss:0.158513\n",
      "Epoch: 1/20; Batch:298/468; Training loss:0.160232\n",
      "Epoch: 1/20; Batch:299/468; Training loss:0.149418\n",
      "Epoch: 1/20; Batch:300/468; Training loss:0.15292\n",
      "Epoch: 1/20; Batch:301/468; Training loss:0.151678\n",
      "Epoch: 1/20; Batch:302/468; Training loss:0.154848\n",
      "Epoch: 1/20; Batch:303/468; Training loss:0.157427\n",
      "Epoch: 1/20; Batch:304/468; Training loss:0.155996\n",
      "Epoch: 1/20; Batch:305/468; Training loss:0.149815\n",
      "Epoch: 1/20; Batch:306/468; Training loss:0.149822\n",
      "Epoch: 1/20; Batch:307/468; Training loss:0.150548\n",
      "Epoch: 1/20; Batch:308/468; Training loss:0.154873\n",
      "Epoch: 1/20; Batch:309/468; Training loss:0.149442\n",
      "Epoch: 1/20; Batch:310/468; Training loss:0.154098\n",
      "Epoch: 1/20; Batch:311/468; Training loss:0.150577\n",
      "Epoch: 1/20; Batch:312/468; Training loss:0.153988\n",
      "Epoch: 1/20; Batch:313/468; Training loss:0.154591\n",
      "Epoch: 1/20; Batch:314/468; Training loss:0.149518\n",
      "Epoch: 1/20; Batch:315/468; Training loss:0.145229\n",
      "Epoch: 1/20; Batch:316/468; Training loss:0.151691\n",
      "Epoch: 1/20; Batch:317/468; Training loss:0.154607\n",
      "Epoch: 1/20; Batch:318/468; Training loss:0.150319\n",
      "Epoch: 1/20; Batch:319/468; Training loss:0.150005\n",
      "Epoch: 1/20; Batch:320/468; Training loss:0.148209\n",
      "Epoch: 1/20; Batch:321/468; Training loss:0.159825\n",
      "Epoch: 1/20; Batch:322/468; Training loss:0.150719\n",
      "Epoch: 1/20; Batch:323/468; Training loss:0.149295\n",
      "Epoch: 1/20; Batch:324/468; Training loss:0.151865\n",
      "Epoch: 1/20; Batch:325/468; Training loss:0.148549\n",
      "Epoch: 1/20; Batch:326/468; Training loss:0.151336\n",
      "Epoch: 1/20; Batch:327/468; Training loss:0.152302\n",
      "Epoch: 1/20; Batch:328/468; Training loss:0.146004\n",
      "Epoch: 1/20; Batch:329/468; Training loss:0.151584\n",
      "Epoch: 1/20; Batch:330/468; Training loss:0.150475\n",
      "Epoch: 1/20; Batch:331/468; Training loss:0.150949\n",
      "Epoch: 1/20; Batch:332/468; Training loss:0.146647\n",
      "Epoch: 1/20; Batch:333/468; Training loss:0.146432\n",
      "Epoch: 1/20; Batch:334/468; Training loss:0.1535\n",
      "Epoch: 1/20; Batch:335/468; Training loss:0.153968\n",
      "Epoch: 1/20; Batch:336/468; Training loss:0.15502\n",
      "Epoch: 1/20; Batch:337/468; Training loss:0.146601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20; Batch:338/468; Training loss:0.153629\n",
      "Epoch: 1/20; Batch:339/468; Training loss:0.148994\n",
      "Epoch: 1/20; Batch:340/468; Training loss:0.145953\n",
      "Epoch: 1/20; Batch:341/468; Training loss:0.14767\n",
      "Epoch: 1/20; Batch:342/468; Training loss:0.150812\n",
      "Epoch: 1/20; Batch:343/468; Training loss:0.149608\n",
      "Epoch: 1/20; Batch:344/468; Training loss:0.150564\n",
      "Epoch: 1/20; Batch:345/468; Training loss:0.152097\n",
      "Epoch: 1/20; Batch:346/468; Training loss:0.149925\n",
      "Epoch: 1/20; Batch:347/468; Training loss:0.145752\n",
      "Epoch: 1/20; Batch:348/468; Training loss:0.142009\n",
      "Epoch: 1/20; Batch:349/468; Training loss:0.151864\n",
      "Epoch: 1/20; Batch:350/468; Training loss:0.152334\n",
      "Epoch: 1/20; Batch:351/468; Training loss:0.145974\n",
      "Epoch: 1/20; Batch:352/468; Training loss:0.148976\n",
      "Epoch: 1/20; Batch:353/468; Training loss:0.150641\n",
      "Epoch: 1/20; Batch:354/468; Training loss:0.14381\n",
      "Epoch: 1/20; Batch:355/468; Training loss:0.148304\n",
      "Epoch: 1/20; Batch:356/468; Training loss:0.154034\n",
      "Epoch: 1/20; Batch:357/468; Training loss:0.151915\n",
      "Epoch: 1/20; Batch:358/468; Training loss:0.141743\n",
      "Epoch: 1/20; Batch:359/468; Training loss:0.141226\n",
      "Epoch: 1/20; Batch:360/468; Training loss:0.153454\n",
      "Epoch: 1/20; Batch:361/468; Training loss:0.14699\n",
      "Epoch: 1/20; Batch:362/468; Training loss:0.147589\n",
      "Epoch: 1/20; Batch:363/468; Training loss:0.144463\n",
      "Epoch: 1/20; Batch:364/468; Training loss:0.145082\n",
      "Epoch: 1/20; Batch:365/468; Training loss:0.149396\n",
      "Epoch: 1/20; Batch:366/468; Training loss:0.137915\n",
      "Epoch: 1/20; Batch:367/468; Training loss:0.149383\n",
      "Epoch: 1/20; Batch:368/468; Training loss:0.148441\n",
      "Epoch: 1/20; Batch:369/468; Training loss:0.14375\n",
      "Epoch: 1/20; Batch:370/468; Training loss:0.151056\n",
      "Epoch: 1/20; Batch:371/468; Training loss:0.148316\n",
      "Epoch: 1/20; Batch:372/468; Training loss:0.148182\n",
      "Epoch: 1/20; Batch:373/468; Training loss:0.148573\n",
      "Epoch: 1/20; Batch:374/468; Training loss:0.144246\n",
      "Epoch: 1/20; Batch:375/468; Training loss:0.150431\n",
      "Epoch: 1/20; Batch:376/468; Training loss:0.14522\n",
      "Epoch: 1/20; Batch:377/468; Training loss:0.151757\n",
      "Epoch: 1/20; Batch:378/468; Training loss:0.14886\n",
      "Epoch: 1/20; Batch:379/468; Training loss:0.143906\n",
      "Epoch: 1/20; Batch:380/468; Training loss:0.148664\n",
      "Epoch: 1/20; Batch:381/468; Training loss:0.148891\n",
      "Epoch: 1/20; Batch:382/468; Training loss:0.145393\n",
      "Epoch: 1/20; Batch:383/468; Training loss:0.145528\n",
      "Epoch: 1/20; Batch:384/468; Training loss:0.144291\n",
      "Epoch: 1/20; Batch:385/468; Training loss:0.142711\n",
      "Epoch: 1/20; Batch:386/468; Training loss:0.144693\n",
      "Epoch: 1/20; Batch:387/468; Training loss:0.143472\n",
      "Epoch: 1/20; Batch:388/468; Training loss:0.145457\n",
      "Epoch: 1/20; Batch:389/468; Training loss:0.142228\n",
      "Epoch: 1/20; Batch:390/468; Training loss:0.151153\n",
      "Epoch: 1/20; Batch:391/468; Training loss:0.15155\n",
      "Epoch: 1/20; Batch:392/468; Training loss:0.14509\n",
      "Epoch: 1/20; Batch:393/468; Training loss:0.141584\n",
      "Epoch: 1/20; Batch:394/468; Training loss:0.145502\n",
      "Epoch: 1/20; Batch:395/468; Training loss:0.1385\n",
      "Epoch: 1/20; Batch:396/468; Training loss:0.144824\n",
      "Epoch: 1/20; Batch:397/468; Training loss:0.144319\n",
      "Epoch: 1/20; Batch:398/468; Training loss:0.14808\n",
      "Epoch: 1/20; Batch:399/468; Training loss:0.142036\n",
      "Epoch: 1/20; Batch:400/468; Training loss:0.13673\n",
      "Epoch: 1/20; Batch:401/468; Training loss:0.147551\n",
      "Epoch: 1/20; Batch:402/468; Training loss:0.148956\n",
      "Epoch: 1/20; Batch:403/468; Training loss:0.143492\n",
      "Epoch: 1/20; Batch:404/468; Training loss:0.140159\n",
      "Epoch: 1/20; Batch:405/468; Training loss:0.14476\n",
      "Epoch: 1/20; Batch:406/468; Training loss:0.141254\n",
      "Epoch: 1/20; Batch:407/468; Training loss:0.147695\n",
      "Epoch: 1/20; Batch:408/468; Training loss:0.145019\n",
      "Epoch: 1/20; Batch:409/468; Training loss:0.141804\n",
      "Epoch: 1/20; Batch:410/468; Training loss:0.13182\n",
      "Epoch: 1/20; Batch:411/468; Training loss:0.146587\n",
      "Epoch: 1/20; Batch:412/468; Training loss:0.142452\n",
      "Epoch: 1/20; Batch:413/468; Training loss:0.144417\n",
      "Epoch: 1/20; Batch:414/468; Training loss:0.141353\n",
      "Epoch: 1/20; Batch:415/468; Training loss:0.147374\n",
      "Epoch: 1/20; Batch:416/468; Training loss:0.149555\n",
      "Epoch: 1/20; Batch:417/468; Training loss:0.135717\n",
      "Epoch: 1/20; Batch:418/468; Training loss:0.144969\n",
      "Epoch: 1/20; Batch:419/468; Training loss:0.149048\n",
      "Epoch: 1/20; Batch:420/468; Training loss:0.149201\n",
      "Epoch: 1/20; Batch:421/468; Training loss:0.147705\n",
      "Epoch: 1/20; Batch:422/468; Training loss:0.139279\n",
      "Epoch: 1/20; Batch:423/468; Training loss:0.144222\n",
      "Epoch: 1/20; Batch:424/468; Training loss:0.143692\n",
      "Epoch: 1/20; Batch:425/468; Training loss:0.142348\n",
      "Epoch: 1/20; Batch:426/468; Training loss:0.143918\n",
      "Epoch: 1/20; Batch:427/468; Training loss:0.140544\n",
      "Epoch: 1/20; Batch:428/468; Training loss:0.145208\n",
      "Epoch: 1/20; Batch:429/468; Training loss:0.135069\n",
      "Epoch: 1/20; Batch:430/468; Training loss:0.15205\n",
      "Epoch: 1/20; Batch:431/468; Training loss:0.144715\n",
      "Epoch: 1/20; Batch:432/468; Training loss:0.139993\n",
      "Epoch: 1/20; Batch:433/468; Training loss:0.139433\n",
      "Epoch: 1/20; Batch:434/468; Training loss:0.137646\n",
      "Epoch: 1/20; Batch:435/468; Training loss:0.141689\n",
      "Epoch: 1/20; Batch:436/468; Training loss:0.147014\n",
      "Epoch: 1/20; Batch:437/468; Training loss:0.139465\n",
      "Epoch: 1/20; Batch:438/468; Training loss:0.133983\n",
      "Epoch: 1/20; Batch:439/468; Training loss:0.135829\n",
      "Epoch: 1/20; Batch:440/468; Training loss:0.136789\n",
      "Epoch: 1/20; Batch:441/468; Training loss:0.138989\n",
      "Epoch: 1/20; Batch:442/468; Training loss:0.142287\n",
      "Epoch: 1/20; Batch:443/468; Training loss:0.137432\n",
      "Epoch: 1/20; Batch:444/468; Training loss:0.143237\n",
      "Epoch: 1/20; Batch:445/468; Training loss:0.133404\n",
      "Epoch: 1/20; Batch:446/468; Training loss:0.138058\n",
      "Epoch: 1/20; Batch:447/468; Training loss:0.140778\n",
      "Epoch: 1/20; Batch:448/468; Training loss:0.140992\n",
      "Epoch: 1/20; Batch:449/468; Training loss:0.142306\n",
      "Epoch: 1/20; Batch:450/468; Training loss:0.145851\n",
      "Epoch: 1/20; Batch:451/468; Training loss:0.13408\n",
      "Epoch: 1/20; Batch:452/468; Training loss:0.14052\n",
      "Epoch: 1/20; Batch:453/468; Training loss:0.1377\n",
      "Epoch: 1/20; Batch:454/468; Training loss:0.135841\n",
      "Epoch: 1/20; Batch:455/468; Training loss:0.139351\n",
      "Epoch: 1/20; Batch:456/468; Training loss:0.14076\n",
      "Epoch: 1/20; Batch:457/468; Training loss:0.142326\n",
      "Epoch: 1/20; Batch:458/468; Training loss:0.138878\n",
      "Epoch: 1/20; Batch:459/468; Training loss:0.135037\n",
      "Epoch: 1/20; Batch:460/468; Training loss:0.14395\n",
      "Epoch: 1/20; Batch:461/468; Training loss:0.141599\n",
      "Epoch: 1/20; Batch:462/468; Training loss:0.130046\n",
      "Epoch: 1/20; Batch:463/468; Training loss:0.14112\n",
      "Epoch: 1/20; Batch:464/468; Training loss:0.138085\n",
      "Epoch: 1/20; Batch:465/468; Training loss:0.134757\n",
      "Epoch: 1/20; Batch:466/468; Training loss:0.141381\n",
      "Epoch: 1/20; Batch:467/468; Training loss:0.137479\n",
      "Epoch: 1/20; Batch:468/468; Training loss:0.135216\n",
      "Epoch: 2/20; Batch:1/468; Training loss:0.134931\n",
      "Epoch: 2/20; Batch:2/468; Training loss:0.136909\n",
      "Epoch: 2/20; Batch:3/468; Training loss:0.138912\n",
      "Epoch: 2/20; Batch:4/468; Training loss:0.135232\n",
      "Epoch: 2/20; Batch:5/468; Training loss:0.135308\n",
      "Epoch: 2/20; Batch:6/468; Training loss:0.142186\n",
      "Epoch: 2/20; Batch:7/468; Training loss:0.139389\n",
      "Epoch: 2/20; Batch:8/468; Training loss:0.137355\n",
      "Epoch: 2/20; Batch:9/468; Training loss:0.137945\n",
      "Epoch: 2/20; Batch:10/468; Training loss:0.136988\n",
      "Epoch: 2/20; Batch:11/468; Training loss:0.133808\n",
      "Epoch: 2/20; Batch:12/468; Training loss:0.138226\n",
      "Epoch: 2/20; Batch:13/468; Training loss:0.136812\n",
      "Epoch: 2/20; Batch:14/468; Training loss:0.136475\n",
      "Epoch: 2/20; Batch:15/468; Training loss:0.137149\n",
      "Epoch: 2/20; Batch:16/468; Training loss:0.139654\n",
      "Epoch: 2/20; Batch:17/468; Training loss:0.141371\n",
      "Epoch: 2/20; Batch:18/468; Training loss:0.13537\n",
      "Epoch: 2/20; Batch:19/468; Training loss:0.13622\n",
      "Epoch: 2/20; Batch:20/468; Training loss:0.140219\n",
      "Epoch: 2/20; Batch:21/468; Training loss:0.133136\n",
      "Epoch: 2/20; Batch:22/468; Training loss:0.139701\n",
      "Epoch: 2/20; Batch:23/468; Training loss:0.134958\n",
      "Epoch: 2/20; Batch:24/468; Training loss:0.137427\n",
      "Epoch: 2/20; Batch:25/468; Training loss:0.13392\n",
      "Epoch: 2/20; Batch:26/468; Training loss:0.140639\n",
      "Epoch: 2/20; Batch:27/468; Training loss:0.135385\n",
      "Epoch: 2/20; Batch:28/468; Training loss:0.137622\n",
      "Epoch: 2/20; Batch:29/468; Training loss:0.133436\n",
      "Epoch: 2/20; Batch:30/468; Training loss:0.140461\n",
      "Epoch: 2/20; Batch:31/468; Training loss:0.140826\n",
      "Epoch: 2/20; Batch:32/468; Training loss:0.140026\n",
      "Epoch: 2/20; Batch:33/468; Training loss:0.133806\n",
      "Epoch: 2/20; Batch:34/468; Training loss:0.135079\n",
      "Epoch: 2/20; Batch:35/468; Training loss:0.134607\n",
      "Epoch: 2/20; Batch:36/468; Training loss:0.139924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20; Batch:37/468; Training loss:0.134569\n",
      "Epoch: 2/20; Batch:38/468; Training loss:0.138183\n",
      "Epoch: 2/20; Batch:39/468; Training loss:0.136536\n",
      "Epoch: 2/20; Batch:40/468; Training loss:0.132428\n",
      "Epoch: 2/20; Batch:41/468; Training loss:0.137055\n",
      "Epoch: 2/20; Batch:42/468; Training loss:0.137986\n",
      "Epoch: 2/20; Batch:43/468; Training loss:0.135534\n",
      "Epoch: 2/20; Batch:44/468; Training loss:0.132098\n",
      "Epoch: 2/20; Batch:45/468; Training loss:0.136631\n",
      "Epoch: 2/20; Batch:46/468; Training loss:0.134927\n",
      "Epoch: 2/20; Batch:47/468; Training loss:0.13515\n",
      "Epoch: 2/20; Batch:48/468; Training loss:0.131899\n",
      "Epoch: 2/20; Batch:49/468; Training loss:0.136059\n",
      "Epoch: 2/20; Batch:50/468; Training loss:0.138532\n",
      "Epoch: 2/20; Batch:51/468; Training loss:0.129952\n",
      "Epoch: 2/20; Batch:52/468; Training loss:0.135463\n",
      "Epoch: 2/20; Batch:53/468; Training loss:0.139412\n",
      "Epoch: 2/20; Batch:54/468; Training loss:0.125604\n",
      "Epoch: 2/20; Batch:55/468; Training loss:0.137899\n",
      "Epoch: 2/20; Batch:56/468; Training loss:0.133552\n",
      "Epoch: 2/20; Batch:57/468; Training loss:0.129114\n",
      "Epoch: 2/20; Batch:58/468; Training loss:0.136049\n",
      "Epoch: 2/20; Batch:59/468; Training loss:0.141229\n",
      "Epoch: 2/20; Batch:60/468; Training loss:0.130918\n",
      "Epoch: 2/20; Batch:61/468; Training loss:0.131271\n",
      "Epoch: 2/20; Batch:62/468; Training loss:0.131706\n",
      "Epoch: 2/20; Batch:63/468; Training loss:0.129675\n",
      "Epoch: 2/20; Batch:64/468; Training loss:0.130794\n",
      "Epoch: 2/20; Batch:65/468; Training loss:0.131683\n",
      "Epoch: 2/20; Batch:66/468; Training loss:0.136095\n",
      "Epoch: 2/20; Batch:67/468; Training loss:0.134682\n",
      "Epoch: 2/20; Batch:68/468; Training loss:0.136209\n",
      "Epoch: 2/20; Batch:69/468; Training loss:0.133777\n",
      "Epoch: 2/20; Batch:70/468; Training loss:0.127\n",
      "Epoch: 2/20; Batch:71/468; Training loss:0.133151\n",
      "Epoch: 2/20; Batch:72/468; Training loss:0.132618\n",
      "Epoch: 2/20; Batch:73/468; Training loss:0.138375\n",
      "Epoch: 2/20; Batch:74/468; Training loss:0.135825\n",
      "Epoch: 2/20; Batch:75/468; Training loss:0.137281\n",
      "Epoch: 2/20; Batch:76/468; Training loss:0.1387\n",
      "Epoch: 2/20; Batch:77/468; Training loss:0.127534\n",
      "Epoch: 2/20; Batch:78/468; Training loss:0.134195\n",
      "Epoch: 2/20; Batch:79/468; Training loss:0.133295\n",
      "Epoch: 2/20; Batch:80/468; Training loss:0.128424\n",
      "Epoch: 2/20; Batch:81/468; Training loss:0.131784\n",
      "Epoch: 2/20; Batch:82/468; Training loss:0.132149\n",
      "Epoch: 2/20; Batch:83/468; Training loss:0.12783\n",
      "Epoch: 2/20; Batch:84/468; Training loss:0.12915\n",
      "Epoch: 2/20; Batch:85/468; Training loss:0.125868\n",
      "Epoch: 2/20; Batch:86/468; Training loss:0.126741\n",
      "Epoch: 2/20; Batch:87/468; Training loss:0.132097\n",
      "Epoch: 2/20; Batch:88/468; Training loss:0.135615\n",
      "Epoch: 2/20; Batch:89/468; Training loss:0.132337\n",
      "Epoch: 2/20; Batch:90/468; Training loss:0.131493\n",
      "Epoch: 2/20; Batch:91/468; Training loss:0.135522\n",
      "Epoch: 2/20; Batch:92/468; Training loss:0.129238\n",
      "Epoch: 2/20; Batch:93/468; Training loss:0.134104\n",
      "Epoch: 2/20; Batch:94/468; Training loss:0.13519\n",
      "Epoch: 2/20; Batch:95/468; Training loss:0.134989\n",
      "Epoch: 2/20; Batch:96/468; Training loss:0.131891\n",
      "Epoch: 2/20; Batch:97/468; Training loss:0.137195\n",
      "Epoch: 2/20; Batch:98/468; Training loss:0.136724\n",
      "Epoch: 2/20; Batch:99/468; Training loss:0.131592\n",
      "Epoch: 2/20; Batch:100/468; Training loss:0.128195\n",
      "Epoch: 2/20; Batch:101/468; Training loss:0.129031\n",
      "Epoch: 2/20; Batch:102/468; Training loss:0.132721\n",
      "Epoch: 2/20; Batch:103/468; Training loss:0.130677\n",
      "Epoch: 2/20; Batch:104/468; Training loss:0.126795\n",
      "Epoch: 2/20; Batch:105/468; Training loss:0.135342\n",
      "Epoch: 2/20; Batch:106/468; Training loss:0.131575\n",
      "Epoch: 2/20; Batch:107/468; Training loss:0.127749\n",
      "Epoch: 2/20; Batch:108/468; Training loss:0.129659\n",
      "Epoch: 2/20; Batch:109/468; Training loss:0.125309\n",
      "Epoch: 2/20; Batch:110/468; Training loss:0.132741\n",
      "Epoch: 2/20; Batch:111/468; Training loss:0.131129\n",
      "Epoch: 2/20; Batch:112/468; Training loss:0.128173\n",
      "Epoch: 2/20; Batch:113/468; Training loss:0.127872\n",
      "Epoch: 2/20; Batch:114/468; Training loss:0.125472\n",
      "Epoch: 2/20; Batch:115/468; Training loss:0.135347\n",
      "Epoch: 2/20; Batch:116/468; Training loss:0.128941\n",
      "Epoch: 2/20; Batch:117/468; Training loss:0.128217\n",
      "Epoch: 2/20; Batch:118/468; Training loss:0.136004\n",
      "Epoch: 2/20; Batch:119/468; Training loss:0.128106\n",
      "Epoch: 2/20; Batch:120/468; Training loss:0.133741\n",
      "Epoch: 2/20; Batch:121/468; Training loss:0.133203\n",
      "Epoch: 2/20; Batch:122/468; Training loss:0.120504\n",
      "Epoch: 2/20; Batch:123/468; Training loss:0.132615\n",
      "Epoch: 2/20; Batch:124/468; Training loss:0.130448\n",
      "Epoch: 2/20; Batch:125/468; Training loss:0.129602\n",
      "Epoch: 2/20; Batch:126/468; Training loss:0.137988\n",
      "Epoch: 2/20; Batch:127/468; Training loss:0.130254\n",
      "Epoch: 2/20; Batch:128/468; Training loss:0.135033\n",
      "Epoch: 2/20; Batch:129/468; Training loss:0.131514\n",
      "Epoch: 2/20; Batch:130/468; Training loss:0.133012\n",
      "Epoch: 2/20; Batch:131/468; Training loss:0.126325\n",
      "Epoch: 2/20; Batch:132/468; Training loss:0.132037\n",
      "Epoch: 2/20; Batch:133/468; Training loss:0.128296\n",
      "Epoch: 2/20; Batch:134/468; Training loss:0.127049\n",
      "Epoch: 2/20; Batch:135/468; Training loss:0.128126\n",
      "Epoch: 2/20; Batch:136/468; Training loss:0.127625\n",
      "Epoch: 2/20; Batch:137/468; Training loss:0.132963\n",
      "Epoch: 2/20; Batch:138/468; Training loss:0.124122\n",
      "Epoch: 2/20; Batch:139/468; Training loss:0.130039\n",
      "Epoch: 2/20; Batch:140/468; Training loss:0.133016\n",
      "Epoch: 2/20; Batch:141/468; Training loss:0.131219\n",
      "Epoch: 2/20; Batch:142/468; Training loss:0.127458\n",
      "Epoch: 2/20; Batch:143/468; Training loss:0.126513\n",
      "Epoch: 2/20; Batch:144/468; Training loss:0.128002\n",
      "Epoch: 2/20; Batch:145/468; Training loss:0.12871\n",
      "Epoch: 2/20; Batch:146/468; Training loss:0.131545\n",
      "Epoch: 2/20; Batch:147/468; Training loss:0.136539\n",
      "Epoch: 2/20; Batch:148/468; Training loss:0.130627\n",
      "Epoch: 2/20; Batch:149/468; Training loss:0.129188\n",
      "Epoch: 2/20; Batch:150/468; Training loss:0.123445\n",
      "Epoch: 2/20; Batch:151/468; Training loss:0.130774\n",
      "Epoch: 2/20; Batch:152/468; Training loss:0.125209\n",
      "Epoch: 2/20; Batch:153/468; Training loss:0.12309\n",
      "Epoch: 2/20; Batch:154/468; Training loss:0.128056\n",
      "Epoch: 2/20; Batch:155/468; Training loss:0.137861\n",
      "Epoch: 2/20; Batch:156/468; Training loss:0.122613\n",
      "Epoch: 2/20; Batch:157/468; Training loss:0.133403\n",
      "Epoch: 2/20; Batch:158/468; Training loss:0.125944\n",
      "Epoch: 2/20; Batch:159/468; Training loss:0.127153\n",
      "Epoch: 2/20; Batch:160/468; Training loss:0.128572\n",
      "Epoch: 2/20; Batch:161/468; Training loss:0.126858\n",
      "Epoch: 2/20; Batch:162/468; Training loss:0.130959\n",
      "Epoch: 2/20; Batch:163/468; Training loss:0.130369\n",
      "Epoch: 2/20; Batch:164/468; Training loss:0.129785\n",
      "Epoch: 2/20; Batch:165/468; Training loss:0.131293\n",
      "Epoch: 2/20; Batch:166/468; Training loss:0.12607\n",
      "Epoch: 2/20; Batch:167/468; Training loss:0.125322\n",
      "Epoch: 2/20; Batch:168/468; Training loss:0.124147\n",
      "Epoch: 2/20; Batch:169/468; Training loss:0.12641\n",
      "Epoch: 2/20; Batch:170/468; Training loss:0.124185\n",
      "Epoch: 2/20; Batch:171/468; Training loss:0.126523\n",
      "Epoch: 2/20; Batch:172/468; Training loss:0.129363\n",
      "Epoch: 2/20; Batch:173/468; Training loss:0.125687\n",
      "Epoch: 2/20; Batch:174/468; Training loss:0.124623\n",
      "Epoch: 2/20; Batch:175/468; Training loss:0.133163\n",
      "Epoch: 2/20; Batch:176/468; Training loss:0.128187\n",
      "Epoch: 2/20; Batch:177/468; Training loss:0.117\n",
      "Epoch: 2/20; Batch:178/468; Training loss:0.127821\n",
      "Epoch: 2/20; Batch:179/468; Training loss:0.127619\n",
      "Epoch: 2/20; Batch:180/468; Training loss:0.126275\n",
      "Epoch: 2/20; Batch:181/468; Training loss:0.126616\n",
      "Epoch: 2/20; Batch:182/468; Training loss:0.122682\n",
      "Epoch: 2/20; Batch:183/468; Training loss:0.126446\n",
      "Epoch: 2/20; Batch:184/468; Training loss:0.126598\n",
      "Epoch: 2/20; Batch:185/468; Training loss:0.129953\n",
      "Epoch: 2/20; Batch:186/468; Training loss:0.122564\n",
      "Epoch: 2/20; Batch:187/468; Training loss:0.126812\n",
      "Epoch: 2/20; Batch:188/468; Training loss:0.1287\n",
      "Epoch: 2/20; Batch:189/468; Training loss:0.123424\n",
      "Epoch: 2/20; Batch:190/468; Training loss:0.127144\n",
      "Epoch: 2/20; Batch:191/468; Training loss:0.128918\n",
      "Epoch: 2/20; Batch:192/468; Training loss:0.12682\n",
      "Epoch: 2/20; Batch:193/468; Training loss:0.124438\n",
      "Epoch: 2/20; Batch:194/468; Training loss:0.128361\n",
      "Epoch: 2/20; Batch:195/468; Training loss:0.126133\n",
      "Epoch: 2/20; Batch:196/468; Training loss:0.125608\n",
      "Epoch: 2/20; Batch:197/468; Training loss:0.126989\n",
      "Epoch: 2/20; Batch:198/468; Training loss:0.120028\n",
      "Epoch: 2/20; Batch:199/468; Training loss:0.125691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20; Batch:200/468; Training loss:0.124415\n",
      "Epoch: 2/20; Batch:201/468; Training loss:0.119159\n",
      "Epoch: 2/20; Batch:202/468; Training loss:0.124492\n",
      "Epoch: 2/20; Batch:203/468; Training loss:0.123829\n",
      "Epoch: 2/20; Batch:204/468; Training loss:0.127456\n",
      "Epoch: 2/20; Batch:205/468; Training loss:0.126037\n",
      "Epoch: 2/20; Batch:206/468; Training loss:0.120712\n",
      "Epoch: 2/20; Batch:207/468; Training loss:0.130956\n",
      "Epoch: 2/20; Batch:208/468; Training loss:0.128788\n",
      "Epoch: 2/20; Batch:209/468; Training loss:0.126049\n",
      "Epoch: 2/20; Batch:210/468; Training loss:0.122454\n",
      "Epoch: 2/20; Batch:211/468; Training loss:0.126499\n",
      "Epoch: 2/20; Batch:212/468; Training loss:0.12578\n",
      "Epoch: 2/20; Batch:213/468; Training loss:0.121732\n",
      "Epoch: 2/20; Batch:214/468; Training loss:0.129392\n",
      "Epoch: 2/20; Batch:215/468; Training loss:0.129485\n",
      "Epoch: 2/20; Batch:216/468; Training loss:0.12101\n",
      "Epoch: 2/20; Batch:217/468; Training loss:0.131684\n",
      "Epoch: 2/20; Batch:218/468; Training loss:0.124627\n",
      "Epoch: 2/20; Batch:219/468; Training loss:0.119201\n",
      "Epoch: 2/20; Batch:220/468; Training loss:0.124252\n",
      "Epoch: 2/20; Batch:221/468; Training loss:0.128533\n",
      "Epoch: 2/20; Batch:222/468; Training loss:0.126867\n",
      "Epoch: 2/20; Batch:223/468; Training loss:0.12634\n",
      "Epoch: 2/20; Batch:224/468; Training loss:0.127163\n",
      "Epoch: 2/20; Batch:225/468; Training loss:0.125024\n",
      "Epoch: 2/20; Batch:226/468; Training loss:0.130943\n",
      "Epoch: 2/20; Batch:227/468; Training loss:0.127366\n",
      "Epoch: 2/20; Batch:228/468; Training loss:0.123543\n",
      "Epoch: 2/20; Batch:229/468; Training loss:0.120488\n",
      "Epoch: 2/20; Batch:230/468; Training loss:0.12777\n",
      "Epoch: 2/20; Batch:231/468; Training loss:0.124791\n",
      "Epoch: 2/20; Batch:232/468; Training loss:0.130587\n",
      "Epoch: 2/20; Batch:233/468; Training loss:0.126363\n",
      "Epoch: 2/20; Batch:234/468; Training loss:0.124209\n",
      "Epoch: 2/20; Batch:235/468; Training loss:0.124881\n",
      "Epoch: 2/20; Batch:236/468; Training loss:0.123282\n",
      "Epoch: 2/20; Batch:237/468; Training loss:0.124163\n",
      "Epoch: 2/20; Batch:238/468; Training loss:0.125338\n",
      "Epoch: 2/20; Batch:239/468; Training loss:0.120601\n",
      "Epoch: 2/20; Batch:240/468; Training loss:0.123653\n",
      "Epoch: 2/20; Batch:241/468; Training loss:0.121569\n",
      "Epoch: 2/20; Batch:242/468; Training loss:0.121588\n",
      "Epoch: 2/20; Batch:243/468; Training loss:0.12204\n",
      "Epoch: 2/20; Batch:244/468; Training loss:0.127198\n",
      "Epoch: 2/20; Batch:245/468; Training loss:0.122011\n",
      "Epoch: 2/20; Batch:246/468; Training loss:0.121548\n",
      "Epoch: 2/20; Batch:247/468; Training loss:0.130105\n",
      "Epoch: 2/20; Batch:248/468; Training loss:0.123751\n",
      "Epoch: 2/20; Batch:249/468; Training loss:0.123403\n",
      "Epoch: 2/20; Batch:250/468; Training loss:0.11873\n",
      "Epoch: 2/20; Batch:251/468; Training loss:0.119227\n",
      "Epoch: 2/20; Batch:252/468; Training loss:0.12765\n",
      "Epoch: 2/20; Batch:253/468; Training loss:0.121827\n",
      "Epoch: 2/20; Batch:254/468; Training loss:0.121238\n",
      "Epoch: 2/20; Batch:255/468; Training loss:0.1333\n",
      "Epoch: 2/20; Batch:256/468; Training loss:0.114794\n",
      "Epoch: 2/20; Batch:257/468; Training loss:0.12608\n",
      "Epoch: 2/20; Batch:258/468; Training loss:0.122046\n",
      "Epoch: 2/20; Batch:259/468; Training loss:0.126104\n",
      "Epoch: 2/20; Batch:260/468; Training loss:0.121998\n",
      "Epoch: 2/20; Batch:261/468; Training loss:0.124935\n",
      "Epoch: 2/20; Batch:262/468; Training loss:0.12438\n",
      "Epoch: 2/20; Batch:263/468; Training loss:0.125278\n",
      "Epoch: 2/20; Batch:264/468; Training loss:0.124979\n",
      "Epoch: 2/20; Batch:265/468; Training loss:0.119076\n",
      "Epoch: 2/20; Batch:266/468; Training loss:0.124542\n",
      "Epoch: 2/20; Batch:267/468; Training loss:0.125828\n",
      "Epoch: 2/20; Batch:268/468; Training loss:0.124473\n",
      "Epoch: 2/20; Batch:269/468; Training loss:0.119542\n",
      "Epoch: 2/20; Batch:270/468; Training loss:0.124511\n",
      "Epoch: 2/20; Batch:271/468; Training loss:0.126995\n",
      "Epoch: 2/20; Batch:272/468; Training loss:0.124355\n",
      "Epoch: 2/20; Batch:273/468; Training loss:0.121781\n",
      "Epoch: 2/20; Batch:274/468; Training loss:0.123297\n",
      "Epoch: 2/20; Batch:275/468; Training loss:0.127486\n",
      "Epoch: 2/20; Batch:276/468; Training loss:0.120532\n",
      "Epoch: 2/20; Batch:277/468; Training loss:0.120199\n",
      "Epoch: 2/20; Batch:278/468; Training loss:0.118022\n",
      "Epoch: 2/20; Batch:279/468; Training loss:0.12241\n",
      "Epoch: 2/20; Batch:280/468; Training loss:0.120682\n",
      "Epoch: 2/20; Batch:281/468; Training loss:0.122715\n",
      "Epoch: 2/20; Batch:282/468; Training loss:0.124375\n",
      "Epoch: 2/20; Batch:283/468; Training loss:0.122497\n",
      "Epoch: 2/20; Batch:284/468; Training loss:0.125369\n",
      "Epoch: 2/20; Batch:285/468; Training loss:0.123264\n",
      "Epoch: 2/20; Batch:286/468; Training loss:0.12282\n",
      "Epoch: 2/20; Batch:287/468; Training loss:0.122359\n",
      "Epoch: 2/20; Batch:288/468; Training loss:0.123134\n",
      "Epoch: 2/20; Batch:289/468; Training loss:0.123945\n",
      "Epoch: 2/20; Batch:290/468; Training loss:0.122579\n",
      "Epoch: 2/20; Batch:291/468; Training loss:0.124131\n",
      "Epoch: 2/20; Batch:292/468; Training loss:0.120435\n",
      "Epoch: 2/20; Batch:293/468; Training loss:0.125079\n",
      "Epoch: 2/20; Batch:294/468; Training loss:0.120532\n",
      "Epoch: 2/20; Batch:295/468; Training loss:0.12236\n",
      "Epoch: 2/20; Batch:296/468; Training loss:0.12352\n",
      "Epoch: 2/20; Batch:297/468; Training loss:0.118866\n",
      "Epoch: 2/20; Batch:298/468; Training loss:0.126963\n",
      "Epoch: 2/20; Batch:299/468; Training loss:0.126462\n",
      "Epoch: 2/20; Batch:300/468; Training loss:0.120928\n",
      "Epoch: 2/20; Batch:301/468; Training loss:0.127475\n",
      "Epoch: 2/20; Batch:302/468; Training loss:0.122356\n",
      "Epoch: 2/20; Batch:303/468; Training loss:0.124576\n",
      "Epoch: 2/20; Batch:304/468; Training loss:0.12606\n",
      "Epoch: 2/20; Batch:305/468; Training loss:0.119436\n",
      "Epoch: 2/20; Batch:306/468; Training loss:0.124894\n",
      "Epoch: 2/20; Batch:307/468; Training loss:0.119653\n",
      "Epoch: 2/20; Batch:308/468; Training loss:0.122484\n",
      "Epoch: 2/20; Batch:309/468; Training loss:0.119861\n",
      "Epoch: 2/20; Batch:310/468; Training loss:0.122322\n",
      "Epoch: 2/20; Batch:311/468; Training loss:0.12416\n",
      "Epoch: 2/20; Batch:312/468; Training loss:0.122758\n",
      "Epoch: 2/20; Batch:313/468; Training loss:0.119571\n",
      "Epoch: 2/20; Batch:314/468; Training loss:0.123164\n",
      "Epoch: 2/20; Batch:315/468; Training loss:0.119551\n",
      "Epoch: 2/20; Batch:316/468; Training loss:0.123793\n",
      "Epoch: 2/20; Batch:317/468; Training loss:0.11759\n",
      "Epoch: 2/20; Batch:318/468; Training loss:0.11987\n",
      "Epoch: 2/20; Batch:319/468; Training loss:0.115732\n",
      "Epoch: 2/20; Batch:320/468; Training loss:0.120307\n",
      "Epoch: 2/20; Batch:321/468; Training loss:0.120098\n",
      "Epoch: 2/20; Batch:322/468; Training loss:0.116703\n",
      "Epoch: 2/20; Batch:323/468; Training loss:0.125312\n",
      "Epoch: 2/20; Batch:324/468; Training loss:0.120145\n",
      "Epoch: 2/20; Batch:325/468; Training loss:0.129814\n",
      "Epoch: 2/20; Batch:326/468; Training loss:0.117417\n",
      "Epoch: 2/20; Batch:327/468; Training loss:0.119503\n",
      "Epoch: 2/20; Batch:328/468; Training loss:0.119606\n",
      "Epoch: 2/20; Batch:329/468; Training loss:0.12628\n",
      "Epoch: 2/20; Batch:330/468; Training loss:0.11845\n",
      "Epoch: 2/20; Batch:331/468; Training loss:0.120279\n",
      "Epoch: 2/20; Batch:332/468; Training loss:0.119819\n",
      "Epoch: 2/20; Batch:333/468; Training loss:0.12471\n",
      "Epoch: 2/20; Batch:334/468; Training loss:0.120441\n",
      "Epoch: 2/20; Batch:335/468; Training loss:0.120692\n",
      "Epoch: 2/20; Batch:336/468; Training loss:0.118726\n",
      "Epoch: 2/20; Batch:337/468; Training loss:0.119913\n",
      "Epoch: 2/20; Batch:338/468; Training loss:0.11904\n",
      "Epoch: 2/20; Batch:339/468; Training loss:0.124605\n",
      "Epoch: 2/20; Batch:340/468; Training loss:0.121014\n",
      "Epoch: 2/20; Batch:341/468; Training loss:0.121183\n",
      "Epoch: 2/20; Batch:342/468; Training loss:0.121585\n",
      "Epoch: 2/20; Batch:343/468; Training loss:0.117561\n",
      "Epoch: 2/20; Batch:344/468; Training loss:0.121097\n",
      "Epoch: 2/20; Batch:345/468; Training loss:0.126375\n",
      "Epoch: 2/20; Batch:346/468; Training loss:0.123729\n",
      "Epoch: 2/20; Batch:347/468; Training loss:0.122434\n",
      "Epoch: 2/20; Batch:348/468; Training loss:0.120766\n",
      "Epoch: 2/20; Batch:349/468; Training loss:0.121843\n",
      "Epoch: 2/20; Batch:350/468; Training loss:0.117803\n",
      "Epoch: 2/20; Batch:351/468; Training loss:0.118948\n",
      "Epoch: 2/20; Batch:352/468; Training loss:0.123053\n",
      "Epoch: 2/20; Batch:353/468; Training loss:0.117811\n",
      "Epoch: 2/20; Batch:354/468; Training loss:0.120803\n",
      "Epoch: 2/20; Batch:355/468; Training loss:0.118513\n",
      "Epoch: 2/20; Batch:356/468; Training loss:0.118645\n",
      "Epoch: 2/20; Batch:357/468; Training loss:0.120442\n",
      "Epoch: 2/20; Batch:358/468; Training loss:0.126556\n",
      "Epoch: 2/20; Batch:359/468; Training loss:0.117276\n",
      "Epoch: 2/20; Batch:360/468; Training loss:0.122237\n",
      "Epoch: 2/20; Batch:361/468; Training loss:0.121255\n",
      "Epoch: 2/20; Batch:362/468; Training loss:0.11822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20; Batch:363/468; Training loss:0.119125\n",
      "Epoch: 2/20; Batch:364/468; Training loss:0.121911\n",
      "Epoch: 2/20; Batch:365/468; Training loss:0.117287\n",
      "Epoch: 2/20; Batch:366/468; Training loss:0.122738\n",
      "Epoch: 2/20; Batch:367/468; Training loss:0.121302\n",
      "Epoch: 2/20; Batch:368/468; Training loss:0.12231\n",
      "Epoch: 2/20; Batch:369/468; Training loss:0.124761\n",
      "Epoch: 2/20; Batch:370/468; Training loss:0.116653\n",
      "Epoch: 2/20; Batch:371/468; Training loss:0.122478\n",
      "Epoch: 2/20; Batch:372/468; Training loss:0.120587\n",
      "Epoch: 2/20; Batch:373/468; Training loss:0.127199\n",
      "Epoch: 2/20; Batch:374/468; Training loss:0.119758\n",
      "Epoch: 2/20; Batch:375/468; Training loss:0.119778\n",
      "Epoch: 2/20; Batch:376/468; Training loss:0.119806\n",
      "Epoch: 2/20; Batch:377/468; Training loss:0.125373\n",
      "Epoch: 2/20; Batch:378/468; Training loss:0.124615\n",
      "Epoch: 2/20; Batch:379/468; Training loss:0.115922\n",
      "Epoch: 2/20; Batch:380/468; Training loss:0.115981\n",
      "Epoch: 2/20; Batch:381/468; Training loss:0.114911\n",
      "Epoch: 2/20; Batch:382/468; Training loss:0.120771\n",
      "Epoch: 2/20; Batch:383/468; Training loss:0.11548\n",
      "Epoch: 2/20; Batch:384/468; Training loss:0.122512\n",
      "Epoch: 2/20; Batch:385/468; Training loss:0.122952\n",
      "Epoch: 2/20; Batch:386/468; Training loss:0.120972\n",
      "Epoch: 2/20; Batch:387/468; Training loss:0.124446\n",
      "Epoch: 2/20; Batch:388/468; Training loss:0.121275\n",
      "Epoch: 2/20; Batch:389/468; Training loss:0.125197\n",
      "Epoch: 2/20; Batch:390/468; Training loss:0.116191\n",
      "Epoch: 2/20; Batch:391/468; Training loss:0.118964\n",
      "Epoch: 2/20; Batch:392/468; Training loss:0.12087\n",
      "Epoch: 2/20; Batch:393/468; Training loss:0.119656\n",
      "Epoch: 2/20; Batch:394/468; Training loss:0.121527\n",
      "Epoch: 2/20; Batch:395/468; Training loss:0.116812\n",
      "Epoch: 2/20; Batch:396/468; Training loss:0.120093\n",
      "Epoch: 2/20; Batch:397/468; Training loss:0.120038\n",
      "Epoch: 2/20; Batch:398/468; Training loss:0.12434\n",
      "Epoch: 2/20; Batch:399/468; Training loss:0.118643\n",
      "Epoch: 2/20; Batch:400/468; Training loss:0.117346\n",
      "Epoch: 2/20; Batch:401/468; Training loss:0.123101\n",
      "Epoch: 2/20; Batch:402/468; Training loss:0.120937\n",
      "Epoch: 2/20; Batch:403/468; Training loss:0.118972\n",
      "Epoch: 2/20; Batch:404/468; Training loss:0.122507\n",
      "Epoch: 2/20; Batch:405/468; Training loss:0.115095\n",
      "Epoch: 2/20; Batch:406/468; Training loss:0.119014\n",
      "Epoch: 2/20; Batch:407/468; Training loss:0.123627\n",
      "Epoch: 2/20; Batch:408/468; Training loss:0.118541\n",
      "Epoch: 2/20; Batch:409/468; Training loss:0.120115\n",
      "Epoch: 2/20; Batch:410/468; Training loss:0.119773\n",
      "Epoch: 2/20; Batch:411/468; Training loss:0.119294\n",
      "Epoch: 2/20; Batch:412/468; Training loss:0.119765\n",
      "Epoch: 2/20; Batch:413/468; Training loss:0.11788\n",
      "Epoch: 2/20; Batch:414/468; Training loss:0.119028\n",
      "Epoch: 2/20; Batch:415/468; Training loss:0.119894\n",
      "Epoch: 2/20; Batch:416/468; Training loss:0.115052\n",
      "Epoch: 2/20; Batch:417/468; Training loss:0.121013\n",
      "Epoch: 2/20; Batch:418/468; Training loss:0.119173\n",
      "Epoch: 2/20; Batch:419/468; Training loss:0.116583\n",
      "Epoch: 2/20; Batch:420/468; Training loss:0.118832\n",
      "Epoch: 2/20; Batch:421/468; Training loss:0.119478\n",
      "Epoch: 2/20; Batch:422/468; Training loss:0.118917\n",
      "Epoch: 2/20; Batch:423/468; Training loss:0.116297\n",
      "Epoch: 2/20; Batch:424/468; Training loss:0.121503\n",
      "Epoch: 2/20; Batch:425/468; Training loss:0.116046\n",
      "Epoch: 2/20; Batch:426/468; Training loss:0.115472\n",
      "Epoch: 2/20; Batch:427/468; Training loss:0.109537\n",
      "Epoch: 2/20; Batch:428/468; Training loss:0.115848\n",
      "Epoch: 2/20; Batch:429/468; Training loss:0.1251\n",
      "Epoch: 2/20; Batch:430/468; Training loss:0.119343\n",
      "Epoch: 2/20; Batch:431/468; Training loss:0.123767\n",
      "Epoch: 2/20; Batch:432/468; Training loss:0.119897\n",
      "Epoch: 2/20; Batch:433/468; Training loss:0.122211\n",
      "Epoch: 2/20; Batch:434/468; Training loss:0.117283\n",
      "Epoch: 2/20; Batch:435/468; Training loss:0.122254\n",
      "Epoch: 2/20; Batch:436/468; Training loss:0.112334\n",
      "Epoch: 2/20; Batch:437/468; Training loss:0.121023\n",
      "Epoch: 2/20; Batch:438/468; Training loss:0.121475\n",
      "Epoch: 2/20; Batch:439/468; Training loss:0.123494\n",
      "Epoch: 2/20; Batch:440/468; Training loss:0.121758\n",
      "Epoch: 2/20; Batch:441/468; Training loss:0.117769\n",
      "Epoch: 2/20; Batch:442/468; Training loss:0.120107\n",
      "Epoch: 2/20; Batch:443/468; Training loss:0.11972\n",
      "Epoch: 2/20; Batch:444/468; Training loss:0.124195\n",
      "Epoch: 2/20; Batch:445/468; Training loss:0.114837\n",
      "Epoch: 2/20; Batch:446/468; Training loss:0.116887\n",
      "Epoch: 2/20; Batch:447/468; Training loss:0.115955\n",
      "Epoch: 2/20; Batch:448/468; Training loss:0.115822\n",
      "Epoch: 2/20; Batch:449/468; Training loss:0.116223\n",
      "Epoch: 2/20; Batch:450/468; Training loss:0.120122\n",
      "Epoch: 2/20; Batch:451/468; Training loss:0.114768\n",
      "Epoch: 2/20; Batch:452/468; Training loss:0.117508\n",
      "Epoch: 2/20; Batch:453/468; Training loss:0.118786\n",
      "Epoch: 2/20; Batch:454/468; Training loss:0.122131\n",
      "Epoch: 2/20; Batch:455/468; Training loss:0.116787\n",
      "Epoch: 2/20; Batch:456/468; Training loss:0.118767\n",
      "Epoch: 2/20; Batch:457/468; Training loss:0.117677\n",
      "Epoch: 2/20; Batch:458/468; Training loss:0.119765\n",
      "Epoch: 2/20; Batch:459/468; Training loss:0.115349\n",
      "Epoch: 2/20; Batch:460/468; Training loss:0.119397\n",
      "Epoch: 2/20; Batch:461/468; Training loss:0.121691\n",
      "Epoch: 2/20; Batch:462/468; Training loss:0.116327\n",
      "Epoch: 2/20; Batch:463/468; Training loss:0.122617\n",
      "Epoch: 2/20; Batch:464/468; Training loss:0.116395\n",
      "Epoch: 2/20; Batch:465/468; Training loss:0.11899\n",
      "Epoch: 2/20; Batch:466/468; Training loss:0.113609\n",
      "Epoch: 2/20; Batch:467/468; Training loss:0.115886\n",
      "Epoch: 2/20; Batch:468/468; Training loss:0.120847\n",
      "Epoch: 3/20; Batch:1/468; Training loss:0.119732\n",
      "Epoch: 3/20; Batch:2/468; Training loss:0.122679\n",
      "Epoch: 3/20; Batch:3/468; Training loss:0.120135\n",
      "Epoch: 3/20; Batch:4/468; Training loss:0.114031\n",
      "Epoch: 3/20; Batch:5/468; Training loss:0.11865\n",
      "Epoch: 3/20; Batch:6/468; Training loss:0.117617\n",
      "Epoch: 3/20; Batch:7/468; Training loss:0.115982\n",
      "Epoch: 3/20; Batch:8/468; Training loss:0.119341\n",
      "Epoch: 3/20; Batch:9/468; Training loss:0.120384\n",
      "Epoch: 3/20; Batch:10/468; Training loss:0.117275\n",
      "Epoch: 3/20; Batch:11/468; Training loss:0.114358\n",
      "Epoch: 3/20; Batch:12/468; Training loss:0.11958\n",
      "Epoch: 3/20; Batch:13/468; Training loss:0.115665\n",
      "Epoch: 3/20; Batch:14/468; Training loss:0.113802\n",
      "Epoch: 3/20; Batch:15/468; Training loss:0.113517\n",
      "Epoch: 3/20; Batch:16/468; Training loss:0.117111\n",
      "Epoch: 3/20; Batch:17/468; Training loss:0.114231\n",
      "Epoch: 3/20; Batch:18/468; Training loss:0.116943\n",
      "Epoch: 3/20; Batch:19/468; Training loss:0.121536\n",
      "Epoch: 3/20; Batch:20/468; Training loss:0.117976\n",
      "Epoch: 3/20; Batch:21/468; Training loss:0.116237\n",
      "Epoch: 3/20; Batch:22/468; Training loss:0.116363\n",
      "Epoch: 3/20; Batch:23/468; Training loss:0.118352\n",
      "Epoch: 3/20; Batch:24/468; Training loss:0.115186\n",
      "Epoch: 3/20; Batch:25/468; Training loss:0.121714\n",
      "Epoch: 3/20; Batch:26/468; Training loss:0.118249\n",
      "Epoch: 3/20; Batch:27/468; Training loss:0.115564\n",
      "Epoch: 3/20; Batch:28/468; Training loss:0.115609\n",
      "Epoch: 3/20; Batch:29/468; Training loss:0.11597\n",
      "Epoch: 3/20; Batch:30/468; Training loss:0.116038\n",
      "Epoch: 3/20; Batch:31/468; Training loss:0.123362\n",
      "Epoch: 3/20; Batch:32/468; Training loss:0.118373\n",
      "Epoch: 3/20; Batch:33/468; Training loss:0.117524\n",
      "Epoch: 3/20; Batch:34/468; Training loss:0.117699\n",
      "Epoch: 3/20; Batch:35/468; Training loss:0.112639\n",
      "Epoch: 3/20; Batch:36/468; Training loss:0.114473\n",
      "Epoch: 3/20; Batch:37/468; Training loss:0.116233\n",
      "Epoch: 3/20; Batch:38/468; Training loss:0.119549\n",
      "Epoch: 3/20; Batch:39/468; Training loss:0.113915\n",
      "Epoch: 3/20; Batch:40/468; Training loss:0.11663\n",
      "Epoch: 3/20; Batch:41/468; Training loss:0.116871\n",
      "Epoch: 3/20; Batch:42/468; Training loss:0.112121\n",
      "Epoch: 3/20; Batch:43/468; Training loss:0.114765\n",
      "Epoch: 3/20; Batch:44/468; Training loss:0.117741\n",
      "Epoch: 3/20; Batch:45/468; Training loss:0.121425\n",
      "Epoch: 3/20; Batch:46/468; Training loss:0.116121\n",
      "Epoch: 3/20; Batch:47/468; Training loss:0.1154\n",
      "Epoch: 3/20; Batch:48/468; Training loss:0.117139\n",
      "Epoch: 3/20; Batch:49/468; Training loss:0.119377\n",
      "Epoch: 3/20; Batch:50/468; Training loss:0.115164\n",
      "Epoch: 3/20; Batch:51/468; Training loss:0.115134\n",
      "Epoch: 3/20; Batch:52/468; Training loss:0.117659\n",
      "Epoch: 3/20; Batch:53/468; Training loss:0.110352\n",
      "Epoch: 3/20; Batch:54/468; Training loss:0.117059\n",
      "Epoch: 3/20; Batch:55/468; Training loss:0.113615\n",
      "Epoch: 3/20; Batch:56/468; Training loss:0.106939\n",
      "Epoch: 3/20; Batch:57/468; Training loss:0.11383\n",
      "Epoch: 3/20; Batch:58/468; Training loss:0.112546\n",
      "Epoch: 3/20; Batch:59/468; Training loss:0.11727\n",
      "Epoch: 3/20; Batch:60/468; Training loss:0.116177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20; Batch:61/468; Training loss:0.113403\n",
      "Epoch: 3/20; Batch:62/468; Training loss:0.119015\n",
      "Epoch: 3/20; Batch:63/468; Training loss:0.120741\n",
      "Epoch: 3/20; Batch:64/468; Training loss:0.116538\n",
      "Epoch: 3/20; Batch:65/468; Training loss:0.114182\n",
      "Epoch: 3/20; Batch:66/468; Training loss:0.117321\n",
      "Epoch: 3/20; Batch:67/468; Training loss:0.114912\n",
      "Epoch: 3/20; Batch:68/468; Training loss:0.114204\n",
      "Epoch: 3/20; Batch:69/468; Training loss:0.113032\n",
      "Epoch: 3/20; Batch:70/468; Training loss:0.113933\n",
      "Epoch: 3/20; Batch:71/468; Training loss:0.116067\n",
      "Epoch: 3/20; Batch:72/468; Training loss:0.12094\n",
      "Epoch: 3/20; Batch:73/468; Training loss:0.116922\n",
      "Epoch: 3/20; Batch:74/468; Training loss:0.119022\n",
      "Epoch: 3/20; Batch:75/468; Training loss:0.113573\n",
      "Epoch: 3/20; Batch:76/468; Training loss:0.114498\n",
      "Epoch: 3/20; Batch:77/468; Training loss:0.120263\n",
      "Epoch: 3/20; Batch:78/468; Training loss:0.116678\n",
      "Epoch: 3/20; Batch:79/468; Training loss:0.118425\n",
      "Epoch: 3/20; Batch:80/468; Training loss:0.116601\n",
      "Epoch: 3/20; Batch:81/468; Training loss:0.115054\n",
      "Epoch: 3/20; Batch:82/468; Training loss:0.121845\n",
      "Epoch: 3/20; Batch:83/468; Training loss:0.119384\n",
      "Epoch: 3/20; Batch:84/468; Training loss:0.116439\n",
      "Epoch: 3/20; Batch:85/468; Training loss:0.113083\n",
      "Epoch: 3/20; Batch:86/468; Training loss:0.117877\n",
      "Epoch: 3/20; Batch:87/468; Training loss:0.121252\n",
      "Epoch: 3/20; Batch:88/468; Training loss:0.114725\n",
      "Epoch: 3/20; Batch:89/468; Training loss:0.118414\n",
      "Epoch: 3/20; Batch:90/468; Training loss:0.115621\n",
      "Epoch: 3/20; Batch:91/468; Training loss:0.112337\n",
      "Epoch: 3/20; Batch:92/468; Training loss:0.11838\n",
      "Epoch: 3/20; Batch:93/468; Training loss:0.115858\n",
      "Epoch: 3/20; Batch:94/468; Training loss:0.115737\n",
      "Epoch: 3/20; Batch:95/468; Training loss:0.116367\n",
      "Epoch: 3/20; Batch:96/468; Training loss:0.121498\n",
      "Epoch: 3/20; Batch:97/468; Training loss:0.113624\n",
      "Epoch: 3/20; Batch:98/468; Training loss:0.116141\n",
      "Epoch: 3/20; Batch:99/468; Training loss:0.115503\n",
      "Epoch: 3/20; Batch:100/468; Training loss:0.114982\n",
      "Epoch: 3/20; Batch:101/468; Training loss:0.113018\n",
      "Epoch: 3/20; Batch:102/468; Training loss:0.117854\n",
      "Epoch: 3/20; Batch:103/468; Training loss:0.115543\n",
      "Epoch: 3/20; Batch:104/468; Training loss:0.117341\n",
      "Epoch: 3/20; Batch:105/468; Training loss:0.118661\n",
      "Epoch: 3/20; Batch:106/468; Training loss:0.115525\n",
      "Epoch: 3/20; Batch:107/468; Training loss:0.114715\n",
      "Epoch: 3/20; Batch:108/468; Training loss:0.121858\n",
      "Epoch: 3/20; Batch:109/468; Training loss:0.111185\n",
      "Epoch: 3/20; Batch:110/468; Training loss:0.121892\n",
      "Epoch: 3/20; Batch:111/468; Training loss:0.112531\n",
      "Epoch: 3/20; Batch:112/468; Training loss:0.117031\n",
      "Epoch: 3/20; Batch:113/468; Training loss:0.112072\n",
      "Epoch: 3/20; Batch:114/468; Training loss:0.119975\n",
      "Epoch: 3/20; Batch:115/468; Training loss:0.112914\n",
      "Epoch: 3/20; Batch:116/468; Training loss:0.124001\n",
      "Epoch: 3/20; Batch:117/468; Training loss:0.114576\n",
      "Epoch: 3/20; Batch:118/468; Training loss:0.115854\n",
      "Epoch: 3/20; Batch:119/468; Training loss:0.116336\n",
      "Epoch: 3/20; Batch:120/468; Training loss:0.114655\n",
      "Epoch: 3/20; Batch:121/468; Training loss:0.116928\n",
      "Epoch: 3/20; Batch:122/468; Training loss:0.115076\n",
      "Epoch: 3/20; Batch:123/468; Training loss:0.116013\n",
      "Epoch: 3/20; Batch:124/468; Training loss:0.119787\n",
      "Epoch: 3/20; Batch:125/468; Training loss:0.113014\n",
      "Epoch: 3/20; Batch:126/468; Training loss:0.113524\n",
      "Epoch: 3/20; Batch:127/468; Training loss:0.116806\n",
      "Epoch: 3/20; Batch:128/468; Training loss:0.120219\n",
      "Epoch: 3/20; Batch:129/468; Training loss:0.117992\n",
      "Epoch: 3/20; Batch:130/468; Training loss:0.111015\n",
      "Epoch: 3/20; Batch:131/468; Training loss:0.116496\n",
      "Epoch: 3/20; Batch:132/468; Training loss:0.11852\n",
      "Epoch: 3/20; Batch:133/468; Training loss:0.112506\n",
      "Epoch: 3/20; Batch:134/468; Training loss:0.116267\n",
      "Epoch: 3/20; Batch:135/468; Training loss:0.114479\n",
      "Epoch: 3/20; Batch:136/468; Training loss:0.115661\n",
      "Epoch: 3/20; Batch:137/468; Training loss:0.11278\n",
      "Epoch: 3/20; Batch:138/468; Training loss:0.113914\n",
      "Epoch: 3/20; Batch:139/468; Training loss:0.117807\n",
      "Epoch: 3/20; Batch:140/468; Training loss:0.115456\n",
      "Epoch: 3/20; Batch:141/468; Training loss:0.120159\n",
      "Epoch: 3/20; Batch:142/468; Training loss:0.113648\n",
      "Epoch: 3/20; Batch:143/468; Training loss:0.116004\n",
      "Epoch: 3/20; Batch:144/468; Training loss:0.117054\n",
      "Epoch: 3/20; Batch:145/468; Training loss:0.121322\n",
      "Epoch: 3/20; Batch:146/468; Training loss:0.113201\n",
      "Epoch: 3/20; Batch:147/468; Training loss:0.113741\n",
      "Epoch: 3/20; Batch:148/468; Training loss:0.111973\n",
      "Epoch: 3/20; Batch:149/468; Training loss:0.115401\n",
      "Epoch: 3/20; Batch:150/468; Training loss:0.119368\n",
      "Epoch: 3/20; Batch:151/468; Training loss:0.111518\n",
      "Epoch: 3/20; Batch:152/468; Training loss:0.11232\n",
      "Epoch: 3/20; Batch:153/468; Training loss:0.116396\n",
      "Epoch: 3/20; Batch:154/468; Training loss:0.115937\n",
      "Epoch: 3/20; Batch:155/468; Training loss:0.118957\n",
      "Epoch: 3/20; Batch:156/468; Training loss:0.116043\n",
      "Epoch: 3/20; Batch:157/468; Training loss:0.115618\n",
      "Epoch: 3/20; Batch:158/468; Training loss:0.109809\n",
      "Epoch: 3/20; Batch:159/468; Training loss:0.110735\n",
      "Epoch: 3/20; Batch:160/468; Training loss:0.115147\n",
      "Epoch: 3/20; Batch:161/468; Training loss:0.116275\n",
      "Epoch: 3/20; Batch:162/468; Training loss:0.113114\n",
      "Epoch: 3/20; Batch:163/468; Training loss:0.113713\n",
      "Epoch: 3/20; Batch:164/468; Training loss:0.11461\n",
      "Epoch: 3/20; Batch:165/468; Training loss:0.112679\n",
      "Epoch: 3/20; Batch:166/468; Training loss:0.111913\n",
      "Epoch: 3/20; Batch:167/468; Training loss:0.115674\n",
      "Epoch: 3/20; Batch:168/468; Training loss:0.114466\n",
      "Epoch: 3/20; Batch:169/468; Training loss:0.113755\n",
      "Epoch: 3/20; Batch:170/468; Training loss:0.109438\n",
      "Epoch: 3/20; Batch:171/468; Training loss:0.11603\n",
      "Epoch: 3/20; Batch:172/468; Training loss:0.112726\n",
      "Epoch: 3/20; Batch:173/468; Training loss:0.109479\n",
      "Epoch: 3/20; Batch:174/468; Training loss:0.112811\n",
      "Epoch: 3/20; Batch:175/468; Training loss:0.117312\n",
      "Epoch: 3/20; Batch:176/468; Training loss:0.111762\n",
      "Epoch: 3/20; Batch:177/468; Training loss:0.112936\n",
      "Epoch: 3/20; Batch:178/468; Training loss:0.113827\n",
      "Epoch: 3/20; Batch:179/468; Training loss:0.113742\n",
      "Epoch: 3/20; Batch:180/468; Training loss:0.115687\n",
      "Epoch: 3/20; Batch:181/468; Training loss:0.118927\n",
      "Epoch: 3/20; Batch:182/468; Training loss:0.111614\n",
      "Epoch: 3/20; Batch:183/468; Training loss:0.113566\n",
      "Epoch: 3/20; Batch:184/468; Training loss:0.111191\n",
      "Epoch: 3/20; Batch:185/468; Training loss:0.119826\n",
      "Epoch: 3/20; Batch:186/468; Training loss:0.113042\n",
      "Epoch: 3/20; Batch:187/468; Training loss:0.112099\n",
      "Epoch: 3/20; Batch:188/468; Training loss:0.115452\n",
      "Epoch: 3/20; Batch:189/468; Training loss:0.116767\n",
      "Epoch: 3/20; Batch:190/468; Training loss:0.109919\n",
      "Epoch: 3/20; Batch:191/468; Training loss:0.112183\n",
      "Epoch: 3/20; Batch:192/468; Training loss:0.110142\n",
      "Epoch: 3/20; Batch:193/468; Training loss:0.113341\n",
      "Epoch: 3/20; Batch:194/468; Training loss:0.11638\n",
      "Epoch: 3/20; Batch:195/468; Training loss:0.114081\n",
      "Epoch: 3/20; Batch:196/468; Training loss:0.11781\n",
      "Epoch: 3/20; Batch:197/468; Training loss:0.115812\n",
      "Epoch: 3/20; Batch:198/468; Training loss:0.114975\n",
      "Epoch: 3/20; Batch:199/468; Training loss:0.11174\n",
      "Epoch: 3/20; Batch:200/468; Training loss:0.107824\n",
      "Epoch: 3/20; Batch:201/468; Training loss:0.114842\n",
      "Epoch: 3/20; Batch:202/468; Training loss:0.112657\n",
      "Epoch: 3/20; Batch:203/468; Training loss:0.119089\n",
      "Epoch: 3/20; Batch:204/468; Training loss:0.119166\n",
      "Epoch: 3/20; Batch:205/468; Training loss:0.114613\n",
      "Epoch: 3/20; Batch:206/468; Training loss:0.11707\n",
      "Epoch: 3/20; Batch:207/468; Training loss:0.115892\n",
      "Epoch: 3/20; Batch:208/468; Training loss:0.115652\n",
      "Epoch: 3/20; Batch:209/468; Training loss:0.113704\n",
      "Epoch: 3/20; Batch:210/468; Training loss:0.118305\n",
      "Epoch: 3/20; Batch:211/468; Training loss:0.114774\n",
      "Epoch: 3/20; Batch:212/468; Training loss:0.108692\n",
      "Epoch: 3/20; Batch:213/468; Training loss:0.115166\n",
      "Epoch: 3/20; Batch:214/468; Training loss:0.112389\n",
      "Epoch: 3/20; Batch:215/468; Training loss:0.115084\n",
      "Epoch: 3/20; Batch:216/468; Training loss:0.11465\n",
      "Epoch: 3/20; Batch:217/468; Training loss:0.116881\n",
      "Epoch: 3/20; Batch:218/468; Training loss:0.114414\n",
      "Epoch: 3/20; Batch:219/468; Training loss:0.117705\n",
      "Epoch: 3/20; Batch:220/468; Training loss:0.112765\n",
      "Epoch: 3/20; Batch:221/468; Training loss:0.110573\n",
      "Epoch: 3/20; Batch:222/468; Training loss:0.117563\n",
      "Epoch: 3/20; Batch:223/468; Training loss:0.114791\n",
      "Epoch: 3/20; Batch:224/468; Training loss:0.106651\n",
      "Epoch: 3/20; Batch:225/468; Training loss:0.113271\n",
      "Epoch: 3/20; Batch:226/468; Training loss:0.116304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20; Batch:227/468; Training loss:0.114354\n",
      "Epoch: 3/20; Batch:228/468; Training loss:0.109498\n",
      "Epoch: 3/20; Batch:229/468; Training loss:0.113459\n",
      "Epoch: 3/20; Batch:230/468; Training loss:0.112741\n",
      "Epoch: 3/20; Batch:231/468; Training loss:0.113922\n",
      "Epoch: 3/20; Batch:232/468; Training loss:0.106272\n",
      "Epoch: 3/20; Batch:233/468; Training loss:0.111135\n",
      "Epoch: 3/20; Batch:234/468; Training loss:0.116264\n",
      "Epoch: 3/20; Batch:235/468; Training loss:0.111028\n",
      "Epoch: 3/20; Batch:236/468; Training loss:0.113262\n",
      "Epoch: 3/20; Batch:237/468; Training loss:0.112174\n",
      "Epoch: 3/20; Batch:238/468; Training loss:0.114112\n",
      "Epoch: 3/20; Batch:239/468; Training loss:0.111237\n",
      "Epoch: 3/20; Batch:240/468; Training loss:0.104717\n",
      "Epoch: 3/20; Batch:241/468; Training loss:0.106282\n",
      "Epoch: 3/20; Batch:242/468; Training loss:0.109553\n",
      "Epoch: 3/20; Batch:243/468; Training loss:0.114431\n",
      "Epoch: 3/20; Batch:244/468; Training loss:0.110641\n",
      "Epoch: 3/20; Batch:245/468; Training loss:0.118756\n",
      "Epoch: 3/20; Batch:246/468; Training loss:0.110684\n",
      "Epoch: 3/20; Batch:247/468; Training loss:0.116526\n",
      "Epoch: 3/20; Batch:248/468; Training loss:0.11443\n",
      "Epoch: 3/20; Batch:249/468; Training loss:0.117755\n",
      "Epoch: 3/20; Batch:250/468; Training loss:0.113318\n",
      "Epoch: 3/20; Batch:251/468; Training loss:0.111205\n",
      "Epoch: 3/20; Batch:252/468; Training loss:0.109678\n",
      "Epoch: 3/20; Batch:253/468; Training loss:0.11386\n",
      "Epoch: 3/20; Batch:254/468; Training loss:0.107387\n",
      "Epoch: 3/20; Batch:255/468; Training loss:0.109448\n",
      "Epoch: 3/20; Batch:256/468; Training loss:0.110757\n",
      "Epoch: 3/20; Batch:257/468; Training loss:0.114064\n",
      "Epoch: 3/20; Batch:258/468; Training loss:0.114136\n",
      "Epoch: 3/20; Batch:259/468; Training loss:0.11352\n",
      "Epoch: 3/20; Batch:260/468; Training loss:0.111194\n",
      "Epoch: 3/20; Batch:261/468; Training loss:0.111374\n",
      "Epoch: 3/20; Batch:262/468; Training loss:0.112784\n",
      "Epoch: 3/20; Batch:263/468; Training loss:0.105924\n",
      "Epoch: 3/20; Batch:264/468; Training loss:0.116366\n",
      "Epoch: 3/20; Batch:265/468; Training loss:0.116137\n",
      "Epoch: 3/20; Batch:266/468; Training loss:0.113114\n",
      "Epoch: 3/20; Batch:267/468; Training loss:0.115889\n",
      "Epoch: 3/20; Batch:268/468; Training loss:0.112826\n",
      "Epoch: 3/20; Batch:269/468; Training loss:0.110924\n",
      "Epoch: 3/20; Batch:270/468; Training loss:0.11413\n",
      "Epoch: 3/20; Batch:271/468; Training loss:0.110993\n",
      "Epoch: 3/20; Batch:272/468; Training loss:0.112508\n",
      "Epoch: 3/20; Batch:273/468; Training loss:0.116661\n",
      "Epoch: 3/20; Batch:274/468; Training loss:0.107985\n",
      "Epoch: 3/20; Batch:275/468; Training loss:0.113045\n",
      "Epoch: 3/20; Batch:276/468; Training loss:0.116989\n",
      "Epoch: 3/20; Batch:277/468; Training loss:0.118094\n",
      "Epoch: 3/20; Batch:278/468; Training loss:0.108781\n",
      "Epoch: 3/20; Batch:279/468; Training loss:0.110578\n",
      "Epoch: 3/20; Batch:280/468; Training loss:0.113235\n",
      "Epoch: 3/20; Batch:281/468; Training loss:0.114415\n",
      "Epoch: 3/20; Batch:282/468; Training loss:0.114902\n",
      "Epoch: 3/20; Batch:283/468; Training loss:0.113627\n",
      "Epoch: 3/20; Batch:284/468; Training loss:0.113447\n",
      "Epoch: 3/20; Batch:285/468; Training loss:0.111235\n",
      "Epoch: 3/20; Batch:286/468; Training loss:0.111549\n",
      "Epoch: 3/20; Batch:287/468; Training loss:0.10814\n",
      "Epoch: 3/20; Batch:288/468; Training loss:0.11218\n",
      "Epoch: 3/20; Batch:289/468; Training loss:0.114849\n",
      "Epoch: 3/20; Batch:290/468; Training loss:0.114258\n",
      "Epoch: 3/20; Batch:291/468; Training loss:0.107709\n",
      "Epoch: 3/20; Batch:292/468; Training loss:0.112386\n",
      "Epoch: 3/20; Batch:293/468; Training loss:0.113162\n",
      "Epoch: 3/20; Batch:294/468; Training loss:0.109163\n",
      "Epoch: 3/20; Batch:295/468; Training loss:0.111593\n",
      "Epoch: 3/20; Batch:296/468; Training loss:0.110546\n",
      "Epoch: 3/20; Batch:297/468; Training loss:0.111878\n",
      "Epoch: 3/20; Batch:298/468; Training loss:0.114851\n",
      "Epoch: 3/20; Batch:299/468; Training loss:0.107781\n",
      "Epoch: 3/20; Batch:300/468; Training loss:0.110501\n",
      "Epoch: 3/20; Batch:301/468; Training loss:0.109306\n",
      "Epoch: 3/20; Batch:302/468; Training loss:0.113289\n",
      "Epoch: 3/20; Batch:303/468; Training loss:0.114947\n",
      "Epoch: 3/20; Batch:304/468; Training loss:0.110161\n",
      "Epoch: 3/20; Batch:305/468; Training loss:0.114321\n",
      "Epoch: 3/20; Batch:306/468; Training loss:0.114406\n",
      "Epoch: 3/20; Batch:307/468; Training loss:0.112437\n",
      "Epoch: 3/20; Batch:308/468; Training loss:0.113811\n",
      "Epoch: 3/20; Batch:309/468; Training loss:0.109925\n",
      "Epoch: 3/20; Batch:310/468; Training loss:0.109351\n",
      "Epoch: 3/20; Batch:311/468; Training loss:0.108832\n",
      "Epoch: 3/20; Batch:312/468; Training loss:0.111225\n",
      "Epoch: 3/20; Batch:313/468; Training loss:0.114356\n",
      "Epoch: 3/20; Batch:314/468; Training loss:0.116072\n",
      "Epoch: 3/20; Batch:315/468; Training loss:0.111\n",
      "Epoch: 3/20; Batch:316/468; Training loss:0.112942\n",
      "Epoch: 3/20; Batch:317/468; Training loss:0.113604\n",
      "Epoch: 3/20; Batch:318/468; Training loss:0.112303\n",
      "Epoch: 3/20; Batch:319/468; Training loss:0.114055\n",
      "Epoch: 3/20; Batch:320/468; Training loss:0.113021\n",
      "Epoch: 3/20; Batch:321/468; Training loss:0.110175\n",
      "Epoch: 3/20; Batch:322/468; Training loss:0.11267\n",
      "Epoch: 3/20; Batch:323/468; Training loss:0.112591\n",
      "Epoch: 3/20; Batch:324/468; Training loss:0.111603\n",
      "Epoch: 3/20; Batch:325/468; Training loss:0.112903\n",
      "Epoch: 3/20; Batch:326/468; Training loss:0.113701\n",
      "Epoch: 3/20; Batch:327/468; Training loss:0.113206\n",
      "Epoch: 3/20; Batch:328/468; Training loss:0.112476\n",
      "Epoch: 3/20; Batch:329/468; Training loss:0.110478\n",
      "Epoch: 3/20; Batch:330/468; Training loss:0.110489\n",
      "Epoch: 3/20; Batch:331/468; Training loss:0.106792\n",
      "Epoch: 3/20; Batch:332/468; Training loss:0.108744\n",
      "Epoch: 3/20; Batch:333/468; Training loss:0.119772\n",
      "Epoch: 3/20; Batch:334/468; Training loss:0.110392\n",
      "Epoch: 3/20; Batch:335/468; Training loss:0.117\n",
      "Epoch: 3/20; Batch:336/468; Training loss:0.115873\n",
      "Epoch: 3/20; Batch:337/468; Training loss:0.11126\n",
      "Epoch: 3/20; Batch:338/468; Training loss:0.116874\n",
      "Epoch: 3/20; Batch:339/468; Training loss:0.112255\n",
      "Epoch: 3/20; Batch:340/468; Training loss:0.116514\n",
      "Epoch: 3/20; Batch:341/468; Training loss:0.111311\n",
      "Epoch: 3/20; Batch:342/468; Training loss:0.114157\n",
      "Epoch: 3/20; Batch:343/468; Training loss:0.111772\n",
      "Epoch: 3/20; Batch:344/468; Training loss:0.114962\n",
      "Epoch: 3/20; Batch:345/468; Training loss:0.113081\n",
      "Epoch: 3/20; Batch:346/468; Training loss:0.108134\n",
      "Epoch: 3/20; Batch:347/468; Training loss:0.113611\n",
      "Epoch: 3/20; Batch:348/468; Training loss:0.109328\n",
      "Epoch: 3/20; Batch:349/468; Training loss:0.115728\n",
      "Epoch: 3/20; Batch:350/468; Training loss:0.108038\n",
      "Epoch: 3/20; Batch:351/468; Training loss:0.110879\n",
      "Epoch: 3/20; Batch:352/468; Training loss:0.108375\n",
      "Epoch: 3/20; Batch:353/468; Training loss:0.111834\n",
      "Epoch: 3/20; Batch:354/468; Training loss:0.113706\n",
      "Epoch: 3/20; Batch:355/468; Training loss:0.116837\n",
      "Epoch: 3/20; Batch:356/468; Training loss:0.113295\n",
      "Epoch: 3/20; Batch:357/468; Training loss:0.112053\n",
      "Epoch: 3/20; Batch:358/468; Training loss:0.113077\n",
      "Epoch: 3/20; Batch:359/468; Training loss:0.11179\n",
      "Epoch: 3/20; Batch:360/468; Training loss:0.106036\n",
      "Epoch: 3/20; Batch:361/468; Training loss:0.108321\n",
      "Epoch: 3/20; Batch:362/468; Training loss:0.116374\n",
      "Epoch: 3/20; Batch:363/468; Training loss:0.111467\n",
      "Epoch: 3/20; Batch:364/468; Training loss:0.117061\n",
      "Epoch: 3/20; Batch:365/468; Training loss:0.108305\n",
      "Epoch: 3/20; Batch:366/468; Training loss:0.111686\n",
      "Epoch: 3/20; Batch:367/468; Training loss:0.111129\n",
      "Epoch: 3/20; Batch:368/468; Training loss:0.107383\n",
      "Epoch: 3/20; Batch:369/468; Training loss:0.109057\n",
      "Epoch: 3/20; Batch:370/468; Training loss:0.113178\n",
      "Epoch: 3/20; Batch:371/468; Training loss:0.110933\n",
      "Epoch: 3/20; Batch:372/468; Training loss:0.110809\n",
      "Epoch: 3/20; Batch:373/468; Training loss:0.109479\n",
      "Epoch: 3/20; Batch:374/468; Training loss:0.110473\n",
      "Epoch: 3/20; Batch:375/468; Training loss:0.108994\n",
      "Epoch: 3/20; Batch:376/468; Training loss:0.109346\n",
      "Epoch: 3/20; Batch:377/468; Training loss:0.117992\n",
      "Epoch: 3/20; Batch:378/468; Training loss:0.117406\n",
      "Epoch: 3/20; Batch:379/468; Training loss:0.112554\n",
      "Epoch: 3/20; Batch:380/468; Training loss:0.110769\n",
      "Epoch: 3/20; Batch:381/468; Training loss:0.111732\n",
      "Epoch: 3/20; Batch:382/468; Training loss:0.109995\n",
      "Epoch: 3/20; Batch:383/468; Training loss:0.113191\n",
      "Epoch: 3/20; Batch:384/468; Training loss:0.109147\n",
      "Epoch: 3/20; Batch:385/468; Training loss:0.106116\n",
      "Epoch: 3/20; Batch:386/468; Training loss:0.112425\n",
      "Epoch: 3/20; Batch:387/468; Training loss:0.113179\n",
      "Epoch: 3/20; Batch:388/468; Training loss:0.114179\n",
      "Epoch: 3/20; Batch:389/468; Training loss:0.112932\n",
      "Epoch: 3/20; Batch:390/468; Training loss:0.111309\n",
      "Epoch: 3/20; Batch:391/468; Training loss:0.113526\n",
      "Epoch: 3/20; Batch:392/468; Training loss:0.114034\n",
      "Epoch: 3/20; Batch:393/468; Training loss:0.114212\n",
      "Epoch: 3/20; Batch:394/468; Training loss:0.11448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20; Batch:395/468; Training loss:0.109051\n",
      "Epoch: 3/20; Batch:396/468; Training loss:0.114596\n",
      "Epoch: 3/20; Batch:397/468; Training loss:0.109742\n",
      "Epoch: 3/20; Batch:398/468; Training loss:0.110911\n",
      "Epoch: 3/20; Batch:399/468; Training loss:0.117987\n",
      "Epoch: 3/20; Batch:400/468; Training loss:0.115566\n",
      "Epoch: 3/20; Batch:401/468; Training loss:0.112601\n",
      "Epoch: 3/20; Batch:402/468; Training loss:0.111179\n",
      "Epoch: 3/20; Batch:403/468; Training loss:0.111788\n",
      "Epoch: 3/20; Batch:404/468; Training loss:0.115592\n",
      "Epoch: 3/20; Batch:405/468; Training loss:0.11094\n",
      "Epoch: 3/20; Batch:406/468; Training loss:0.112132\n",
      "Epoch: 3/20; Batch:407/468; Training loss:0.111937\n",
      "Epoch: 3/20; Batch:408/468; Training loss:0.110326\n",
      "Epoch: 3/20; Batch:409/468; Training loss:0.110548\n",
      "Epoch: 3/20; Batch:410/468; Training loss:0.111615\n",
      "Epoch: 3/20; Batch:411/468; Training loss:0.111856\n",
      "Epoch: 3/20; Batch:412/468; Training loss:0.105869\n",
      "Epoch: 3/20; Batch:413/468; Training loss:0.109703\n",
      "Epoch: 3/20; Batch:414/468; Training loss:0.110357\n",
      "Epoch: 3/20; Batch:415/468; Training loss:0.109678\n",
      "Epoch: 3/20; Batch:416/468; Training loss:0.114256\n",
      "Epoch: 3/20; Batch:417/468; Training loss:0.11078\n",
      "Epoch: 3/20; Batch:418/468; Training loss:0.114762\n",
      "Epoch: 3/20; Batch:419/468; Training loss:0.10983\n",
      "Epoch: 3/20; Batch:420/468; Training loss:0.108973\n",
      "Epoch: 3/20; Batch:421/468; Training loss:0.109063\n",
      "Epoch: 3/20; Batch:422/468; Training loss:0.110454\n",
      "Epoch: 3/20; Batch:423/468; Training loss:0.114137\n",
      "Epoch: 3/20; Batch:424/468; Training loss:0.114535\n",
      "Epoch: 3/20; Batch:425/468; Training loss:0.11376\n",
      "Epoch: 3/20; Batch:426/468; Training loss:0.113129\n",
      "Epoch: 3/20; Batch:427/468; Training loss:0.110957\n",
      "Epoch: 3/20; Batch:428/468; Training loss:0.114347\n",
      "Epoch: 3/20; Batch:429/468; Training loss:0.110534\n",
      "Epoch: 3/20; Batch:430/468; Training loss:0.113664\n",
      "Epoch: 3/20; Batch:431/468; Training loss:0.111517\n",
      "Epoch: 3/20; Batch:432/468; Training loss:0.115637\n",
      "Epoch: 3/20; Batch:433/468; Training loss:0.11155\n",
      "Epoch: 3/20; Batch:434/468; Training loss:0.111664\n",
      "Epoch: 3/20; Batch:435/468; Training loss:0.107454\n",
      "Epoch: 3/20; Batch:436/468; Training loss:0.112568\n",
      "Epoch: 3/20; Batch:437/468; Training loss:0.111359\n",
      "Epoch: 3/20; Batch:438/468; Training loss:0.113185\n",
      "Epoch: 3/20; Batch:439/468; Training loss:0.109128\n",
      "Epoch: 3/20; Batch:440/468; Training loss:0.108037\n",
      "Epoch: 3/20; Batch:441/468; Training loss:0.110974\n",
      "Epoch: 3/20; Batch:442/468; Training loss:0.107585\n",
      "Epoch: 3/20; Batch:443/468; Training loss:0.110186\n",
      "Epoch: 3/20; Batch:444/468; Training loss:0.106701\n",
      "Epoch: 3/20; Batch:445/468; Training loss:0.112825\n",
      "Epoch: 3/20; Batch:446/468; Training loss:0.11299\n",
      "Epoch: 3/20; Batch:447/468; Training loss:0.110534\n",
      "Epoch: 3/20; Batch:448/468; Training loss:0.113098\n",
      "Epoch: 3/20; Batch:449/468; Training loss:0.112242\n",
      "Epoch: 3/20; Batch:450/468; Training loss:0.108183\n",
      "Epoch: 3/20; Batch:451/468; Training loss:0.111745\n",
      "Epoch: 3/20; Batch:452/468; Training loss:0.109551\n",
      "Epoch: 3/20; Batch:453/468; Training loss:0.114371\n",
      "Epoch: 3/20; Batch:454/468; Training loss:0.112277\n",
      "Epoch: 3/20; Batch:455/468; Training loss:0.108675\n",
      "Epoch: 3/20; Batch:456/468; Training loss:0.109885\n",
      "Epoch: 3/20; Batch:457/468; Training loss:0.108053\n",
      "Epoch: 3/20; Batch:458/468; Training loss:0.111177\n",
      "Epoch: 3/20; Batch:459/468; Training loss:0.113586\n",
      "Epoch: 3/20; Batch:460/468; Training loss:0.109053\n",
      "Epoch: 3/20; Batch:461/468; Training loss:0.10963\n",
      "Epoch: 3/20; Batch:462/468; Training loss:0.110747\n",
      "Epoch: 3/20; Batch:463/468; Training loss:0.115662\n",
      "Epoch: 3/20; Batch:464/468; Training loss:0.113541\n",
      "Epoch: 3/20; Batch:465/468; Training loss:0.109802\n",
      "Epoch: 3/20; Batch:466/468; Training loss:0.109875\n",
      "Epoch: 3/20; Batch:467/468; Training loss:0.110568\n",
      "Epoch: 3/20; Batch:468/468; Training loss:0.112791\n",
      "Epoch: 4/20; Batch:1/468; Training loss:0.108581\n",
      "Epoch: 4/20; Batch:2/468; Training loss:0.112076\n",
      "Epoch: 4/20; Batch:3/468; Training loss:0.111902\n",
      "Epoch: 4/20; Batch:4/468; Training loss:0.109273\n",
      "Epoch: 4/20; Batch:5/468; Training loss:0.111434\n",
      "Epoch: 4/20; Batch:6/468; Training loss:0.111978\n",
      "Epoch: 4/20; Batch:7/468; Training loss:0.113684\n",
      "Epoch: 4/20; Batch:8/468; Training loss:0.11058\n",
      "Epoch: 4/20; Batch:9/468; Training loss:0.111824\n",
      "Epoch: 4/20; Batch:10/468; Training loss:0.109338\n",
      "Epoch: 4/20; Batch:11/468; Training loss:0.111407\n",
      "Epoch: 4/20; Batch:12/468; Training loss:0.111365\n",
      "Epoch: 4/20; Batch:13/468; Training loss:0.114389\n",
      "Epoch: 4/20; Batch:14/468; Training loss:0.110788\n",
      "Epoch: 4/20; Batch:15/468; Training loss:0.110331\n",
      "Epoch: 4/20; Batch:16/468; Training loss:0.111263\n",
      "Epoch: 4/20; Batch:17/468; Training loss:0.111834\n",
      "Epoch: 4/20; Batch:18/468; Training loss:0.105842\n",
      "Epoch: 4/20; Batch:19/468; Training loss:0.106389\n",
      "Epoch: 4/20; Batch:20/468; Training loss:0.115132\n",
      "Epoch: 4/20; Batch:21/468; Training loss:0.113071\n",
      "Epoch: 4/20; Batch:22/468; Training loss:0.110623\n",
      "Epoch: 4/20; Batch:23/468; Training loss:0.110287\n",
      "Epoch: 4/20; Batch:24/468; Training loss:0.109112\n",
      "Epoch: 4/20; Batch:25/468; Training loss:0.109799\n",
      "Epoch: 4/20; Batch:26/468; Training loss:0.110752\n",
      "Epoch: 4/20; Batch:27/468; Training loss:0.11095\n",
      "Epoch: 4/20; Batch:28/468; Training loss:0.105974\n",
      "Epoch: 4/20; Batch:29/468; Training loss:0.11087\n",
      "Epoch: 4/20; Batch:30/468; Training loss:0.107657\n",
      "Epoch: 4/20; Batch:31/468; Training loss:0.10747\n",
      "Epoch: 4/20; Batch:32/468; Training loss:0.107686\n",
      "Epoch: 4/20; Batch:33/468; Training loss:0.113295\n",
      "Epoch: 4/20; Batch:34/468; Training loss:0.11386\n",
      "Epoch: 4/20; Batch:35/468; Training loss:0.11335\n",
      "Epoch: 4/20; Batch:36/468; Training loss:0.107357\n",
      "Epoch: 4/20; Batch:37/468; Training loss:0.111644\n",
      "Epoch: 4/20; Batch:38/468; Training loss:0.106076\n",
      "Epoch: 4/20; Batch:39/468; Training loss:0.106855\n",
      "Epoch: 4/20; Batch:40/468; Training loss:0.109144\n",
      "Epoch: 4/20; Batch:41/468; Training loss:0.110332\n",
      "Epoch: 4/20; Batch:42/468; Training loss:0.114317\n",
      "Epoch: 4/20; Batch:43/468; Training loss:0.113991\n",
      "Epoch: 4/20; Batch:44/468; Training loss:0.1062\n",
      "Epoch: 4/20; Batch:45/468; Training loss:0.112483\n",
      "Epoch: 4/20; Batch:46/468; Training loss:0.111337\n",
      "Epoch: 4/20; Batch:47/468; Training loss:0.113321\n",
      "Epoch: 4/20; Batch:48/468; Training loss:0.10978\n",
      "Epoch: 4/20; Batch:49/468; Training loss:0.110344\n",
      "Epoch: 4/20; Batch:50/468; Training loss:0.110504\n",
      "Epoch: 4/20; Batch:51/468; Training loss:0.111382\n",
      "Epoch: 4/20; Batch:52/468; Training loss:0.109607\n",
      "Epoch: 4/20; Batch:53/468; Training loss:0.11045\n",
      "Epoch: 4/20; Batch:54/468; Training loss:0.111263\n",
      "Epoch: 4/20; Batch:55/468; Training loss:0.106754\n",
      "Epoch: 4/20; Batch:56/468; Training loss:0.106966\n",
      "Epoch: 4/20; Batch:57/468; Training loss:0.107675\n",
      "Epoch: 4/20; Batch:58/468; Training loss:0.109328\n",
      "Epoch: 4/20; Batch:59/468; Training loss:0.113057\n",
      "Epoch: 4/20; Batch:60/468; Training loss:0.114537\n",
      "Epoch: 4/20; Batch:61/468; Training loss:0.106436\n",
      "Epoch: 4/20; Batch:62/468; Training loss:0.110088\n",
      "Epoch: 4/20; Batch:63/468; Training loss:0.110781\n",
      "Epoch: 4/20; Batch:64/468; Training loss:0.110752\n",
      "Epoch: 4/20; Batch:65/468; Training loss:0.111715\n",
      "Epoch: 4/20; Batch:66/468; Training loss:0.107555\n",
      "Epoch: 4/20; Batch:67/468; Training loss:0.107427\n",
      "Epoch: 4/20; Batch:68/468; Training loss:0.110659\n",
      "Epoch: 4/20; Batch:69/468; Training loss:0.111482\n",
      "Epoch: 4/20; Batch:70/468; Training loss:0.111892\n",
      "Epoch: 4/20; Batch:71/468; Training loss:0.107662\n",
      "Epoch: 4/20; Batch:72/468; Training loss:0.108894\n",
      "Epoch: 4/20; Batch:73/468; Training loss:0.11036\n",
      "Epoch: 4/20; Batch:74/468; Training loss:0.109404\n",
      "Epoch: 4/20; Batch:75/468; Training loss:0.109702\n",
      "Epoch: 4/20; Batch:76/468; Training loss:0.1114\n",
      "Epoch: 4/20; Batch:77/468; Training loss:0.114695\n",
      "Epoch: 4/20; Batch:78/468; Training loss:0.107477\n",
      "Epoch: 4/20; Batch:79/468; Training loss:0.113684\n",
      "Epoch: 4/20; Batch:80/468; Training loss:0.103894\n",
      "Epoch: 4/20; Batch:81/468; Training loss:0.109281\n",
      "Epoch: 4/20; Batch:82/468; Training loss:0.105068\n",
      "Epoch: 4/20; Batch:83/468; Training loss:0.109891\n",
      "Epoch: 4/20; Batch:84/468; Training loss:0.112136\n",
      "Epoch: 4/20; Batch:85/468; Training loss:0.107175\n",
      "Epoch: 4/20; Batch:86/468; Training loss:0.112722\n",
      "Epoch: 4/20; Batch:87/468; Training loss:0.106951\n",
      "Epoch: 4/20; Batch:88/468; Training loss:0.113192\n",
      "Epoch: 4/20; Batch:89/468; Training loss:0.110439\n",
      "Epoch: 4/20; Batch:90/468; Training loss:0.113089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20; Batch:91/468; Training loss:0.10982\n",
      "Epoch: 4/20; Batch:92/468; Training loss:0.11025\n",
      "Epoch: 4/20; Batch:93/468; Training loss:0.11065\n",
      "Epoch: 4/20; Batch:94/468; Training loss:0.109429\n",
      "Epoch: 4/20; Batch:95/468; Training loss:0.108066\n",
      "Epoch: 4/20; Batch:96/468; Training loss:0.111394\n",
      "Epoch: 4/20; Batch:97/468; Training loss:0.104742\n",
      "Epoch: 4/20; Batch:98/468; Training loss:0.109368\n",
      "Epoch: 4/20; Batch:99/468; Training loss:0.10758\n",
      "Epoch: 4/20; Batch:100/468; Training loss:0.113476\n",
      "Epoch: 4/20; Batch:101/468; Training loss:0.107626\n",
      "Epoch: 4/20; Batch:102/468; Training loss:0.107453\n",
      "Epoch: 4/20; Batch:103/468; Training loss:0.109079\n",
      "Epoch: 4/20; Batch:104/468; Training loss:0.106055\n",
      "Epoch: 4/20; Batch:105/468; Training loss:0.105374\n",
      "Epoch: 4/20; Batch:106/468; Training loss:0.110103\n",
      "Epoch: 4/20; Batch:107/468; Training loss:0.112279\n",
      "Epoch: 4/20; Batch:108/468; Training loss:0.104814\n",
      "Epoch: 4/20; Batch:109/468; Training loss:0.110762\n",
      "Epoch: 4/20; Batch:110/468; Training loss:0.110439\n",
      "Epoch: 4/20; Batch:111/468; Training loss:0.10984\n",
      "Epoch: 4/20; Batch:112/468; Training loss:0.112087\n",
      "Epoch: 4/20; Batch:113/468; Training loss:0.103496\n",
      "Epoch: 4/20; Batch:114/468; Training loss:0.109808\n",
      "Epoch: 4/20; Batch:115/468; Training loss:0.108406\n",
      "Epoch: 4/20; Batch:116/468; Training loss:0.108539\n",
      "Epoch: 4/20; Batch:117/468; Training loss:0.108644\n",
      "Epoch: 4/20; Batch:118/468; Training loss:0.108349\n",
      "Epoch: 4/20; Batch:119/468; Training loss:0.106558\n",
      "Epoch: 4/20; Batch:120/468; Training loss:0.112223\n",
      "Epoch: 4/20; Batch:121/468; Training loss:0.112233\n",
      "Epoch: 4/20; Batch:122/468; Training loss:0.107872\n",
      "Epoch: 4/20; Batch:123/468; Training loss:0.109884\n",
      "Epoch: 4/20; Batch:124/468; Training loss:0.111368\n",
      "Epoch: 4/20; Batch:125/468; Training loss:0.105621\n",
      "Epoch: 4/20; Batch:126/468; Training loss:0.115191\n",
      "Epoch: 4/20; Batch:127/468; Training loss:0.110795\n",
      "Epoch: 4/20; Batch:128/468; Training loss:0.110907\n",
      "Epoch: 4/20; Batch:129/468; Training loss:0.109966\n",
      "Epoch: 4/20; Batch:130/468; Training loss:0.108799\n",
      "Epoch: 4/20; Batch:131/468; Training loss:0.111124\n",
      "Epoch: 4/20; Batch:132/468; Training loss:0.105021\n",
      "Epoch: 4/20; Batch:133/468; Training loss:0.106728\n",
      "Epoch: 4/20; Batch:134/468; Training loss:0.107559\n",
      "Epoch: 4/20; Batch:135/468; Training loss:0.111645\n",
      "Epoch: 4/20; Batch:136/468; Training loss:0.110214\n",
      "Epoch: 4/20; Batch:137/468; Training loss:0.112722\n",
      "Epoch: 4/20; Batch:138/468; Training loss:0.112907\n",
      "Epoch: 4/20; Batch:139/468; Training loss:0.113234\n",
      "Epoch: 4/20; Batch:140/468; Training loss:0.105331\n",
      "Epoch: 4/20; Batch:141/468; Training loss:0.109676\n",
      "Epoch: 4/20; Batch:142/468; Training loss:0.110033\n",
      "Epoch: 4/20; Batch:143/468; Training loss:0.109347\n",
      "Epoch: 4/20; Batch:144/468; Training loss:0.107237\n",
      "Epoch: 4/20; Batch:145/468; Training loss:0.112096\n",
      "Epoch: 4/20; Batch:146/468; Training loss:0.109544\n",
      "Epoch: 4/20; Batch:147/468; Training loss:0.106381\n",
      "Epoch: 4/20; Batch:148/468; Training loss:0.110517\n",
      "Epoch: 4/20; Batch:149/468; Training loss:0.114311\n",
      "Epoch: 4/20; Batch:150/468; Training loss:0.108278\n",
      "Epoch: 4/20; Batch:151/468; Training loss:0.11294\n",
      "Epoch: 4/20; Batch:152/468; Training loss:0.112903\n",
      "Epoch: 4/20; Batch:153/468; Training loss:0.105308\n",
      "Epoch: 4/20; Batch:154/468; Training loss:0.110354\n",
      "Epoch: 4/20; Batch:155/468; Training loss:0.111735\n",
      "Epoch: 4/20; Batch:156/468; Training loss:0.107324\n",
      "Epoch: 4/20; Batch:157/468; Training loss:0.104356\n",
      "Epoch: 4/20; Batch:158/468; Training loss:0.113793\n",
      "Epoch: 4/20; Batch:159/468; Training loss:0.107522\n",
      "Epoch: 4/20; Batch:160/468; Training loss:0.110059\n",
      "Epoch: 4/20; Batch:161/468; Training loss:0.113214\n",
      "Epoch: 4/20; Batch:162/468; Training loss:0.112586\n",
      "Epoch: 4/20; Batch:163/468; Training loss:0.112453\n",
      "Epoch: 4/20; Batch:164/468; Training loss:0.109841\n",
      "Epoch: 4/20; Batch:165/468; Training loss:0.107566\n",
      "Epoch: 4/20; Batch:166/468; Training loss:0.108193\n",
      "Epoch: 4/20; Batch:167/468; Training loss:0.11028\n",
      "Epoch: 4/20; Batch:168/468; Training loss:0.11063\n",
      "Epoch: 4/20; Batch:169/468; Training loss:0.110278\n",
      "Epoch: 4/20; Batch:170/468; Training loss:0.107229\n",
      "Epoch: 4/20; Batch:171/468; Training loss:0.110089\n",
      "Epoch: 4/20; Batch:172/468; Training loss:0.104904\n",
      "Epoch: 4/20; Batch:173/468; Training loss:0.110717\n",
      "Epoch: 4/20; Batch:174/468; Training loss:0.111419\n",
      "Epoch: 4/20; Batch:175/468; Training loss:0.109007\n",
      "Epoch: 4/20; Batch:176/468; Training loss:0.11089\n",
      "Epoch: 4/20; Batch:177/468; Training loss:0.113533\n",
      "Epoch: 4/20; Batch:178/468; Training loss:0.108354\n",
      "Epoch: 4/20; Batch:179/468; Training loss:0.110544\n",
      "Epoch: 4/20; Batch:180/468; Training loss:0.108772\n",
      "Epoch: 4/20; Batch:181/468; Training loss:0.107847\n",
      "Epoch: 4/20; Batch:182/468; Training loss:0.107905\n",
      "Epoch: 4/20; Batch:183/468; Training loss:0.107718\n",
      "Epoch: 4/20; Batch:184/468; Training loss:0.105753\n",
      "Epoch: 4/20; Batch:185/468; Training loss:0.108784\n",
      "Epoch: 4/20; Batch:186/468; Training loss:0.104019\n",
      "Epoch: 4/20; Batch:187/468; Training loss:0.109239\n",
      "Epoch: 4/20; Batch:188/468; Training loss:0.112486\n",
      "Epoch: 4/20; Batch:189/468; Training loss:0.108765\n",
      "Epoch: 4/20; Batch:190/468; Training loss:0.109855\n",
      "Epoch: 4/20; Batch:191/468; Training loss:0.109601\n",
      "Epoch: 4/20; Batch:192/468; Training loss:0.108789\n",
      "Epoch: 4/20; Batch:193/468; Training loss:0.107554\n",
      "Epoch: 4/20; Batch:194/468; Training loss:0.103923\n",
      "Epoch: 4/20; Batch:195/468; Training loss:0.107139\n",
      "Epoch: 4/20; Batch:196/468; Training loss:0.1103\n",
      "Epoch: 4/20; Batch:197/468; Training loss:0.110252\n",
      "Epoch: 4/20; Batch:198/468; Training loss:0.105714\n",
      "Epoch: 4/20; Batch:199/468; Training loss:0.110448\n",
      "Epoch: 4/20; Batch:200/468; Training loss:0.110685\n",
      "Epoch: 4/20; Batch:201/468; Training loss:0.108703\n",
      "Epoch: 4/20; Batch:202/468; Training loss:0.104992\n",
      "Epoch: 4/20; Batch:203/468; Training loss:0.106993\n",
      "Epoch: 4/20; Batch:204/468; Training loss:0.104487\n",
      "Epoch: 4/20; Batch:205/468; Training loss:0.107198\n",
      "Epoch: 4/20; Batch:206/468; Training loss:0.108418\n",
      "Epoch: 4/20; Batch:207/468; Training loss:0.106634\n",
      "Epoch: 4/20; Batch:208/468; Training loss:0.103729\n",
      "Epoch: 4/20; Batch:209/468; Training loss:0.111204\n",
      "Epoch: 4/20; Batch:210/468; Training loss:0.101836\n",
      "Epoch: 4/20; Batch:211/468; Training loss:0.111421\n",
      "Epoch: 4/20; Batch:212/468; Training loss:0.108951\n",
      "Epoch: 4/20; Batch:213/468; Training loss:0.114918\n",
      "Epoch: 4/20; Batch:214/468; Training loss:0.109012\n",
      "Epoch: 4/20; Batch:215/468; Training loss:0.106641\n",
      "Epoch: 4/20; Batch:216/468; Training loss:0.111163\n",
      "Epoch: 4/20; Batch:217/468; Training loss:0.106612\n",
      "Epoch: 4/20; Batch:218/468; Training loss:0.104655\n",
      "Epoch: 4/20; Batch:219/468; Training loss:0.106666\n",
      "Epoch: 4/20; Batch:220/468; Training loss:0.10942\n",
      "Epoch: 4/20; Batch:221/468; Training loss:0.108786\n",
      "Epoch: 4/20; Batch:222/468; Training loss:0.108876\n",
      "Epoch: 4/20; Batch:223/468; Training loss:0.105734\n",
      "Epoch: 4/20; Batch:224/468; Training loss:0.109033\n",
      "Epoch: 4/20; Batch:225/468; Training loss:0.108131\n",
      "Epoch: 4/20; Batch:226/468; Training loss:0.111008\n",
      "Epoch: 4/20; Batch:227/468; Training loss:0.11001\n",
      "Epoch: 4/20; Batch:228/468; Training loss:0.107693\n",
      "Epoch: 4/20; Batch:229/468; Training loss:0.110019\n",
      "Epoch: 4/20; Batch:230/468; Training loss:0.104835\n",
      "Epoch: 4/20; Batch:231/468; Training loss:0.110013\n",
      "Epoch: 4/20; Batch:232/468; Training loss:0.109742\n",
      "Epoch: 4/20; Batch:233/468; Training loss:0.11111\n",
      "Epoch: 4/20; Batch:234/468; Training loss:0.108195\n",
      "Epoch: 4/20; Batch:235/468; Training loss:0.108416\n",
      "Epoch: 4/20; Batch:236/468; Training loss:0.113229\n",
      "Epoch: 4/20; Batch:237/468; Training loss:0.109491\n",
      "Epoch: 4/20; Batch:238/468; Training loss:0.113952\n",
      "Epoch: 4/20; Batch:239/468; Training loss:0.10918\n",
      "Epoch: 4/20; Batch:240/468; Training loss:0.113921\n",
      "Epoch: 4/20; Batch:241/468; Training loss:0.109258\n",
      "Epoch: 4/20; Batch:242/468; Training loss:0.111974\n",
      "Epoch: 4/20; Batch:243/468; Training loss:0.110254\n",
      "Epoch: 4/20; Batch:244/468; Training loss:0.106421\n",
      "Epoch: 4/20; Batch:245/468; Training loss:0.106147\n",
      "Epoch: 4/20; Batch:246/468; Training loss:0.105247\n",
      "Epoch: 4/20; Batch:247/468; Training loss:0.110553\n",
      "Epoch: 4/20; Batch:248/468; Training loss:0.108138\n",
      "Epoch: 4/20; Batch:249/468; Training loss:0.113999\n",
      "Epoch: 4/20; Batch:250/468; Training loss:0.109881\n",
      "Epoch: 4/20; Batch:251/468; Training loss:0.112885\n",
      "Epoch: 4/20; Batch:252/468; Training loss:0.109597\n",
      "Epoch: 4/20; Batch:253/468; Training loss:0.107718\n",
      "Epoch: 4/20; Batch:254/468; Training loss:0.100287\n",
      "Epoch: 4/20; Batch:255/468; Training loss:0.113961\n",
      "Epoch: 4/20; Batch:256/468; Training loss:0.105758\n",
      "Epoch: 4/20; Batch:257/468; Training loss:0.10538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20; Batch:258/468; Training loss:0.106586\n",
      "Epoch: 4/20; Batch:259/468; Training loss:0.111447\n",
      "Epoch: 4/20; Batch:260/468; Training loss:0.111181\n",
      "Epoch: 4/20; Batch:261/468; Training loss:0.108287\n",
      "Epoch: 4/20; Batch:262/468; Training loss:0.110732\n",
      "Epoch: 4/20; Batch:263/468; Training loss:0.105876\n",
      "Epoch: 4/20; Batch:264/468; Training loss:0.108313\n",
      "Epoch: 4/20; Batch:265/468; Training loss:0.107142\n",
      "Epoch: 4/20; Batch:266/468; Training loss:0.107966\n",
      "Epoch: 4/20; Batch:267/468; Training loss:0.112166\n",
      "Epoch: 4/20; Batch:268/468; Training loss:0.106691\n",
      "Epoch: 4/20; Batch:269/468; Training loss:0.109315\n",
      "Epoch: 4/20; Batch:270/468; Training loss:0.109255\n",
      "Epoch: 4/20; Batch:271/468; Training loss:0.10802\n",
      "Epoch: 4/20; Batch:272/468; Training loss:0.109487\n",
      "Epoch: 4/20; Batch:273/468; Training loss:0.106955\n",
      "Epoch: 4/20; Batch:274/468; Training loss:0.10739\n",
      "Epoch: 4/20; Batch:275/468; Training loss:0.10834\n",
      "Epoch: 4/20; Batch:276/468; Training loss:0.105519\n",
      "Epoch: 4/20; Batch:277/468; Training loss:0.112509\n",
      "Epoch: 4/20; Batch:278/468; Training loss:0.110022\n",
      "Epoch: 4/20; Batch:279/468; Training loss:0.113157\n",
      "Epoch: 4/20; Batch:280/468; Training loss:0.107386\n",
      "Epoch: 4/20; Batch:281/468; Training loss:0.110029\n",
      "Epoch: 4/20; Batch:282/468; Training loss:0.1082\n",
      "Epoch: 4/20; Batch:283/468; Training loss:0.107804\n",
      "Epoch: 4/20; Batch:284/468; Training loss:0.108058\n",
      "Epoch: 4/20; Batch:285/468; Training loss:0.107331\n",
      "Epoch: 4/20; Batch:286/468; Training loss:0.106919\n",
      "Epoch: 4/20; Batch:287/468; Training loss:0.106883\n",
      "Epoch: 4/20; Batch:288/468; Training loss:0.107888\n",
      "Epoch: 4/20; Batch:289/468; Training loss:0.105392\n",
      "Epoch: 4/20; Batch:290/468; Training loss:0.11168\n",
      "Epoch: 4/20; Batch:291/468; Training loss:0.111691\n",
      "Epoch: 4/20; Batch:292/468; Training loss:0.107972\n",
      "Epoch: 4/20; Batch:293/468; Training loss:0.110007\n",
      "Epoch: 4/20; Batch:294/468; Training loss:0.111469\n",
      "Epoch: 4/20; Batch:295/468; Training loss:0.10957\n",
      "Epoch: 4/20; Batch:296/468; Training loss:0.111661\n",
      "Epoch: 4/20; Batch:297/468; Training loss:0.102249\n",
      "Epoch: 4/20; Batch:298/468; Training loss:0.113018\n",
      "Epoch: 4/20; Batch:299/468; Training loss:0.110599\n",
      "Epoch: 4/20; Batch:300/468; Training loss:0.104284\n",
      "Epoch: 4/20; Batch:301/468; Training loss:0.108533\n",
      "Epoch: 4/20; Batch:302/468; Training loss:0.110983\n",
      "Epoch: 4/20; Batch:303/468; Training loss:0.108205\n",
      "Epoch: 4/20; Batch:304/468; Training loss:0.112385\n",
      "Epoch: 4/20; Batch:305/468; Training loss:0.108897\n",
      "Epoch: 4/20; Batch:306/468; Training loss:0.107196\n",
      "Epoch: 4/20; Batch:307/468; Training loss:0.108912\n",
      "Epoch: 4/20; Batch:308/468; Training loss:0.106707\n",
      "Epoch: 4/20; Batch:309/468; Training loss:0.107408\n",
      "Epoch: 4/20; Batch:310/468; Training loss:0.0996226\n",
      "Epoch: 4/20; Batch:311/468; Training loss:0.109954\n",
      "Epoch: 4/20; Batch:312/468; Training loss:0.106216\n",
      "Epoch: 4/20; Batch:313/468; Training loss:0.108694\n",
      "Epoch: 4/20; Batch:314/468; Training loss:0.102125\n",
      "Epoch: 4/20; Batch:315/468; Training loss:0.106799\n",
      "Epoch: 4/20; Batch:316/468; Training loss:0.110279\n",
      "Epoch: 4/20; Batch:317/468; Training loss:0.109056\n",
      "Epoch: 4/20; Batch:318/468; Training loss:0.104327\n",
      "Epoch: 4/20; Batch:319/468; Training loss:0.111238\n",
      "Epoch: 4/20; Batch:320/468; Training loss:0.107663\n",
      "Epoch: 4/20; Batch:321/468; Training loss:0.107729\n",
      "Epoch: 4/20; Batch:322/468; Training loss:0.107881\n",
      "Epoch: 4/20; Batch:323/468; Training loss:0.109606\n",
      "Epoch: 4/20; Batch:324/468; Training loss:0.10637\n",
      "Epoch: 4/20; Batch:325/468; Training loss:0.108558\n",
      "Epoch: 4/20; Batch:326/468; Training loss:0.107318\n",
      "Epoch: 4/20; Batch:327/468; Training loss:0.107795\n",
      "Epoch: 4/20; Batch:328/468; Training loss:0.112711\n",
      "Epoch: 4/20; Batch:329/468; Training loss:0.113511\n",
      "Epoch: 4/20; Batch:330/468; Training loss:0.105428\n",
      "Epoch: 4/20; Batch:331/468; Training loss:0.10568\n",
      "Epoch: 4/20; Batch:332/468; Training loss:0.114057\n",
      "Epoch: 4/20; Batch:333/468; Training loss:0.108011\n",
      "Epoch: 4/20; Batch:334/468; Training loss:0.110329\n",
      "Epoch: 4/20; Batch:335/468; Training loss:0.107441\n",
      "Epoch: 4/20; Batch:336/468; Training loss:0.109598\n",
      "Epoch: 4/20; Batch:337/468; Training loss:0.103243\n",
      "Epoch: 4/20; Batch:338/468; Training loss:0.106683\n",
      "Epoch: 4/20; Batch:339/468; Training loss:0.107173\n",
      "Epoch: 4/20; Batch:340/468; Training loss:0.108142\n",
      "Epoch: 4/20; Batch:341/468; Training loss:0.106679\n",
      "Epoch: 4/20; Batch:342/468; Training loss:0.100962\n",
      "Epoch: 4/20; Batch:343/468; Training loss:0.109622\n",
      "Epoch: 4/20; Batch:344/468; Training loss:0.110964\n",
      "Epoch: 4/20; Batch:345/468; Training loss:0.111027\n",
      "Epoch: 4/20; Batch:346/468; Training loss:0.111635\n",
      "Epoch: 4/20; Batch:347/468; Training loss:0.10852\n",
      "Epoch: 4/20; Batch:348/468; Training loss:0.108076\n",
      "Epoch: 4/20; Batch:349/468; Training loss:0.108069\n",
      "Epoch: 4/20; Batch:350/468; Training loss:0.107765\n",
      "Epoch: 4/20; Batch:351/468; Training loss:0.107224\n",
      "Epoch: 4/20; Batch:352/468; Training loss:0.10951\n",
      "Epoch: 4/20; Batch:353/468; Training loss:0.109116\n",
      "Epoch: 4/20; Batch:354/468; Training loss:0.101301\n",
      "Epoch: 4/20; Batch:355/468; Training loss:0.110472\n",
      "Epoch: 4/20; Batch:356/468; Training loss:0.104176\n",
      "Epoch: 4/20; Batch:357/468; Training loss:0.108092\n",
      "Epoch: 4/20; Batch:358/468; Training loss:0.10676\n",
      "Epoch: 4/20; Batch:359/468; Training loss:0.108139\n",
      "Epoch: 4/20; Batch:360/468; Training loss:0.105698\n",
      "Epoch: 4/20; Batch:361/468; Training loss:0.109254\n",
      "Epoch: 4/20; Batch:362/468; Training loss:0.104959\n",
      "Epoch: 4/20; Batch:363/468; Training loss:0.106574\n",
      "Epoch: 4/20; Batch:364/468; Training loss:0.114756\n",
      "Epoch: 4/20; Batch:365/468; Training loss:0.108826\n",
      "Epoch: 4/20; Batch:366/468; Training loss:0.106374\n",
      "Epoch: 4/20; Batch:367/468; Training loss:0.104019\n",
      "Epoch: 4/20; Batch:368/468; Training loss:0.106458\n",
      "Epoch: 4/20; Batch:369/468; Training loss:0.109766\n",
      "Epoch: 4/20; Batch:370/468; Training loss:0.110494\n",
      "Epoch: 4/20; Batch:371/468; Training loss:0.106391\n",
      "Epoch: 4/20; Batch:372/468; Training loss:0.110255\n",
      "Epoch: 4/20; Batch:373/468; Training loss:0.109338\n",
      "Epoch: 4/20; Batch:374/468; Training loss:0.106017\n",
      "Epoch: 4/20; Batch:375/468; Training loss:0.106927\n",
      "Epoch: 4/20; Batch:376/468; Training loss:0.108595\n",
      "Epoch: 4/20; Batch:377/468; Training loss:0.107215\n",
      "Epoch: 4/20; Batch:378/468; Training loss:0.10596\n",
      "Epoch: 4/20; Batch:379/468; Training loss:0.107453\n",
      "Epoch: 4/20; Batch:380/468; Training loss:0.107658\n",
      "Epoch: 4/20; Batch:381/468; Training loss:0.106555\n",
      "Epoch: 4/20; Batch:382/468; Training loss:0.110737\n",
      "Epoch: 4/20; Batch:383/468; Training loss:0.104107\n",
      "Epoch: 4/20; Batch:384/468; Training loss:0.107964\n",
      "Epoch: 4/20; Batch:385/468; Training loss:0.106692\n",
      "Epoch: 4/20; Batch:386/468; Training loss:0.10977\n",
      "Epoch: 4/20; Batch:387/468; Training loss:0.109174\n",
      "Epoch: 4/20; Batch:388/468; Training loss:0.107861\n",
      "Epoch: 4/20; Batch:389/468; Training loss:0.112032\n",
      "Epoch: 4/20; Batch:390/468; Training loss:0.108195\n",
      "Epoch: 4/20; Batch:391/468; Training loss:0.108426\n",
      "Epoch: 4/20; Batch:392/468; Training loss:0.11007\n",
      "Epoch: 4/20; Batch:393/468; Training loss:0.106869\n",
      "Epoch: 4/20; Batch:394/468; Training loss:0.104812\n",
      "Epoch: 4/20; Batch:395/468; Training loss:0.107491\n",
      "Epoch: 4/20; Batch:396/468; Training loss:0.111755\n",
      "Epoch: 4/20; Batch:397/468; Training loss:0.107018\n",
      "Epoch: 4/20; Batch:398/468; Training loss:0.107079\n",
      "Epoch: 4/20; Batch:399/468; Training loss:0.109498\n",
      "Epoch: 4/20; Batch:400/468; Training loss:0.108773\n",
      "Epoch: 4/20; Batch:401/468; Training loss:0.110667\n",
      "Epoch: 4/20; Batch:402/468; Training loss:0.10817\n",
      "Epoch: 4/20; Batch:403/468; Training loss:0.109771\n",
      "Epoch: 4/20; Batch:404/468; Training loss:0.108092\n",
      "Epoch: 4/20; Batch:405/468; Training loss:0.104883\n",
      "Epoch: 4/20; Batch:406/468; Training loss:0.104532\n",
      "Epoch: 4/20; Batch:407/468; Training loss:0.10621\n",
      "Epoch: 4/20; Batch:408/468; Training loss:0.107842\n",
      "Epoch: 4/20; Batch:409/468; Training loss:0.109606\n",
      "Epoch: 4/20; Batch:410/468; Training loss:0.105037\n",
      "Epoch: 4/20; Batch:411/468; Training loss:0.108255\n",
      "Epoch: 4/20; Batch:412/468; Training loss:0.108026\n",
      "Epoch: 4/20; Batch:413/468; Training loss:0.107144\n",
      "Epoch: 4/20; Batch:414/468; Training loss:0.108994\n",
      "Epoch: 4/20; Batch:415/468; Training loss:0.104045\n",
      "Epoch: 4/20; Batch:416/468; Training loss:0.110641\n",
      "Epoch: 4/20; Batch:417/468; Training loss:0.107837\n",
      "Epoch: 4/20; Batch:418/468; Training loss:0.109764\n",
      "Epoch: 4/20; Batch:419/468; Training loss:0.108966\n",
      "Epoch: 4/20; Batch:420/468; Training loss:0.111387\n",
      "Epoch: 4/20; Batch:421/468; Training loss:0.105747\n",
      "Epoch: 4/20; Batch:422/468; Training loss:0.109427\n",
      "Epoch: 4/20; Batch:423/468; Training loss:0.106961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20; Batch:424/468; Training loss:0.104195\n",
      "Epoch: 4/20; Batch:425/468; Training loss:0.108063\n",
      "Epoch: 4/20; Batch:426/468; Training loss:0.106746\n",
      "Epoch: 4/20; Batch:427/468; Training loss:0.106858\n",
      "Epoch: 4/20; Batch:428/468; Training loss:0.108747\n",
      "Epoch: 4/20; Batch:429/468; Training loss:0.109899\n",
      "Epoch: 4/20; Batch:430/468; Training loss:0.106351\n",
      "Epoch: 4/20; Batch:431/468; Training loss:0.110116\n",
      "Epoch: 4/20; Batch:432/468; Training loss:0.105637\n",
      "Epoch: 4/20; Batch:433/468; Training loss:0.109381\n",
      "Epoch: 4/20; Batch:434/468; Training loss:0.104753\n",
      "Epoch: 4/20; Batch:435/468; Training loss:0.108394\n",
      "Epoch: 4/20; Batch:436/468; Training loss:0.110796\n",
      "Epoch: 4/20; Batch:437/468; Training loss:0.10678\n",
      "Epoch: 4/20; Batch:438/468; Training loss:0.105092\n",
      "Epoch: 4/20; Batch:439/468; Training loss:0.107019\n",
      "Epoch: 4/20; Batch:440/468; Training loss:0.102221\n",
      "Epoch: 4/20; Batch:441/468; Training loss:0.108738\n",
      "Epoch: 4/20; Batch:442/468; Training loss:0.105793\n",
      "Epoch: 4/20; Batch:443/468; Training loss:0.11201\n",
      "Epoch: 4/20; Batch:444/468; Training loss:0.101657\n",
      "Epoch: 4/20; Batch:445/468; Training loss:0.10938\n",
      "Epoch: 4/20; Batch:446/468; Training loss:0.110343\n",
      "Epoch: 4/20; Batch:447/468; Training loss:0.107486\n",
      "Epoch: 4/20; Batch:448/468; Training loss:0.107047\n",
      "Epoch: 4/20; Batch:449/468; Training loss:0.103307\n",
      "Epoch: 4/20; Batch:450/468; Training loss:0.107397\n",
      "Epoch: 4/20; Batch:451/468; Training loss:0.105542\n",
      "Epoch: 4/20; Batch:452/468; Training loss:0.109751\n",
      "Epoch: 4/20; Batch:453/468; Training loss:0.106487\n",
      "Epoch: 4/20; Batch:454/468; Training loss:0.111945\n",
      "Epoch: 4/20; Batch:455/468; Training loss:0.108937\n",
      "Epoch: 4/20; Batch:456/468; Training loss:0.106044\n",
      "Epoch: 4/20; Batch:457/468; Training loss:0.104315\n",
      "Epoch: 4/20; Batch:458/468; Training loss:0.106084\n",
      "Epoch: 4/20; Batch:459/468; Training loss:0.108668\n",
      "Epoch: 4/20; Batch:460/468; Training loss:0.107622\n",
      "Epoch: 4/20; Batch:461/468; Training loss:0.101981\n",
      "Epoch: 4/20; Batch:462/468; Training loss:0.104385\n",
      "Epoch: 4/20; Batch:463/468; Training loss:0.10357\n",
      "Epoch: 4/20; Batch:464/468; Training loss:0.111313\n",
      "Epoch: 4/20; Batch:465/468; Training loss:0.103426\n",
      "Epoch: 4/20; Batch:466/468; Training loss:0.109406\n",
      "Epoch: 4/20; Batch:467/468; Training loss:0.107652\n",
      "Epoch: 4/20; Batch:468/468; Training loss:0.108216\n",
      "Epoch: 5/20; Batch:1/468; Training loss:0.10103\n",
      "Epoch: 5/20; Batch:2/468; Training loss:0.108398\n",
      "Epoch: 5/20; Batch:3/468; Training loss:0.108001\n",
      "Epoch: 5/20; Batch:4/468; Training loss:0.103696\n",
      "Epoch: 5/20; Batch:5/468; Training loss:0.108049\n",
      "Epoch: 5/20; Batch:6/468; Training loss:0.108139\n",
      "Epoch: 5/20; Batch:7/468; Training loss:0.108141\n",
      "Epoch: 5/20; Batch:8/468; Training loss:0.103512\n",
      "Epoch: 5/20; Batch:9/468; Training loss:0.107233\n",
      "Epoch: 5/20; Batch:10/468; Training loss:0.108472\n",
      "Epoch: 5/20; Batch:11/468; Training loss:0.105474\n",
      "Epoch: 5/20; Batch:12/468; Training loss:0.108105\n",
      "Epoch: 5/20; Batch:13/468; Training loss:0.11\n",
      "Epoch: 5/20; Batch:14/468; Training loss:0.108786\n",
      "Epoch: 5/20; Batch:15/468; Training loss:0.103702\n",
      "Epoch: 5/20; Batch:16/468; Training loss:0.106778\n",
      "Epoch: 5/20; Batch:17/468; Training loss:0.110572\n",
      "Epoch: 5/20; Batch:18/468; Training loss:0.108746\n",
      "Epoch: 5/20; Batch:19/468; Training loss:0.105744\n",
      "Epoch: 5/20; Batch:20/468; Training loss:0.104087\n",
      "Epoch: 5/20; Batch:21/468; Training loss:0.10771\n",
      "Epoch: 5/20; Batch:22/468; Training loss:0.10919\n",
      "Epoch: 5/20; Batch:23/468; Training loss:0.108671\n",
      "Epoch: 5/20; Batch:24/468; Training loss:0.111908\n",
      "Epoch: 5/20; Batch:25/468; Training loss:0.105916\n",
      "Epoch: 5/20; Batch:26/468; Training loss:0.109226\n",
      "Epoch: 5/20; Batch:27/468; Training loss:0.107993\n",
      "Epoch: 5/20; Batch:28/468; Training loss:0.105726\n",
      "Epoch: 5/20; Batch:29/468; Training loss:0.106544\n",
      "Epoch: 5/20; Batch:30/468; Training loss:0.104935\n",
      "Epoch: 5/20; Batch:31/468; Training loss:0.107758\n",
      "Epoch: 5/20; Batch:32/468; Training loss:0.110665\n",
      "Epoch: 5/20; Batch:33/468; Training loss:0.104871\n",
      "Epoch: 5/20; Batch:34/468; Training loss:0.106332\n",
      "Epoch: 5/20; Batch:35/468; Training loss:0.103474\n",
      "Epoch: 5/20; Batch:36/468; Training loss:0.107398\n",
      "Epoch: 5/20; Batch:37/468; Training loss:0.105925\n",
      "Epoch: 5/20; Batch:38/468; Training loss:0.107561\n",
      "Epoch: 5/20; Batch:39/468; Training loss:0.108349\n",
      "Epoch: 5/20; Batch:40/468; Training loss:0.106941\n",
      "Epoch: 5/20; Batch:41/468; Training loss:0.110104\n",
      "Epoch: 5/20; Batch:42/468; Training loss:0.106942\n",
      "Epoch: 5/20; Batch:43/468; Training loss:0.108258\n",
      "Epoch: 5/20; Batch:44/468; Training loss:0.105043\n",
      "Epoch: 5/20; Batch:45/468; Training loss:0.109097\n",
      "Epoch: 5/20; Batch:46/468; Training loss:0.108507\n",
      "Epoch: 5/20; Batch:47/468; Training loss:0.110973\n",
      "Epoch: 5/20; Batch:48/468; Training loss:0.106161\n",
      "Epoch: 5/20; Batch:49/468; Training loss:0.108831\n",
      "Epoch: 5/20; Batch:50/468; Training loss:0.105481\n",
      "Epoch: 5/20; Batch:51/468; Training loss:0.106891\n",
      "Epoch: 5/20; Batch:52/468; Training loss:0.108056\n",
      "Epoch: 5/20; Batch:53/468; Training loss:0.104704\n",
      "Epoch: 5/20; Batch:54/468; Training loss:0.106533\n",
      "Epoch: 5/20; Batch:55/468; Training loss:0.104143\n",
      "Epoch: 5/20; Batch:56/468; Training loss:0.108565\n",
      "Epoch: 5/20; Batch:57/468; Training loss:0.107497\n",
      "Epoch: 5/20; Batch:58/468; Training loss:0.104164\n",
      "Epoch: 5/20; Batch:59/468; Training loss:0.103746\n",
      "Epoch: 5/20; Batch:60/468; Training loss:0.107704\n",
      "Epoch: 5/20; Batch:61/468; Training loss:0.106591\n",
      "Epoch: 5/20; Batch:62/468; Training loss:0.10486\n",
      "Epoch: 5/20; Batch:63/468; Training loss:0.108206\n",
      "Epoch: 5/20; Batch:64/468; Training loss:0.110169\n",
      "Epoch: 5/20; Batch:65/468; Training loss:0.104834\n",
      "Epoch: 5/20; Batch:66/468; Training loss:0.107532\n",
      "Epoch: 5/20; Batch:67/468; Training loss:0.110024\n",
      "Epoch: 5/20; Batch:68/468; Training loss:0.107321\n",
      "Epoch: 5/20; Batch:69/468; Training loss:0.106882\n",
      "Epoch: 5/20; Batch:70/468; Training loss:0.105588\n",
      "Epoch: 5/20; Batch:71/468; Training loss:0.101483\n",
      "Epoch: 5/20; Batch:72/468; Training loss:0.105615\n",
      "Epoch: 5/20; Batch:73/468; Training loss:0.109099\n",
      "Epoch: 5/20; Batch:74/468; Training loss:0.104094\n",
      "Epoch: 5/20; Batch:75/468; Training loss:0.108292\n",
      "Epoch: 5/20; Batch:76/468; Training loss:0.108525\n",
      "Epoch: 5/20; Batch:77/468; Training loss:0.110948\n",
      "Epoch: 5/20; Batch:78/468; Training loss:0.10794\n",
      "Epoch: 5/20; Batch:79/468; Training loss:0.102871\n",
      "Epoch: 5/20; Batch:80/468; Training loss:0.105665\n",
      "Epoch: 5/20; Batch:81/468; Training loss:0.107641\n",
      "Epoch: 5/20; Batch:82/468; Training loss:0.102577\n",
      "Epoch: 5/20; Batch:83/468; Training loss:0.103905\n",
      "Epoch: 5/20; Batch:84/468; Training loss:0.108009\n",
      "Epoch: 5/20; Batch:85/468; Training loss:0.104439\n",
      "Epoch: 5/20; Batch:86/468; Training loss:0.104295\n",
      "Epoch: 5/20; Batch:87/468; Training loss:0.110673\n",
      "Epoch: 5/20; Batch:88/468; Training loss:0.108155\n",
      "Epoch: 5/20; Batch:89/468; Training loss:0.109335\n",
      "Epoch: 5/20; Batch:90/468; Training loss:0.107468\n",
      "Epoch: 5/20; Batch:91/468; Training loss:0.103058\n",
      "Epoch: 5/20; Batch:92/468; Training loss:0.107716\n",
      "Epoch: 5/20; Batch:93/468; Training loss:0.109863\n",
      "Epoch: 5/20; Batch:94/468; Training loss:0.106527\n",
      "Epoch: 5/20; Batch:95/468; Training loss:0.100713\n",
      "Epoch: 5/20; Batch:96/468; Training loss:0.105694\n",
      "Epoch: 5/20; Batch:97/468; Training loss:0.102435\n",
      "Epoch: 5/20; Batch:98/468; Training loss:0.106514\n",
      "Epoch: 5/20; Batch:99/468; Training loss:0.106092\n",
      "Epoch: 5/20; Batch:100/468; Training loss:0.109732\n",
      "Epoch: 5/20; Batch:101/468; Training loss:0.106583\n",
      "Epoch: 5/20; Batch:102/468; Training loss:0.104224\n",
      "Epoch: 5/20; Batch:103/468; Training loss:0.104727\n",
      "Epoch: 5/20; Batch:104/468; Training loss:0.105354\n",
      "Epoch: 5/20; Batch:105/468; Training loss:0.104504\n",
      "Epoch: 5/20; Batch:106/468; Training loss:0.108875\n",
      "Epoch: 5/20; Batch:107/468; Training loss:0.103938\n",
      "Epoch: 5/20; Batch:108/468; Training loss:0.1017\n",
      "Epoch: 5/20; Batch:109/468; Training loss:0.10712\n",
      "Epoch: 5/20; Batch:110/468; Training loss:0.105422\n",
      "Epoch: 5/20; Batch:111/468; Training loss:0.110348\n",
      "Epoch: 5/20; Batch:112/468; Training loss:0.105729\n",
      "Epoch: 5/20; Batch:113/468; Training loss:0.105221\n",
      "Epoch: 5/20; Batch:114/468; Training loss:0.105597\n",
      "Epoch: 5/20; Batch:115/468; Training loss:0.106742\n",
      "Epoch: 5/20; Batch:116/468; Training loss:0.107427\n",
      "Epoch: 5/20; Batch:117/468; Training loss:0.106307\n",
      "Epoch: 5/20; Batch:118/468; Training loss:0.109507\n",
      "Epoch: 5/20; Batch:119/468; Training loss:0.104037\n",
      "Epoch: 5/20; Batch:120/468; Training loss:0.107754\n",
      "Epoch: 5/20; Batch:121/468; Training loss:0.0989693\n",
      "Epoch: 5/20; Batch:122/468; Training loss:0.103565\n",
      "Epoch: 5/20; Batch:123/468; Training loss:0.106056\n",
      "Epoch: 5/20; Batch:124/468; Training loss:0.107034\n",
      "Epoch: 5/20; Batch:125/468; Training loss:0.105659\n",
      "Epoch: 5/20; Batch:126/468; Training loss:0.107406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20; Batch:127/468; Training loss:0.104396\n",
      "Epoch: 5/20; Batch:128/468; Training loss:0.108712\n",
      "Epoch: 5/20; Batch:129/468; Training loss:0.107545\n",
      "Epoch: 5/20; Batch:130/468; Training loss:0.106505\n",
      "Epoch: 5/20; Batch:131/468; Training loss:0.108747\n",
      "Epoch: 5/20; Batch:132/468; Training loss:0.106135\n",
      "Epoch: 5/20; Batch:133/468; Training loss:0.107659\n",
      "Epoch: 5/20; Batch:134/468; Training loss:0.104405\n",
      "Epoch: 5/20; Batch:135/468; Training loss:0.106191\n",
      "Epoch: 5/20; Batch:136/468; Training loss:0.107683\n",
      "Epoch: 5/20; Batch:137/468; Training loss:0.113519\n",
      "Epoch: 5/20; Batch:138/468; Training loss:0.103316\n",
      "Epoch: 5/20; Batch:139/468; Training loss:0.104227\n",
      "Epoch: 5/20; Batch:140/468; Training loss:0.107782\n",
      "Epoch: 5/20; Batch:141/468; Training loss:0.110082\n",
      "Epoch: 5/20; Batch:142/468; Training loss:0.111641\n",
      "Epoch: 5/20; Batch:143/468; Training loss:0.102203\n",
      "Epoch: 5/20; Batch:144/468; Training loss:0.103924\n",
      "Epoch: 5/20; Batch:145/468; Training loss:0.109027\n",
      "Epoch: 5/20; Batch:146/468; Training loss:0.110194\n",
      "Epoch: 5/20; Batch:147/468; Training loss:0.109238\n",
      "Epoch: 5/20; Batch:148/468; Training loss:0.106588\n",
      "Epoch: 5/20; Batch:149/468; Training loss:0.108394\n",
      "Epoch: 5/20; Batch:150/468; Training loss:0.106584\n",
      "Epoch: 5/20; Batch:151/468; Training loss:0.101372\n",
      "Epoch: 5/20; Batch:152/468; Training loss:0.104286\n",
      "Epoch: 5/20; Batch:153/468; Training loss:0.108424\n",
      "Epoch: 5/20; Batch:154/468; Training loss:0.109882\n",
      "Epoch: 5/20; Batch:155/468; Training loss:0.104214\n",
      "Epoch: 5/20; Batch:156/468; Training loss:0.107283\n",
      "Epoch: 5/20; Batch:157/468; Training loss:0.102418\n",
      "Epoch: 5/20; Batch:158/468; Training loss:0.103472\n",
      "Epoch: 5/20; Batch:159/468; Training loss:0.10799\n",
      "Epoch: 5/20; Batch:160/468; Training loss:0.108152\n",
      "Epoch: 5/20; Batch:161/468; Training loss:0.108485\n",
      "Epoch: 5/20; Batch:162/468; Training loss:0.107116\n",
      "Epoch: 5/20; Batch:163/468; Training loss:0.105691\n",
      "Epoch: 5/20; Batch:164/468; Training loss:0.104073\n",
      "Epoch: 5/20; Batch:165/468; Training loss:0.107531\n",
      "Epoch: 5/20; Batch:166/468; Training loss:0.104016\n",
      "Epoch: 5/20; Batch:167/468; Training loss:0.103916\n",
      "Epoch: 5/20; Batch:168/468; Training loss:0.105724\n",
      "Epoch: 5/20; Batch:169/468; Training loss:0.103609\n",
      "Epoch: 5/20; Batch:170/468; Training loss:0.107437\n",
      "Epoch: 5/20; Batch:171/468; Training loss:0.101007\n",
      "Epoch: 5/20; Batch:172/468; Training loss:0.10542\n",
      "Epoch: 5/20; Batch:173/468; Training loss:0.105656\n",
      "Epoch: 5/20; Batch:174/468; Training loss:0.103888\n",
      "Epoch: 5/20; Batch:175/468; Training loss:0.110248\n",
      "Epoch: 5/20; Batch:176/468; Training loss:0.107811\n",
      "Epoch: 5/20; Batch:177/468; Training loss:0.106791\n",
      "Epoch: 5/20; Batch:178/468; Training loss:0.106885\n",
      "Epoch: 5/20; Batch:179/468; Training loss:0.101411\n",
      "Epoch: 5/20; Batch:180/468; Training loss:0.108699\n",
      "Epoch: 5/20; Batch:181/468; Training loss:0.103315\n",
      "Epoch: 5/20; Batch:182/468; Training loss:0.104488\n",
      "Epoch: 5/20; Batch:183/468; Training loss:0.103883\n",
      "Epoch: 5/20; Batch:184/468; Training loss:0.105342\n",
      "Epoch: 5/20; Batch:185/468; Training loss:0.106329\n",
      "Epoch: 5/20; Batch:186/468; Training loss:0.106694\n",
      "Epoch: 5/20; Batch:187/468; Training loss:0.105342\n",
      "Epoch: 5/20; Batch:188/468; Training loss:0.110987\n",
      "Epoch: 5/20; Batch:189/468; Training loss:0.104517\n",
      "Epoch: 5/20; Batch:190/468; Training loss:0.104325\n",
      "Epoch: 5/20; Batch:191/468; Training loss:0.10726\n",
      "Epoch: 5/20; Batch:192/468; Training loss:0.10864\n",
      "Epoch: 5/20; Batch:193/468; Training loss:0.108037\n",
      "Epoch: 5/20; Batch:194/468; Training loss:0.106145\n",
      "Epoch: 5/20; Batch:195/468; Training loss:0.107232\n",
      "Epoch: 5/20; Batch:196/468; Training loss:0.110163\n",
      "Epoch: 5/20; Batch:197/468; Training loss:0.107861\n",
      "Epoch: 5/20; Batch:198/468; Training loss:0.102421\n",
      "Epoch: 5/20; Batch:199/468; Training loss:0.107503\n",
      "Epoch: 5/20; Batch:200/468; Training loss:0.10897\n",
      "Epoch: 5/20; Batch:201/468; Training loss:0.103625\n",
      "Epoch: 5/20; Batch:202/468; Training loss:0.104675\n",
      "Epoch: 5/20; Batch:203/468; Training loss:0.106304\n",
      "Epoch: 5/20; Batch:204/468; Training loss:0.109201\n",
      "Epoch: 5/20; Batch:205/468; Training loss:0.107208\n",
      "Epoch: 5/20; Batch:206/468; Training loss:0.108565\n",
      "Epoch: 5/20; Batch:207/468; Training loss:0.110704\n",
      "Epoch: 5/20; Batch:208/468; Training loss:0.107703\n",
      "Epoch: 5/20; Batch:209/468; Training loss:0.103153\n",
      "Epoch: 5/20; Batch:210/468; Training loss:0.105878\n",
      "Epoch: 5/20; Batch:211/468; Training loss:0.109675\n",
      "Epoch: 5/20; Batch:212/468; Training loss:0.102959\n",
      "Epoch: 5/20; Batch:213/468; Training loss:0.100346\n",
      "Epoch: 5/20; Batch:214/468; Training loss:0.107882\n",
      "Epoch: 5/20; Batch:215/468; Training loss:0.10897\n",
      "Epoch: 5/20; Batch:216/468; Training loss:0.108099\n",
      "Epoch: 5/20; Batch:217/468; Training loss:0.110327\n",
      "Epoch: 5/20; Batch:218/468; Training loss:0.105398\n",
      "Epoch: 5/20; Batch:219/468; Training loss:0.107483\n",
      "Epoch: 5/20; Batch:220/468; Training loss:0.107637\n",
      "Epoch: 5/20; Batch:221/468; Training loss:0.103707\n",
      "Epoch: 5/20; Batch:222/468; Training loss:0.103367\n",
      "Epoch: 5/20; Batch:223/468; Training loss:0.104902\n",
      "Epoch: 5/20; Batch:224/468; Training loss:0.107173\n",
      "Epoch: 5/20; Batch:225/468; Training loss:0.104491\n",
      "Epoch: 5/20; Batch:226/468; Training loss:0.104759\n",
      "Epoch: 5/20; Batch:227/468; Training loss:0.106545\n",
      "Epoch: 5/20; Batch:228/468; Training loss:0.110008\n",
      "Epoch: 5/20; Batch:229/468; Training loss:0.105592\n",
      "Epoch: 5/20; Batch:230/468; Training loss:0.107436\n",
      "Epoch: 5/20; Batch:231/468; Training loss:0.107564\n",
      "Epoch: 5/20; Batch:232/468; Training loss:0.106443\n",
      "Epoch: 5/20; Batch:233/468; Training loss:0.107695\n",
      "Epoch: 5/20; Batch:234/468; Training loss:0.105651\n",
      "Epoch: 5/20; Batch:235/468; Training loss:0.107312\n",
      "Epoch: 5/20; Batch:236/468; Training loss:0.104098\n",
      "Epoch: 5/20; Batch:237/468; Training loss:0.10907\n",
      "Epoch: 5/20; Batch:238/468; Training loss:0.106057\n",
      "Epoch: 5/20; Batch:239/468; Training loss:0.10646\n",
      "Epoch: 5/20; Batch:240/468; Training loss:0.105778\n",
      "Epoch: 5/20; Batch:241/468; Training loss:0.107397\n",
      "Epoch: 5/20; Batch:242/468; Training loss:0.105352\n",
      "Epoch: 5/20; Batch:243/468; Training loss:0.105922\n",
      "Epoch: 5/20; Batch:244/468; Training loss:0.103528\n",
      "Epoch: 5/20; Batch:245/468; Training loss:0.102392\n",
      "Epoch: 5/20; Batch:246/468; Training loss:0.1057\n",
      "Epoch: 5/20; Batch:247/468; Training loss:0.105455\n",
      "Epoch: 5/20; Batch:248/468; Training loss:0.10687\n",
      "Epoch: 5/20; Batch:249/468; Training loss:0.105724\n",
      "Epoch: 5/20; Batch:250/468; Training loss:0.103642\n",
      "Epoch: 5/20; Batch:251/468; Training loss:0.0995033\n",
      "Epoch: 5/20; Batch:252/468; Training loss:0.109081\n",
      "Epoch: 5/20; Batch:253/468; Training loss:0.107746\n",
      "Epoch: 5/20; Batch:254/468; Training loss:0.107815\n",
      "Epoch: 5/20; Batch:255/468; Training loss:0.108197\n",
      "Epoch: 5/20; Batch:256/468; Training loss:0.105527\n",
      "Epoch: 5/20; Batch:257/468; Training loss:0.11151\n",
      "Epoch: 5/20; Batch:258/468; Training loss:0.106761\n",
      "Epoch: 5/20; Batch:259/468; Training loss:0.105815\n",
      "Epoch: 5/20; Batch:260/468; Training loss:0.105888\n",
      "Epoch: 5/20; Batch:261/468; Training loss:0.105146\n",
      "Epoch: 5/20; Batch:262/468; Training loss:0.104276\n",
      "Epoch: 5/20; Batch:263/468; Training loss:0.102706\n",
      "Epoch: 5/20; Batch:264/468; Training loss:0.104955\n",
      "Epoch: 5/20; Batch:265/468; Training loss:0.102851\n",
      "Epoch: 5/20; Batch:266/468; Training loss:0.10322\n",
      "Epoch: 5/20; Batch:267/468; Training loss:0.104352\n",
      "Epoch: 5/20; Batch:268/468; Training loss:0.102347\n",
      "Epoch: 5/20; Batch:269/468; Training loss:0.106855\n",
      "Epoch: 5/20; Batch:270/468; Training loss:0.100904\n",
      "Epoch: 5/20; Batch:271/468; Training loss:0.103741\n",
      "Epoch: 5/20; Batch:272/468; Training loss:0.103111\n",
      "Epoch: 5/20; Batch:273/468; Training loss:0.105871\n",
      "Epoch: 5/20; Batch:274/468; Training loss:0.107619\n",
      "Epoch: 5/20; Batch:275/468; Training loss:0.105987\n",
      "Epoch: 5/20; Batch:276/468; Training loss:0.106257\n",
      "Epoch: 5/20; Batch:277/468; Training loss:0.109727\n",
      "Epoch: 5/20; Batch:278/468; Training loss:0.10721\n",
      "Epoch: 5/20; Batch:279/468; Training loss:0.104503\n",
      "Epoch: 5/20; Batch:280/468; Training loss:0.103358\n",
      "Epoch: 5/20; Batch:281/468; Training loss:0.103935\n",
      "Epoch: 5/20; Batch:282/468; Training loss:0.109891\n",
      "Epoch: 5/20; Batch:283/468; Training loss:0.103426\n",
      "Epoch: 5/20; Batch:284/468; Training loss:0.105484\n",
      "Epoch: 5/20; Batch:285/468; Training loss:0.106453\n",
      "Epoch: 5/20; Batch:286/468; Training loss:0.104883\n",
      "Epoch: 5/20; Batch:287/468; Training loss:0.104853\n",
      "Epoch: 5/20; Batch:288/468; Training loss:0.103665\n",
      "Epoch: 5/20; Batch:289/468; Training loss:0.107349\n",
      "Epoch: 5/20; Batch:290/468; Training loss:0.104829\n",
      "Epoch: 5/20; Batch:291/468; Training loss:0.104133\n",
      "Epoch: 5/20; Batch:292/468; Training loss:0.107717\n",
      "Epoch: 5/20; Batch:293/468; Training loss:0.10421\n",
      "Epoch: 5/20; Batch:294/468; Training loss:0.105487\n",
      "Epoch: 5/20; Batch:295/468; Training loss:0.104284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20; Batch:296/468; Training loss:0.102585\n",
      "Epoch: 5/20; Batch:297/468; Training loss:0.106215\n",
      "Epoch: 5/20; Batch:298/468; Training loss:0.109712\n",
      "Epoch: 5/20; Batch:299/468; Training loss:0.104419\n",
      "Epoch: 5/20; Batch:300/468; Training loss:0.102928\n",
      "Epoch: 5/20; Batch:301/468; Training loss:0.106098\n",
      "Epoch: 5/20; Batch:302/468; Training loss:0.100608\n",
      "Epoch: 5/20; Batch:303/468; Training loss:0.107908\n",
      "Epoch: 5/20; Batch:304/468; Training loss:0.104012\n",
      "Epoch: 5/20; Batch:305/468; Training loss:0.101718\n",
      "Epoch: 5/20; Batch:306/468; Training loss:0.104217\n",
      "Epoch: 5/20; Batch:307/468; Training loss:0.105976\n",
      "Epoch: 5/20; Batch:308/468; Training loss:0.106804\n",
      "Epoch: 5/20; Batch:309/468; Training loss:0.11019\n",
      "Epoch: 5/20; Batch:310/468; Training loss:0.108348\n",
      "Epoch: 5/20; Batch:311/468; Training loss:0.105621\n",
      "Epoch: 5/20; Batch:312/468; Training loss:0.102092\n",
      "Epoch: 5/20; Batch:313/468; Training loss:0.107915\n",
      "Epoch: 5/20; Batch:314/468; Training loss:0.106598\n",
      "Epoch: 5/20; Batch:315/468; Training loss:0.109739\n",
      "Epoch: 5/20; Batch:316/468; Training loss:0.105084\n",
      "Epoch: 5/20; Batch:317/468; Training loss:0.0993033\n",
      "Epoch: 5/20; Batch:318/468; Training loss:0.0999202\n",
      "Epoch: 5/20; Batch:319/468; Training loss:0.102615\n",
      "Epoch: 5/20; Batch:320/468; Training loss:0.10802\n",
      "Epoch: 5/20; Batch:321/468; Training loss:0.10358\n",
      "Epoch: 5/20; Batch:322/468; Training loss:0.104385\n",
      "Epoch: 5/20; Batch:323/468; Training loss:0.101174\n",
      "Epoch: 5/20; Batch:324/468; Training loss:0.108813\n",
      "Epoch: 5/20; Batch:325/468; Training loss:0.105985\n",
      "Epoch: 5/20; Batch:326/468; Training loss:0.109996\n",
      "Epoch: 5/20; Batch:327/468; Training loss:0.0981778\n",
      "Epoch: 5/20; Batch:328/468; Training loss:0.105217\n",
      "Epoch: 5/20; Batch:329/468; Training loss:0.105143\n",
      "Epoch: 5/20; Batch:330/468; Training loss:0.101431\n",
      "Epoch: 5/20; Batch:331/468; Training loss:0.109642\n",
      "Epoch: 5/20; Batch:332/468; Training loss:0.105237\n",
      "Epoch: 5/20; Batch:333/468; Training loss:0.102536\n",
      "Epoch: 5/20; Batch:334/468; Training loss:0.103077\n",
      "Epoch: 5/20; Batch:335/468; Training loss:0.10426\n",
      "Epoch: 5/20; Batch:336/468; Training loss:0.101894\n",
      "Epoch: 5/20; Batch:337/468; Training loss:0.103252\n",
      "Epoch: 5/20; Batch:338/468; Training loss:0.10938\n",
      "Epoch: 5/20; Batch:339/468; Training loss:0.104186\n",
      "Epoch: 5/20; Batch:340/468; Training loss:0.106868\n",
      "Epoch: 5/20; Batch:341/468; Training loss:0.109739\n",
      "Epoch: 5/20; Batch:342/468; Training loss:0.108065\n",
      "Epoch: 5/20; Batch:343/468; Training loss:0.109796\n",
      "Epoch: 5/20; Batch:344/468; Training loss:0.105445\n",
      "Epoch: 5/20; Batch:345/468; Training loss:0.10304\n",
      "Epoch: 5/20; Batch:346/468; Training loss:0.102175\n",
      "Epoch: 5/20; Batch:347/468; Training loss:0.106087\n",
      "Epoch: 5/20; Batch:348/468; Training loss:0.102491\n",
      "Epoch: 5/20; Batch:349/468; Training loss:0.105876\n",
      "Epoch: 5/20; Batch:350/468; Training loss:0.103238\n",
      "Epoch: 5/20; Batch:351/468; Training loss:0.105175\n",
      "Epoch: 5/20; Batch:352/468; Training loss:0.107761\n",
      "Epoch: 5/20; Batch:353/468; Training loss:0.103516\n",
      "Epoch: 5/20; Batch:354/468; Training loss:0.104064\n",
      "Epoch: 5/20; Batch:355/468; Training loss:0.108032\n",
      "Epoch: 5/20; Batch:356/468; Training loss:0.102785\n",
      "Epoch: 5/20; Batch:357/468; Training loss:0.103698\n",
      "Epoch: 5/20; Batch:358/468; Training loss:0.100586\n",
      "Epoch: 5/20; Batch:359/468; Training loss:0.105431\n",
      "Epoch: 5/20; Batch:360/468; Training loss:0.105491\n",
      "Epoch: 5/20; Batch:361/468; Training loss:0.101673\n",
      "Epoch: 5/20; Batch:362/468; Training loss:0.102424\n",
      "Epoch: 5/20; Batch:363/468; Training loss:0.104624\n",
      "Epoch: 5/20; Batch:364/468; Training loss:0.106966\n",
      "Epoch: 5/20; Batch:365/468; Training loss:0.106257\n",
      "Epoch: 5/20; Batch:366/468; Training loss:0.108817\n",
      "Epoch: 5/20; Batch:367/468; Training loss:0.101752\n",
      "Epoch: 5/20; Batch:368/468; Training loss:0.105436\n",
      "Epoch: 5/20; Batch:369/468; Training loss:0.103267\n",
      "Epoch: 5/20; Batch:370/468; Training loss:0.106371\n",
      "Epoch: 5/20; Batch:371/468; Training loss:0.103407\n",
      "Epoch: 5/20; Batch:372/468; Training loss:0.109149\n",
      "Epoch: 5/20; Batch:373/468; Training loss:0.107212\n",
      "Epoch: 5/20; Batch:374/468; Training loss:0.108159\n",
      "Epoch: 5/20; Batch:375/468; Training loss:0.10778\n",
      "Epoch: 5/20; Batch:376/468; Training loss:0.107217\n",
      "Epoch: 5/20; Batch:377/468; Training loss:0.105136\n",
      "Epoch: 5/20; Batch:378/468; Training loss:0.108934\n",
      "Epoch: 5/20; Batch:379/468; Training loss:0.108001\n",
      "Epoch: 5/20; Batch:380/468; Training loss:0.107058\n",
      "Epoch: 5/20; Batch:381/468; Training loss:0.105284\n",
      "Epoch: 5/20; Batch:382/468; Training loss:0.103989\n",
      "Epoch: 5/20; Batch:383/468; Training loss:0.106038\n",
      "Epoch: 5/20; Batch:384/468; Training loss:0.104308\n",
      "Epoch: 5/20; Batch:385/468; Training loss:0.105749\n",
      "Epoch: 5/20; Batch:386/468; Training loss:0.104745\n",
      "Epoch: 5/20; Batch:387/468; Training loss:0.101896\n",
      "Epoch: 5/20; Batch:388/468; Training loss:0.1022\n",
      "Epoch: 5/20; Batch:389/468; Training loss:0.107681\n",
      "Epoch: 5/20; Batch:390/468; Training loss:0.10599\n",
      "Epoch: 5/20; Batch:391/468; Training loss:0.104405\n",
      "Epoch: 5/20; Batch:392/468; Training loss:0.103393\n",
      "Epoch: 5/20; Batch:393/468; Training loss:0.105517\n",
      "Epoch: 5/20; Batch:394/468; Training loss:0.105308\n",
      "Epoch: 5/20; Batch:395/468; Training loss:0.106825\n",
      "Epoch: 5/20; Batch:396/468; Training loss:0.105898\n",
      "Epoch: 5/20; Batch:397/468; Training loss:0.10951\n",
      "Epoch: 5/20; Batch:398/468; Training loss:0.106825\n",
      "Epoch: 5/20; Batch:399/468; Training loss:0.103854\n",
      "Epoch: 5/20; Batch:400/468; Training loss:0.106943\n",
      "Epoch: 5/20; Batch:401/468; Training loss:0.10411\n",
      "Epoch: 5/20; Batch:402/468; Training loss:0.102574\n",
      "Epoch: 5/20; Batch:403/468; Training loss:0.109438\n",
      "Epoch: 5/20; Batch:404/468; Training loss:0.104943\n",
      "Epoch: 5/20; Batch:405/468; Training loss:0.101798\n",
      "Epoch: 5/20; Batch:406/468; Training loss:0.106543\n",
      "Epoch: 5/20; Batch:407/468; Training loss:0.10514\n",
      "Epoch: 5/20; Batch:408/468; Training loss:0.104194\n",
      "Epoch: 5/20; Batch:409/468; Training loss:0.101662\n",
      "Epoch: 5/20; Batch:410/468; Training loss:0.106166\n",
      "Epoch: 5/20; Batch:411/468; Training loss:0.106112\n",
      "Epoch: 5/20; Batch:412/468; Training loss:0.104914\n",
      "Epoch: 5/20; Batch:413/468; Training loss:0.105097\n",
      "Epoch: 5/20; Batch:414/468; Training loss:0.106931\n",
      "Epoch: 5/20; Batch:415/468; Training loss:0.104083\n",
      "Epoch: 5/20; Batch:416/468; Training loss:0.106205\n",
      "Epoch: 5/20; Batch:417/468; Training loss:0.106877\n",
      "Epoch: 5/20; Batch:418/468; Training loss:0.103027\n",
      "Epoch: 5/20; Batch:419/468; Training loss:0.102106\n",
      "Epoch: 5/20; Batch:420/468; Training loss:0.102006\n",
      "Epoch: 5/20; Batch:421/468; Training loss:0.100523\n",
      "Epoch: 5/20; Batch:422/468; Training loss:0.099974\n",
      "Epoch: 5/20; Batch:423/468; Training loss:0.101997\n",
      "Epoch: 5/20; Batch:424/468; Training loss:0.0976453\n",
      "Epoch: 5/20; Batch:425/468; Training loss:0.101629\n",
      "Epoch: 5/20; Batch:426/468; Training loss:0.105735\n",
      "Epoch: 5/20; Batch:427/468; Training loss:0.106641\n",
      "Epoch: 5/20; Batch:428/468; Training loss:0.105748\n",
      "Epoch: 5/20; Batch:429/468; Training loss:0.109349\n",
      "Epoch: 5/20; Batch:430/468; Training loss:0.102772\n",
      "Epoch: 5/20; Batch:431/468; Training loss:0.102292\n",
      "Epoch: 5/20; Batch:432/468; Training loss:0.101853\n",
      "Epoch: 5/20; Batch:433/468; Training loss:0.104613\n",
      "Epoch: 5/20; Batch:434/468; Training loss:0.110535\n",
      "Epoch: 5/20; Batch:435/468; Training loss:0.103634\n",
      "Epoch: 5/20; Batch:436/468; Training loss:0.104812\n",
      "Epoch: 5/20; Batch:437/468; Training loss:0.101981\n",
      "Epoch: 5/20; Batch:438/468; Training loss:0.107557\n",
      "Epoch: 5/20; Batch:439/468; Training loss:0.106647\n",
      "Epoch: 5/20; Batch:440/468; Training loss:0.102329\n",
      "Epoch: 5/20; Batch:441/468; Training loss:0.0996905\n",
      "Epoch: 5/20; Batch:442/468; Training loss:0.103501\n",
      "Epoch: 5/20; Batch:443/468; Training loss:0.107134\n",
      "Epoch: 5/20; Batch:444/468; Training loss:0.103666\n",
      "Epoch: 5/20; Batch:445/468; Training loss:0.104207\n",
      "Epoch: 5/20; Batch:446/468; Training loss:0.100408\n",
      "Epoch: 5/20; Batch:447/468; Training loss:0.106137\n",
      "Epoch: 5/20; Batch:448/468; Training loss:0.104249\n",
      "Epoch: 5/20; Batch:449/468; Training loss:0.10695\n",
      "Epoch: 5/20; Batch:450/468; Training loss:0.106441\n",
      "Epoch: 5/20; Batch:451/468; Training loss:0.103979\n",
      "Epoch: 5/20; Batch:452/468; Training loss:0.107669\n",
      "Epoch: 5/20; Batch:453/468; Training loss:0.106034\n",
      "Epoch: 5/20; Batch:454/468; Training loss:0.101134\n",
      "Epoch: 5/20; Batch:455/468; Training loss:0.107907\n",
      "Epoch: 5/20; Batch:456/468; Training loss:0.10019\n",
      "Epoch: 5/20; Batch:457/468; Training loss:0.102503\n",
      "Epoch: 5/20; Batch:458/468; Training loss:0.108047\n",
      "Epoch: 5/20; Batch:459/468; Training loss:0.102023\n",
      "Epoch: 5/20; Batch:460/468; Training loss:0.107169\n",
      "Epoch: 5/20; Batch:461/468; Training loss:0.103344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20; Batch:462/468; Training loss:0.103905\n",
      "Epoch: 5/20; Batch:463/468; Training loss:0.108894\n",
      "Epoch: 5/20; Batch:464/468; Training loss:0.104537\n",
      "Epoch: 5/20; Batch:465/468; Training loss:0.105629\n",
      "Epoch: 5/20; Batch:466/468; Training loss:0.1055\n",
      "Epoch: 5/20; Batch:467/468; Training loss:0.103379\n",
      "Epoch: 5/20; Batch:468/468; Training loss:0.107448\n",
      "Epoch: 6/20; Batch:1/468; Training loss:0.103444\n",
      "Epoch: 6/20; Batch:2/468; Training loss:0.103867\n",
      "Epoch: 6/20; Batch:3/468; Training loss:0.102852\n",
      "Epoch: 6/20; Batch:4/468; Training loss:0.107248\n",
      "Epoch: 6/20; Batch:5/468; Training loss:0.102772\n",
      "Epoch: 6/20; Batch:6/468; Training loss:0.106671\n",
      "Epoch: 6/20; Batch:7/468; Training loss:0.102236\n",
      "Epoch: 6/20; Batch:8/468; Training loss:0.101181\n",
      "Epoch: 6/20; Batch:9/468; Training loss:0.102043\n",
      "Epoch: 6/20; Batch:10/468; Training loss:0.107089\n",
      "Epoch: 6/20; Batch:11/468; Training loss:0.1047\n",
      "Epoch: 6/20; Batch:12/468; Training loss:0.109164\n",
      "Epoch: 6/20; Batch:13/468; Training loss:0.106755\n",
      "Epoch: 6/20; Batch:14/468; Training loss:0.107372\n",
      "Epoch: 6/20; Batch:15/468; Training loss:0.102656\n",
      "Epoch: 6/20; Batch:16/468; Training loss:0.107628\n",
      "Epoch: 6/20; Batch:17/468; Training loss:0.10535\n",
      "Epoch: 6/20; Batch:18/468; Training loss:0.107668\n",
      "Epoch: 6/20; Batch:19/468; Training loss:0.104474\n",
      "Epoch: 6/20; Batch:20/468; Training loss:0.102851\n",
      "Epoch: 6/20; Batch:21/468; Training loss:0.106566\n",
      "Epoch: 6/20; Batch:22/468; Training loss:0.0987278\n",
      "Epoch: 6/20; Batch:23/468; Training loss:0.10468\n",
      "Epoch: 6/20; Batch:24/468; Training loss:0.10502\n",
      "Epoch: 6/20; Batch:25/468; Training loss:0.101614\n",
      "Epoch: 6/20; Batch:26/468; Training loss:0.107941\n",
      "Epoch: 6/20; Batch:27/468; Training loss:0.105764\n",
      "Epoch: 6/20; Batch:28/468; Training loss:0.104029\n",
      "Epoch: 6/20; Batch:29/468; Training loss:0.105317\n",
      "Epoch: 6/20; Batch:30/468; Training loss:0.10735\n",
      "Epoch: 6/20; Batch:31/468; Training loss:0.105682\n",
      "Epoch: 6/20; Batch:32/468; Training loss:0.10522\n",
      "Epoch: 6/20; Batch:33/468; Training loss:0.103453\n",
      "Epoch: 6/20; Batch:34/468; Training loss:0.104419\n",
      "Epoch: 6/20; Batch:35/468; Training loss:0.101716\n",
      "Epoch: 6/20; Batch:36/468; Training loss:0.108485\n",
      "Epoch: 6/20; Batch:37/468; Training loss:0.104554\n",
      "Epoch: 6/20; Batch:38/468; Training loss:0.107593\n",
      "Epoch: 6/20; Batch:39/468; Training loss:0.105714\n",
      "Epoch: 6/20; Batch:40/468; Training loss:0.104881\n",
      "Epoch: 6/20; Batch:41/468; Training loss:0.0992463\n",
      "Epoch: 6/20; Batch:42/468; Training loss:0.102284\n",
      "Epoch: 6/20; Batch:43/468; Training loss:0.109066\n",
      "Epoch: 6/20; Batch:44/468; Training loss:0.103369\n",
      "Epoch: 6/20; Batch:45/468; Training loss:0.106967\n",
      "Epoch: 6/20; Batch:46/468; Training loss:0.104282\n",
      "Epoch: 6/20; Batch:47/468; Training loss:0.105191\n",
      "Epoch: 6/20; Batch:48/468; Training loss:0.110161\n",
      "Epoch: 6/20; Batch:49/468; Training loss:0.102257\n",
      "Epoch: 6/20; Batch:50/468; Training loss:0.0988925\n",
      "Epoch: 6/20; Batch:51/468; Training loss:0.101667\n",
      "Epoch: 6/20; Batch:52/468; Training loss:0.102682\n",
      "Epoch: 6/20; Batch:53/468; Training loss:0.0964184\n",
      "Epoch: 6/20; Batch:54/468; Training loss:0.103889\n",
      "Epoch: 6/20; Batch:55/468; Training loss:0.104615\n",
      "Epoch: 6/20; Batch:56/468; Training loss:0.10939\n",
      "Epoch: 6/20; Batch:57/468; Training loss:0.105371\n",
      "Epoch: 6/20; Batch:58/468; Training loss:0.101669\n",
      "Epoch: 6/20; Batch:59/468; Training loss:0.10547\n",
      "Epoch: 6/20; Batch:60/468; Training loss:0.1039\n",
      "Epoch: 6/20; Batch:61/468; Training loss:0.103959\n",
      "Epoch: 6/20; Batch:62/468; Training loss:0.104786\n",
      "Epoch: 6/20; Batch:63/468; Training loss:0.101515\n",
      "Epoch: 6/20; Batch:64/468; Training loss:0.106056\n",
      "Epoch: 6/20; Batch:65/468; Training loss:0.103987\n",
      "Epoch: 6/20; Batch:66/468; Training loss:0.102863\n",
      "Epoch: 6/20; Batch:67/468; Training loss:0.102765\n",
      "Epoch: 6/20; Batch:68/468; Training loss:0.104139\n",
      "Epoch: 6/20; Batch:69/468; Training loss:0.101745\n",
      "Epoch: 6/20; Batch:70/468; Training loss:0.10721\n",
      "Epoch: 6/20; Batch:71/468; Training loss:0.102211\n",
      "Epoch: 6/20; Batch:72/468; Training loss:0.102613\n",
      "Epoch: 6/20; Batch:73/468; Training loss:0.0995218\n",
      "Epoch: 6/20; Batch:74/468; Training loss:0.106115\n",
      "Epoch: 6/20; Batch:75/468; Training loss:0.10554\n",
      "Epoch: 6/20; Batch:76/468; Training loss:0.104786\n",
      "Epoch: 6/20; Batch:77/468; Training loss:0.107055\n",
      "Epoch: 6/20; Batch:78/468; Training loss:0.104434\n",
      "Epoch: 6/20; Batch:79/468; Training loss:0.100855\n",
      "Epoch: 6/20; Batch:80/468; Training loss:0.106988\n",
      "Epoch: 6/20; Batch:81/468; Training loss:0.102965\n",
      "Epoch: 6/20; Batch:82/468; Training loss:0.103777\n",
      "Epoch: 6/20; Batch:83/468; Training loss:0.107868\n",
      "Epoch: 6/20; Batch:84/468; Training loss:0.0990943\n",
      "Epoch: 6/20; Batch:85/468; Training loss:0.108523\n",
      "Epoch: 6/20; Batch:86/468; Training loss:0.105039\n",
      "Epoch: 6/20; Batch:87/468; Training loss:0.10249\n",
      "Epoch: 6/20; Batch:88/468; Training loss:0.105544\n",
      "Epoch: 6/20; Batch:89/468; Training loss:0.10088\n",
      "Epoch: 6/20; Batch:90/468; Training loss:0.106238\n",
      "Epoch: 6/20; Batch:91/468; Training loss:0.0991235\n",
      "Epoch: 6/20; Batch:92/468; Training loss:0.105909\n",
      "Epoch: 6/20; Batch:93/468; Training loss:0.10329\n",
      "Epoch: 6/20; Batch:94/468; Training loss:0.103696\n",
      "Epoch: 6/20; Batch:95/468; Training loss:0.0974533\n",
      "Epoch: 6/20; Batch:96/468; Training loss:0.100862\n",
      "Epoch: 6/20; Batch:97/468; Training loss:0.105625\n",
      "Epoch: 6/20; Batch:98/468; Training loss:0.104023\n",
      "Epoch: 6/20; Batch:99/468; Training loss:0.101613\n",
      "Epoch: 6/20; Batch:100/468; Training loss:0.102604\n",
      "Epoch: 6/20; Batch:101/468; Training loss:0.105734\n",
      "Epoch: 6/20; Batch:102/468; Training loss:0.104562\n",
      "Epoch: 6/20; Batch:103/468; Training loss:0.101137\n",
      "Epoch: 6/20; Batch:104/468; Training loss:0.102065\n",
      "Epoch: 6/20; Batch:105/468; Training loss:0.102463\n",
      "Epoch: 6/20; Batch:106/468; Training loss:0.10285\n",
      "Epoch: 6/20; Batch:107/468; Training loss:0.101971\n",
      "Epoch: 6/20; Batch:108/468; Training loss:0.103579\n",
      "Epoch: 6/20; Batch:109/468; Training loss:0.1064\n",
      "Epoch: 6/20; Batch:110/468; Training loss:0.105767\n",
      "Epoch: 6/20; Batch:111/468; Training loss:0.109264\n",
      "Epoch: 6/20; Batch:112/468; Training loss:0.102161\n",
      "Epoch: 6/20; Batch:113/468; Training loss:0.10376\n",
      "Epoch: 6/20; Batch:114/468; Training loss:0.105177\n",
      "Epoch: 6/20; Batch:115/468; Training loss:0.100998\n",
      "Epoch: 6/20; Batch:116/468; Training loss:0.100127\n",
      "Epoch: 6/20; Batch:117/468; Training loss:0.100927\n",
      "Epoch: 6/20; Batch:118/468; Training loss:0.107105\n",
      "Epoch: 6/20; Batch:119/468; Training loss:0.10357\n",
      "Epoch: 6/20; Batch:120/468; Training loss:0.106713\n",
      "Epoch: 6/20; Batch:121/468; Training loss:0.10564\n",
      "Epoch: 6/20; Batch:122/468; Training loss:0.103488\n",
      "Epoch: 6/20; Batch:123/468; Training loss:0.103296\n",
      "Epoch: 6/20; Batch:124/468; Training loss:0.10628\n",
      "Epoch: 6/20; Batch:125/468; Training loss:0.103737\n",
      "Epoch: 6/20; Batch:126/468; Training loss:0.100223\n",
      "Epoch: 6/20; Batch:127/468; Training loss:0.102736\n",
      "Epoch: 6/20; Batch:128/468; Training loss:0.100301\n",
      "Epoch: 6/20; Batch:129/468; Training loss:0.105332\n",
      "Epoch: 6/20; Batch:130/468; Training loss:0.105954\n",
      "Epoch: 6/20; Batch:131/468; Training loss:0.103134\n",
      "Epoch: 6/20; Batch:132/468; Training loss:0.102097\n",
      "Epoch: 6/20; Batch:133/468; Training loss:0.100571\n",
      "Epoch: 6/20; Batch:134/468; Training loss:0.104661\n",
      "Epoch: 6/20; Batch:135/468; Training loss:0.105125\n",
      "Epoch: 6/20; Batch:136/468; Training loss:0.106895\n",
      "Epoch: 6/20; Batch:137/468; Training loss:0.105061\n",
      "Epoch: 6/20; Batch:138/468; Training loss:0.103474\n",
      "Epoch: 6/20; Batch:139/468; Training loss:0.10252\n",
      "Epoch: 6/20; Batch:140/468; Training loss:0.105258\n",
      "Epoch: 6/20; Batch:141/468; Training loss:0.106536\n",
      "Epoch: 6/20; Batch:142/468; Training loss:0.103698\n",
      "Epoch: 6/20; Batch:143/468; Training loss:0.105624\n",
      "Epoch: 6/20; Batch:144/468; Training loss:0.103171\n",
      "Epoch: 6/20; Batch:145/468; Training loss:0.103426\n",
      "Epoch: 6/20; Batch:146/468; Training loss:0.105094\n",
      "Epoch: 6/20; Batch:147/468; Training loss:0.102293\n",
      "Epoch: 6/20; Batch:148/468; Training loss:0.102924\n",
      "Epoch: 6/20; Batch:149/468; Training loss:0.106055\n",
      "Epoch: 6/20; Batch:150/468; Training loss:0.105616\n",
      "Epoch: 6/20; Batch:151/468; Training loss:0.104577\n",
      "Epoch: 6/20; Batch:152/468; Training loss:0.100987\n",
      "Epoch: 6/20; Batch:153/468; Training loss:0.104869\n",
      "Epoch: 6/20; Batch:154/468; Training loss:0.103614\n",
      "Epoch: 6/20; Batch:155/468; Training loss:0.104741\n",
      "Epoch: 6/20; Batch:156/468; Training loss:0.103948\n",
      "Epoch: 6/20; Batch:157/468; Training loss:0.103464\n",
      "Epoch: 6/20; Batch:158/468; Training loss:0.105674\n",
      "Epoch: 6/20; Batch:159/468; Training loss:0.102915\n",
      "Epoch: 6/20; Batch:160/468; Training loss:0.101838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20; Batch:161/468; Training loss:0.102845\n",
      "Epoch: 6/20; Batch:162/468; Training loss:0.0984309\n",
      "Epoch: 6/20; Batch:163/468; Training loss:0.100446\n",
      "Epoch: 6/20; Batch:164/468; Training loss:0.105046\n",
      "Epoch: 6/20; Batch:165/468; Training loss:0.103623\n",
      "Epoch: 6/20; Batch:166/468; Training loss:0.104354\n",
      "Epoch: 6/20; Batch:167/468; Training loss:0.107317\n",
      "Epoch: 6/20; Batch:168/468; Training loss:0.110929\n",
      "Epoch: 6/20; Batch:169/468; Training loss:0.101732\n",
      "Epoch: 6/20; Batch:170/468; Training loss:0.105721\n",
      "Epoch: 6/20; Batch:171/468; Training loss:0.108658\n",
      "Epoch: 6/20; Batch:172/468; Training loss:0.103616\n",
      "Epoch: 6/20; Batch:173/468; Training loss:0.106019\n",
      "Epoch: 6/20; Batch:174/468; Training loss:0.104664\n",
      "Epoch: 6/20; Batch:175/468; Training loss:0.102413\n",
      "Epoch: 6/20; Batch:176/468; Training loss:0.104089\n",
      "Epoch: 6/20; Batch:177/468; Training loss:0.1025\n",
      "Epoch: 6/20; Batch:178/468; Training loss:0.102838\n",
      "Epoch: 6/20; Batch:179/468; Training loss:0.103766\n",
      "Epoch: 6/20; Batch:180/468; Training loss:0.102924\n",
      "Epoch: 6/20; Batch:181/468; Training loss:0.108285\n",
      "Epoch: 6/20; Batch:182/468; Training loss:0.103569\n",
      "Epoch: 6/20; Batch:183/468; Training loss:0.100119\n",
      "Epoch: 6/20; Batch:184/468; Training loss:0.106027\n",
      "Epoch: 6/20; Batch:185/468; Training loss:0.102456\n",
      "Epoch: 6/20; Batch:186/468; Training loss:0.10417\n",
      "Epoch: 6/20; Batch:187/468; Training loss:0.106438\n",
      "Epoch: 6/20; Batch:188/468; Training loss:0.106822\n",
      "Epoch: 6/20; Batch:189/468; Training loss:0.105298\n",
      "Epoch: 6/20; Batch:190/468; Training loss:0.105591\n",
      "Epoch: 6/20; Batch:191/468; Training loss:0.105792\n",
      "Epoch: 6/20; Batch:192/468; Training loss:0.104961\n",
      "Epoch: 6/20; Batch:193/468; Training loss:0.103503\n",
      "Epoch: 6/20; Batch:194/468; Training loss:0.100795\n",
      "Epoch: 6/20; Batch:195/468; Training loss:0.107207\n",
      "Epoch: 6/20; Batch:196/468; Training loss:0.0993512\n",
      "Epoch: 6/20; Batch:197/468; Training loss:0.103553\n",
      "Epoch: 6/20; Batch:198/468; Training loss:0.0991422\n",
      "Epoch: 6/20; Batch:199/468; Training loss:0.106989\n",
      "Epoch: 6/20; Batch:200/468; Training loss:0.0976762\n",
      "Epoch: 6/20; Batch:201/468; Training loss:0.104512\n",
      "Epoch: 6/20; Batch:202/468; Training loss:0.102071\n",
      "Epoch: 6/20; Batch:203/468; Training loss:0.101979\n",
      "Epoch: 6/20; Batch:204/468; Training loss:0.10175\n",
      "Epoch: 6/20; Batch:205/468; Training loss:0.0997938\n",
      "Epoch: 6/20; Batch:206/468; Training loss:0.106097\n",
      "Epoch: 6/20; Batch:207/468; Training loss:0.103517\n",
      "Epoch: 6/20; Batch:208/468; Training loss:0.101193\n",
      "Epoch: 6/20; Batch:209/468; Training loss:0.103369\n",
      "Epoch: 6/20; Batch:210/468; Training loss:0.102404\n",
      "Epoch: 6/20; Batch:211/468; Training loss:0.102798\n",
      "Epoch: 6/20; Batch:212/468; Training loss:0.107038\n",
      "Epoch: 6/20; Batch:213/468; Training loss:0.109768\n",
      "Epoch: 6/20; Batch:214/468; Training loss:0.104656\n",
      "Epoch: 6/20; Batch:215/468; Training loss:0.105097\n",
      "Epoch: 6/20; Batch:216/468; Training loss:0.104444\n",
      "Epoch: 6/20; Batch:217/468; Training loss:0.103127\n",
      "Epoch: 6/20; Batch:218/468; Training loss:0.103264\n",
      "Epoch: 6/20; Batch:219/468; Training loss:0.10334\n",
      "Epoch: 6/20; Batch:220/468; Training loss:0.10415\n",
      "Epoch: 6/20; Batch:221/468; Training loss:0.102438\n",
      "Epoch: 6/20; Batch:222/468; Training loss:0.102973\n",
      "Epoch: 6/20; Batch:223/468; Training loss:0.101463\n",
      "Epoch: 6/20; Batch:224/468; Training loss:0.109444\n",
      "Epoch: 6/20; Batch:225/468; Training loss:0.104704\n",
      "Epoch: 6/20; Batch:226/468; Training loss:0.106933\n",
      "Epoch: 6/20; Batch:227/468; Training loss:0.104836\n",
      "Epoch: 6/20; Batch:228/468; Training loss:0.101719\n",
      "Epoch: 6/20; Batch:229/468; Training loss:0.103689\n",
      "Epoch: 6/20; Batch:230/468; Training loss:0.107211\n",
      "Epoch: 6/20; Batch:231/468; Training loss:0.107322\n",
      "Epoch: 6/20; Batch:232/468; Training loss:0.102962\n",
      "Epoch: 6/20; Batch:233/468; Training loss:0.104941\n",
      "Epoch: 6/20; Batch:234/468; Training loss:0.107815\n",
      "Epoch: 6/20; Batch:235/468; Training loss:0.10379\n",
      "Epoch: 6/20; Batch:236/468; Training loss:0.103759\n",
      "Epoch: 6/20; Batch:237/468; Training loss:0.106418\n",
      "Epoch: 6/20; Batch:238/468; Training loss:0.104678\n",
      "Epoch: 6/20; Batch:239/468; Training loss:0.103811\n",
      "Epoch: 6/20; Batch:240/468; Training loss:0.1078\n",
      "Epoch: 6/20; Batch:241/468; Training loss:0.105324\n",
      "Epoch: 6/20; Batch:242/468; Training loss:0.10277\n",
      "Epoch: 6/20; Batch:243/468; Training loss:0.101489\n",
      "Epoch: 6/20; Batch:244/468; Training loss:0.101767\n",
      "Epoch: 6/20; Batch:245/468; Training loss:0.102467\n",
      "Epoch: 6/20; Batch:246/468; Training loss:0.100962\n",
      "Epoch: 6/20; Batch:247/468; Training loss:0.102865\n",
      "Epoch: 6/20; Batch:248/468; Training loss:0.0992879\n",
      "Epoch: 6/20; Batch:249/468; Training loss:0.107035\n",
      "Epoch: 6/20; Batch:250/468; Training loss:0.102035\n",
      "Epoch: 6/20; Batch:251/468; Training loss:0.100094\n",
      "Epoch: 6/20; Batch:252/468; Training loss:0.101801\n",
      "Epoch: 6/20; Batch:253/468; Training loss:0.102658\n",
      "Epoch: 6/20; Batch:254/468; Training loss:0.105931\n",
      "Epoch: 6/20; Batch:255/468; Training loss:0.10284\n",
      "Epoch: 6/20; Batch:256/468; Training loss:0.096966\n",
      "Epoch: 6/20; Batch:257/468; Training loss:0.101198\n",
      "Epoch: 6/20; Batch:258/468; Training loss:0.108578\n",
      "Epoch: 6/20; Batch:259/468; Training loss:0.102428\n",
      "Epoch: 6/20; Batch:260/468; Training loss:0.104295\n",
      "Epoch: 6/20; Batch:261/468; Training loss:0.10407\n",
      "Epoch: 6/20; Batch:262/468; Training loss:0.102726\n",
      "Epoch: 6/20; Batch:263/468; Training loss:0.106178\n",
      "Epoch: 6/20; Batch:264/468; Training loss:0.108918\n",
      "Epoch: 6/20; Batch:265/468; Training loss:0.102341\n",
      "Epoch: 6/20; Batch:266/468; Training loss:0.100621\n",
      "Epoch: 6/20; Batch:267/468; Training loss:0.100228\n",
      "Epoch: 6/20; Batch:268/468; Training loss:0.101175\n",
      "Epoch: 6/20; Batch:269/468; Training loss:0.103472\n",
      "Epoch: 6/20; Batch:270/468; Training loss:0.101804\n",
      "Epoch: 6/20; Batch:271/468; Training loss:0.10564\n",
      "Epoch: 6/20; Batch:272/468; Training loss:0.103628\n",
      "Epoch: 6/20; Batch:273/468; Training loss:0.10436\n",
      "Epoch: 6/20; Batch:274/468; Training loss:0.106746\n",
      "Epoch: 6/20; Batch:275/468; Training loss:0.1055\n",
      "Epoch: 6/20; Batch:276/468; Training loss:0.10515\n",
      "Epoch: 6/20; Batch:277/468; Training loss:0.101235\n",
      "Epoch: 6/20; Batch:278/468; Training loss:0.103904\n",
      "Epoch: 6/20; Batch:279/468; Training loss:0.103007\n",
      "Epoch: 6/20; Batch:280/468; Training loss:0.110589\n",
      "Epoch: 6/20; Batch:281/468; Training loss:0.0980242\n",
      "Epoch: 6/20; Batch:282/468; Training loss:0.103768\n",
      "Epoch: 6/20; Batch:283/468; Training loss:0.103698\n",
      "Epoch: 6/20; Batch:284/468; Training loss:0.10268\n",
      "Epoch: 6/20; Batch:285/468; Training loss:0.103338\n",
      "Epoch: 6/20; Batch:286/468; Training loss:0.10235\n",
      "Epoch: 6/20; Batch:287/468; Training loss:0.107144\n",
      "Epoch: 6/20; Batch:288/468; Training loss:0.101125\n",
      "Epoch: 6/20; Batch:289/468; Training loss:0.107266\n",
      "Epoch: 6/20; Batch:290/468; Training loss:0.100976\n",
      "Epoch: 6/20; Batch:291/468; Training loss:0.103826\n",
      "Epoch: 6/20; Batch:292/468; Training loss:0.102099\n",
      "Epoch: 6/20; Batch:293/468; Training loss:0.106672\n",
      "Epoch: 6/20; Batch:294/468; Training loss:0.0998642\n",
      "Epoch: 6/20; Batch:295/468; Training loss:0.106103\n",
      "Epoch: 6/20; Batch:296/468; Training loss:0.106529\n",
      "Epoch: 6/20; Batch:297/468; Training loss:0.0986898\n",
      "Epoch: 6/20; Batch:298/468; Training loss:0.104133\n",
      "Epoch: 6/20; Batch:299/468; Training loss:0.107597\n",
      "Epoch: 6/20; Batch:300/468; Training loss:0.10686\n",
      "Epoch: 6/20; Batch:301/468; Training loss:0.0989205\n",
      "Epoch: 6/20; Batch:302/468; Training loss:0.102685\n",
      "Epoch: 6/20; Batch:303/468; Training loss:0.104056\n",
      "Epoch: 6/20; Batch:304/468; Training loss:0.101085\n",
      "Epoch: 6/20; Batch:305/468; Training loss:0.101071\n",
      "Epoch: 6/20; Batch:306/468; Training loss:0.097963\n",
      "Epoch: 6/20; Batch:307/468; Training loss:0.104858\n",
      "Epoch: 6/20; Batch:308/468; Training loss:0.105186\n",
      "Epoch: 6/20; Batch:309/468; Training loss:0.106757\n",
      "Epoch: 6/20; Batch:310/468; Training loss:0.107565\n",
      "Epoch: 6/20; Batch:311/468; Training loss:0.100312\n",
      "Epoch: 6/20; Batch:312/468; Training loss:0.105062\n",
      "Epoch: 6/20; Batch:313/468; Training loss:0.103847\n",
      "Epoch: 6/20; Batch:314/468; Training loss:0.102903\n",
      "Epoch: 6/20; Batch:315/468; Training loss:0.100927\n",
      "Epoch: 6/20; Batch:316/468; Training loss:0.100294\n",
      "Epoch: 6/20; Batch:317/468; Training loss:0.10575\n",
      "Epoch: 6/20; Batch:318/468; Training loss:0.101945\n",
      "Epoch: 6/20; Batch:319/468; Training loss:0.102564\n",
      "Epoch: 6/20; Batch:320/468; Training loss:0.101374\n",
      "Epoch: 6/20; Batch:321/468; Training loss:0.100089\n",
      "Epoch: 6/20; Batch:322/468; Training loss:0.0984054\n",
      "Epoch: 6/20; Batch:323/468; Training loss:0.103548\n",
      "Epoch: 6/20; Batch:324/468; Training loss:0.108898\n",
      "Epoch: 6/20; Batch:325/468; Training loss:0.105939\n",
      "Epoch: 6/20; Batch:326/468; Training loss:0.0979223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20; Batch:327/468; Training loss:0.108932\n",
      "Epoch: 6/20; Batch:328/468; Training loss:0.103867\n",
      "Epoch: 6/20; Batch:329/468; Training loss:0.106895\n",
      "Epoch: 6/20; Batch:330/468; Training loss:0.107098\n",
      "Epoch: 6/20; Batch:331/468; Training loss:0.101762\n",
      "Epoch: 6/20; Batch:332/468; Training loss:0.10484\n",
      "Epoch: 6/20; Batch:333/468; Training loss:0.103832\n",
      "Epoch: 6/20; Batch:334/468; Training loss:0.106357\n",
      "Epoch: 6/20; Batch:335/468; Training loss:0.103448\n",
      "Epoch: 6/20; Batch:336/468; Training loss:0.105116\n",
      "Epoch: 6/20; Batch:337/468; Training loss:0.102554\n",
      "Epoch: 6/20; Batch:338/468; Training loss:0.104799\n",
      "Epoch: 6/20; Batch:339/468; Training loss:0.102888\n",
      "Epoch: 6/20; Batch:340/468; Training loss:0.103931\n",
      "Epoch: 6/20; Batch:341/468; Training loss:0.100012\n",
      "Epoch: 6/20; Batch:342/468; Training loss:0.109871\n",
      "Epoch: 6/20; Batch:343/468; Training loss:0.107826\n",
      "Epoch: 6/20; Batch:344/468; Training loss:0.103648\n",
      "Epoch: 6/20; Batch:345/468; Training loss:0.105501\n",
      "Epoch: 6/20; Batch:346/468; Training loss:0.101391\n",
      "Epoch: 6/20; Batch:347/468; Training loss:0.102035\n",
      "Epoch: 6/20; Batch:348/468; Training loss:0.107332\n",
      "Epoch: 6/20; Batch:349/468; Training loss:0.104866\n",
      "Epoch: 6/20; Batch:350/468; Training loss:0.104431\n",
      "Epoch: 6/20; Batch:351/468; Training loss:0.106565\n",
      "Epoch: 6/20; Batch:352/468; Training loss:0.0992787\n",
      "Epoch: 6/20; Batch:353/468; Training loss:0.109768\n",
      "Epoch: 6/20; Batch:354/468; Training loss:0.105718\n",
      "Epoch: 6/20; Batch:355/468; Training loss:0.107957\n",
      "Epoch: 6/20; Batch:356/468; Training loss:0.101828\n",
      "Epoch: 6/20; Batch:357/468; Training loss:0.101956\n",
      "Epoch: 6/20; Batch:358/468; Training loss:0.0992729\n",
      "Epoch: 6/20; Batch:359/468; Training loss:0.102038\n",
      "Epoch: 6/20; Batch:360/468; Training loss:0.105893\n",
      "Epoch: 6/20; Batch:361/468; Training loss:0.10339\n",
      "Epoch: 6/20; Batch:362/468; Training loss:0.100787\n",
      "Epoch: 6/20; Batch:363/468; Training loss:0.106112\n",
      "Epoch: 6/20; Batch:364/468; Training loss:0.106307\n",
      "Epoch: 6/20; Batch:365/468; Training loss:0.100511\n",
      "Epoch: 6/20; Batch:366/468; Training loss:0.0995619\n",
      "Epoch: 6/20; Batch:367/468; Training loss:0.102803\n",
      "Epoch: 6/20; Batch:368/468; Training loss:0.105961\n",
      "Epoch: 6/20; Batch:369/468; Training loss:0.0988524\n",
      "Epoch: 6/20; Batch:370/468; Training loss:0.0997521\n",
      "Epoch: 6/20; Batch:371/468; Training loss:0.103773\n",
      "Epoch: 6/20; Batch:372/468; Training loss:0.100688\n",
      "Epoch: 6/20; Batch:373/468; Training loss:0.102532\n",
      "Epoch: 6/20; Batch:374/468; Training loss:0.103005\n",
      "Epoch: 6/20; Batch:375/468; Training loss:0.103861\n",
      "Epoch: 6/20; Batch:376/468; Training loss:0.103736\n",
      "Epoch: 6/20; Batch:377/468; Training loss:0.105297\n",
      "Epoch: 6/20; Batch:378/468; Training loss:0.105843\n",
      "Epoch: 6/20; Batch:379/468; Training loss:0.105285\n",
      "Epoch: 6/20; Batch:380/468; Training loss:0.101557\n",
      "Epoch: 6/20; Batch:381/468; Training loss:0.10541\n",
      "Epoch: 6/20; Batch:382/468; Training loss:0.106895\n",
      "Epoch: 6/20; Batch:383/468; Training loss:0.100358\n",
      "Epoch: 6/20; Batch:384/468; Training loss:0.0994712\n",
      "Epoch: 6/20; Batch:385/468; Training loss:0.100563\n",
      "Epoch: 6/20; Batch:386/468; Training loss:0.103029\n",
      "Epoch: 6/20; Batch:387/468; Training loss:0.102287\n",
      "Epoch: 6/20; Batch:388/468; Training loss:0.103331\n",
      "Epoch: 6/20; Batch:389/468; Training loss:0.103015\n",
      "Epoch: 6/20; Batch:390/468; Training loss:0.101491\n",
      "Epoch: 6/20; Batch:391/468; Training loss:0.100432\n",
      "Epoch: 6/20; Batch:392/468; Training loss:0.107891\n",
      "Epoch: 6/20; Batch:393/468; Training loss:0.101013\n",
      "Epoch: 6/20; Batch:394/468; Training loss:0.104633\n",
      "Epoch: 6/20; Batch:395/468; Training loss:0.100177\n",
      "Epoch: 6/20; Batch:396/468; Training loss:0.103741\n",
      "Epoch: 6/20; Batch:397/468; Training loss:0.1026\n",
      "Epoch: 6/20; Batch:398/468; Training loss:0.0981729\n",
      "Epoch: 6/20; Batch:399/468; Training loss:0.101957\n",
      "Epoch: 6/20; Batch:400/468; Training loss:0.107909\n",
      "Epoch: 6/20; Batch:401/468; Training loss:0.102726\n",
      "Epoch: 6/20; Batch:402/468; Training loss:0.104091\n",
      "Epoch: 6/20; Batch:403/468; Training loss:0.103564\n",
      "Epoch: 6/20; Batch:404/468; Training loss:0.103599\n",
      "Epoch: 6/20; Batch:405/468; Training loss:0.105893\n",
      "Epoch: 6/20; Batch:406/468; Training loss:0.103717\n",
      "Epoch: 6/20; Batch:407/468; Training loss:0.100294\n",
      "Epoch: 6/20; Batch:408/468; Training loss:0.103393\n",
      "Epoch: 6/20; Batch:409/468; Training loss:0.106421\n",
      "Epoch: 6/20; Batch:410/468; Training loss:0.100636\n",
      "Epoch: 6/20; Batch:411/468; Training loss:0.105953\n",
      "Epoch: 6/20; Batch:412/468; Training loss:0.102954\n",
      "Epoch: 6/20; Batch:413/468; Training loss:0.100435\n",
      "Epoch: 6/20; Batch:414/468; Training loss:0.102982\n",
      "Epoch: 6/20; Batch:415/468; Training loss:0.104408\n",
      "Epoch: 6/20; Batch:416/468; Training loss:0.106605\n",
      "Epoch: 6/20; Batch:417/468; Training loss:0.101682\n",
      "Epoch: 6/20; Batch:418/468; Training loss:0.105373\n",
      "Epoch: 6/20; Batch:419/468; Training loss:0.101644\n",
      "Epoch: 6/20; Batch:420/468; Training loss:0.103391\n",
      "Epoch: 6/20; Batch:421/468; Training loss:0.107658\n",
      "Epoch: 6/20; Batch:422/468; Training loss:0.10524\n",
      "Epoch: 6/20; Batch:423/468; Training loss:0.100884\n",
      "Epoch: 6/20; Batch:424/468; Training loss:0.103927\n",
      "Epoch: 6/20; Batch:425/468; Training loss:0.104172\n",
      "Epoch: 6/20; Batch:426/468; Training loss:0.105263\n",
      "Epoch: 6/20; Batch:427/468; Training loss:0.102005\n",
      "Epoch: 6/20; Batch:428/468; Training loss:0.101337\n",
      "Epoch: 6/20; Batch:429/468; Training loss:0.0994619\n",
      "Epoch: 6/20; Batch:430/468; Training loss:0.101215\n",
      "Epoch: 6/20; Batch:431/468; Training loss:0.104695\n",
      "Epoch: 6/20; Batch:432/468; Training loss:0.104855\n",
      "Epoch: 6/20; Batch:433/468; Training loss:0.103034\n",
      "Epoch: 6/20; Batch:434/468; Training loss:0.106569\n",
      "Epoch: 6/20; Batch:435/468; Training loss:0.10594\n",
      "Epoch: 6/20; Batch:436/468; Training loss:0.098655\n",
      "Epoch: 6/20; Batch:437/468; Training loss:0.1037\n",
      "Epoch: 6/20; Batch:438/468; Training loss:0.104314\n",
      "Epoch: 6/20; Batch:439/468; Training loss:0.0995344\n",
      "Epoch: 6/20; Batch:440/468; Training loss:0.10099\n",
      "Epoch: 6/20; Batch:441/468; Training loss:0.101938\n",
      "Epoch: 6/20; Batch:442/468; Training loss:0.101687\n",
      "Epoch: 6/20; Batch:443/468; Training loss:0.101404\n",
      "Epoch: 6/20; Batch:444/468; Training loss:0.107751\n",
      "Epoch: 6/20; Batch:445/468; Training loss:0.101556\n",
      "Epoch: 6/20; Batch:446/468; Training loss:0.103802\n",
      "Epoch: 6/20; Batch:447/468; Training loss:0.106044\n",
      "Epoch: 6/20; Batch:448/468; Training loss:0.101162\n",
      "Epoch: 6/20; Batch:449/468; Training loss:0.102\n",
      "Epoch: 6/20; Batch:450/468; Training loss:0.103092\n",
      "Epoch: 6/20; Batch:451/468; Training loss:0.104562\n",
      "Epoch: 6/20; Batch:452/468; Training loss:0.105219\n",
      "Epoch: 6/20; Batch:453/468; Training loss:0.1037\n",
      "Epoch: 6/20; Batch:454/468; Training loss:0.104709\n",
      "Epoch: 6/20; Batch:455/468; Training loss:0.0991237\n",
      "Epoch: 6/20; Batch:456/468; Training loss:0.101956\n",
      "Epoch: 6/20; Batch:457/468; Training loss:0.100485\n",
      "Epoch: 6/20; Batch:458/468; Training loss:0.100725\n",
      "Epoch: 6/20; Batch:459/468; Training loss:0.101004\n",
      "Epoch: 6/20; Batch:460/468; Training loss:0.100327\n",
      "Epoch: 6/20; Batch:461/468; Training loss:0.109865\n",
      "Epoch: 6/20; Batch:462/468; Training loss:0.102666\n",
      "Epoch: 6/20; Batch:463/468; Training loss:0.10398\n",
      "Epoch: 6/20; Batch:464/468; Training loss:0.101306\n",
      "Epoch: 6/20; Batch:465/468; Training loss:0.105481\n",
      "Epoch: 6/20; Batch:466/468; Training loss:0.101938\n",
      "Epoch: 6/20; Batch:467/468; Training loss:0.101325\n",
      "Epoch: 6/20; Batch:468/468; Training loss:0.10552\n",
      "Epoch: 7/20; Batch:1/468; Training loss:0.104778\n",
      "Epoch: 7/20; Batch:2/468; Training loss:0.103197\n",
      "Epoch: 7/20; Batch:3/468; Training loss:0.102472\n",
      "Epoch: 7/20; Batch:4/468; Training loss:0.102518\n",
      "Epoch: 7/20; Batch:5/468; Training loss:0.103871\n",
      "Epoch: 7/20; Batch:6/468; Training loss:0.102058\n",
      "Epoch: 7/20; Batch:7/468; Training loss:0.101261\n",
      "Epoch: 7/20; Batch:8/468; Training loss:0.102205\n",
      "Epoch: 7/20; Batch:9/468; Training loss:0.103048\n",
      "Epoch: 7/20; Batch:10/468; Training loss:0.103496\n",
      "Epoch: 7/20; Batch:11/468; Training loss:0.106157\n",
      "Epoch: 7/20; Batch:12/468; Training loss:0.101746\n",
      "Epoch: 7/20; Batch:13/468; Training loss:0.106451\n",
      "Epoch: 7/20; Batch:14/468; Training loss:0.102494\n",
      "Epoch: 7/20; Batch:15/468; Training loss:0.105645\n",
      "Epoch: 7/20; Batch:16/468; Training loss:0.102055\n",
      "Epoch: 7/20; Batch:17/468; Training loss:0.104636\n",
      "Epoch: 7/20; Batch:18/468; Training loss:0.101664\n",
      "Epoch: 7/20; Batch:19/468; Training loss:0.106789\n",
      "Epoch: 7/20; Batch:20/468; Training loss:0.0994855\n",
      "Epoch: 7/20; Batch:21/468; Training loss:0.105352\n",
      "Epoch: 7/20; Batch:22/468; Training loss:0.099395\n",
      "Epoch: 7/20; Batch:23/468; Training loss:0.102393\n",
      "Epoch: 7/20; Batch:24/468; Training loss:0.0978352\n",
      "Epoch: 7/20; Batch:25/468; Training loss:0.101675\n",
      "Epoch: 7/20; Batch:26/468; Training loss:0.10277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20; Batch:27/468; Training loss:0.101423\n",
      "Epoch: 7/20; Batch:28/468; Training loss:0.103439\n",
      "Epoch: 7/20; Batch:29/468; Training loss:0.106289\n",
      "Epoch: 7/20; Batch:30/468; Training loss:0.103902\n",
      "Epoch: 7/20; Batch:31/468; Training loss:0.105561\n",
      "Epoch: 7/20; Batch:32/468; Training loss:0.105438\n",
      "Epoch: 7/20; Batch:33/468; Training loss:0.106036\n",
      "Epoch: 7/20; Batch:34/468; Training loss:0.0995602\n",
      "Epoch: 7/20; Batch:35/468; Training loss:0.101375\n",
      "Epoch: 7/20; Batch:36/468; Training loss:0.102108\n",
      "Epoch: 7/20; Batch:37/468; Training loss:0.0992854\n",
      "Epoch: 7/20; Batch:38/468; Training loss:0.102228\n",
      "Epoch: 7/20; Batch:39/468; Training loss:0.0996675\n",
      "Epoch: 7/20; Batch:40/468; Training loss:0.104066\n",
      "Epoch: 7/20; Batch:41/468; Training loss:0.105424\n",
      "Epoch: 7/20; Batch:42/468; Training loss:0.100912\n",
      "Epoch: 7/20; Batch:43/468; Training loss:0.105843\n",
      "Epoch: 7/20; Batch:44/468; Training loss:0.100464\n",
      "Epoch: 7/20; Batch:45/468; Training loss:0.103198\n",
      "Epoch: 7/20; Batch:46/468; Training loss:0.102927\n",
      "Epoch: 7/20; Batch:47/468; Training loss:0.102119\n",
      "Epoch: 7/20; Batch:48/468; Training loss:0.102668\n",
      "Epoch: 7/20; Batch:49/468; Training loss:0.105493\n",
      "Epoch: 7/20; Batch:50/468; Training loss:0.103313\n",
      "Epoch: 7/20; Batch:51/468; Training loss:0.105843\n",
      "Epoch: 7/20; Batch:52/468; Training loss:0.101112\n",
      "Epoch: 7/20; Batch:53/468; Training loss:0.100858\n",
      "Epoch: 7/20; Batch:54/468; Training loss:0.107872\n",
      "Epoch: 7/20; Batch:55/468; Training loss:0.103738\n",
      "Epoch: 7/20; Batch:56/468; Training loss:0.10421\n",
      "Epoch: 7/20; Batch:57/468; Training loss:0.106173\n",
      "Epoch: 7/20; Batch:58/468; Training loss:0.107933\n",
      "Epoch: 7/20; Batch:59/468; Training loss:0.105919\n",
      "Epoch: 7/20; Batch:60/468; Training loss:0.102504\n",
      "Epoch: 7/20; Batch:61/468; Training loss:0.103802\n",
      "Epoch: 7/20; Batch:62/468; Training loss:0.100802\n",
      "Epoch: 7/20; Batch:63/468; Training loss:0.0999167\n",
      "Epoch: 7/20; Batch:64/468; Training loss:0.1077\n",
      "Epoch: 7/20; Batch:65/468; Training loss:0.103855\n",
      "Epoch: 7/20; Batch:66/468; Training loss:0.106876\n",
      "Epoch: 7/20; Batch:67/468; Training loss:0.103222\n",
      "Epoch: 7/20; Batch:68/468; Training loss:0.100215\n",
      "Epoch: 7/20; Batch:69/468; Training loss:0.10815\n",
      "Epoch: 7/20; Batch:70/468; Training loss:0.103118\n",
      "Epoch: 7/20; Batch:71/468; Training loss:0.103568\n",
      "Epoch: 7/20; Batch:72/468; Training loss:0.101073\n",
      "Epoch: 7/20; Batch:73/468; Training loss:0.102686\n",
      "Epoch: 7/20; Batch:74/468; Training loss:0.103366\n",
      "Epoch: 7/20; Batch:75/468; Training loss:0.101382\n",
      "Epoch: 7/20; Batch:76/468; Training loss:0.100152\n",
      "Epoch: 7/20; Batch:77/468; Training loss:0.105865\n",
      "Epoch: 7/20; Batch:78/468; Training loss:0.102967\n",
      "Epoch: 7/20; Batch:79/468; Training loss:0.102721\n",
      "Epoch: 7/20; Batch:80/468; Training loss:0.098363\n",
      "Epoch: 7/20; Batch:81/468; Training loss:0.105397\n",
      "Epoch: 7/20; Batch:82/468; Training loss:0.104416\n",
      "Epoch: 7/20; Batch:83/468; Training loss:0.102569\n",
      "Epoch: 7/20; Batch:84/468; Training loss:0.0999763\n",
      "Epoch: 7/20; Batch:85/468; Training loss:0.102078\n",
      "Epoch: 7/20; Batch:86/468; Training loss:0.102126\n",
      "Epoch: 7/20; Batch:87/468; Training loss:0.098117\n",
      "Epoch: 7/20; Batch:88/468; Training loss:0.106766\n",
      "Epoch: 7/20; Batch:89/468; Training loss:0.103751\n",
      "Epoch: 7/20; Batch:90/468; Training loss:0.107252\n",
      "Epoch: 7/20; Batch:91/468; Training loss:0.100096\n",
      "Epoch: 7/20; Batch:92/468; Training loss:0.103516\n",
      "Epoch: 7/20; Batch:93/468; Training loss:0.0989638\n",
      "Epoch: 7/20; Batch:94/468; Training loss:0.102268\n",
      "Epoch: 7/20; Batch:95/468; Training loss:0.102673\n",
      "Epoch: 7/20; Batch:96/468; Training loss:0.106573\n",
      "Epoch: 7/20; Batch:97/468; Training loss:0.106839\n",
      "Epoch: 7/20; Batch:98/468; Training loss:0.106758\n",
      "Epoch: 7/20; Batch:99/468; Training loss:0.100173\n",
      "Epoch: 7/20; Batch:100/468; Training loss:0.104343\n",
      "Epoch: 7/20; Batch:101/468; Training loss:0.106617\n",
      "Epoch: 7/20; Batch:102/468; Training loss:0.10214\n",
      "Epoch: 7/20; Batch:103/468; Training loss:0.103688\n",
      "Epoch: 7/20; Batch:104/468; Training loss:0.104537\n",
      "Epoch: 7/20; Batch:105/468; Training loss:0.105475\n",
      "Epoch: 7/20; Batch:106/468; Training loss:0.10364\n",
      "Epoch: 7/20; Batch:107/468; Training loss:0.100464\n",
      "Epoch: 7/20; Batch:108/468; Training loss:0.100826\n",
      "Epoch: 7/20; Batch:109/468; Training loss:0.100668\n",
      "Epoch: 7/20; Batch:110/468; Training loss:0.10721\n",
      "Epoch: 7/20; Batch:111/468; Training loss:0.103239\n",
      "Epoch: 7/20; Batch:112/468; Training loss:0.10365\n",
      "Epoch: 7/20; Batch:113/468; Training loss:0.100179\n",
      "Epoch: 7/20; Batch:114/468; Training loss:0.105817\n",
      "Epoch: 7/20; Batch:115/468; Training loss:0.0988853\n",
      "Epoch: 7/20; Batch:116/468; Training loss:0.101507\n",
      "Epoch: 7/20; Batch:117/468; Training loss:0.10328\n",
      "Epoch: 7/20; Batch:118/468; Training loss:0.104424\n",
      "Epoch: 7/20; Batch:119/468; Training loss:0.103877\n",
      "Epoch: 7/20; Batch:120/468; Training loss:0.10594\n",
      "Epoch: 7/20; Batch:121/468; Training loss:0.100013\n",
      "Epoch: 7/20; Batch:122/468; Training loss:0.103982\n",
      "Epoch: 7/20; Batch:123/468; Training loss:0.100539\n",
      "Epoch: 7/20; Batch:124/468; Training loss:0.0978596\n",
      "Epoch: 7/20; Batch:125/468; Training loss:0.101324\n",
      "Epoch: 7/20; Batch:126/468; Training loss:0.0990523\n",
      "Epoch: 7/20; Batch:127/468; Training loss:0.103125\n",
      "Epoch: 7/20; Batch:128/468; Training loss:0.102304\n",
      "Epoch: 7/20; Batch:129/468; Training loss:0.104245\n",
      "Epoch: 7/20; Batch:130/468; Training loss:0.10451\n",
      "Epoch: 7/20; Batch:131/468; Training loss:0.104512\n",
      "Epoch: 7/20; Batch:132/468; Training loss:0.101234\n",
      "Epoch: 7/20; Batch:133/468; Training loss:0.104653\n",
      "Epoch: 7/20; Batch:134/468; Training loss:0.101758\n",
      "Epoch: 7/20; Batch:135/468; Training loss:0.0975903\n",
      "Epoch: 7/20; Batch:136/468; Training loss:0.104048\n",
      "Epoch: 7/20; Batch:137/468; Training loss:0.106951\n",
      "Epoch: 7/20; Batch:138/468; Training loss:0.105058\n",
      "Epoch: 7/20; Batch:139/468; Training loss:0.108103\n",
      "Epoch: 7/20; Batch:140/468; Training loss:0.103774\n",
      "Epoch: 7/20; Batch:141/468; Training loss:0.105714\n",
      "Epoch: 7/20; Batch:142/468; Training loss:0.104432\n",
      "Epoch: 7/20; Batch:143/468; Training loss:0.102205\n",
      "Epoch: 7/20; Batch:144/468; Training loss:0.100871\n",
      "Epoch: 7/20; Batch:145/468; Training loss:0.101353\n",
      "Epoch: 7/20; Batch:146/468; Training loss:0.101031\n",
      "Epoch: 7/20; Batch:147/468; Training loss:0.0969204\n",
      "Epoch: 7/20; Batch:148/468; Training loss:0.105154\n",
      "Epoch: 7/20; Batch:149/468; Training loss:0.105176\n",
      "Epoch: 7/20; Batch:150/468; Training loss:0.0981521\n",
      "Epoch: 7/20; Batch:151/468; Training loss:0.106232\n",
      "Epoch: 7/20; Batch:152/468; Training loss:0.100673\n",
      "Epoch: 7/20; Batch:153/468; Training loss:0.102609\n",
      "Epoch: 7/20; Batch:154/468; Training loss:0.105755\n",
      "Epoch: 7/20; Batch:155/468; Training loss:0.103767\n",
      "Epoch: 7/20; Batch:156/468; Training loss:0.103631\n",
      "Epoch: 7/20; Batch:157/468; Training loss:0.1025\n",
      "Epoch: 7/20; Batch:158/468; Training loss:0.102828\n",
      "Epoch: 7/20; Batch:159/468; Training loss:0.100009\n",
      "Epoch: 7/20; Batch:160/468; Training loss:0.10236\n",
      "Epoch: 7/20; Batch:161/468; Training loss:0.0972663\n",
      "Epoch: 7/20; Batch:162/468; Training loss:0.103249\n",
      "Epoch: 7/20; Batch:163/468; Training loss:0.101075\n",
      "Epoch: 7/20; Batch:164/468; Training loss:0.0987528\n",
      "Epoch: 7/20; Batch:165/468; Training loss:0.100204\n",
      "Epoch: 7/20; Batch:166/468; Training loss:0.102784\n",
      "Epoch: 7/20; Batch:167/468; Training loss:0.107123\n",
      "Epoch: 7/20; Batch:168/468; Training loss:0.101719\n",
      "Epoch: 7/20; Batch:169/468; Training loss:0.103265\n",
      "Epoch: 7/20; Batch:170/468; Training loss:0.10187\n",
      "Epoch: 7/20; Batch:171/468; Training loss:0.104334\n",
      "Epoch: 7/20; Batch:172/468; Training loss:0.103454\n",
      "Epoch: 7/20; Batch:173/468; Training loss:0.100768\n",
      "Epoch: 7/20; Batch:174/468; Training loss:0.0963316\n",
      "Epoch: 7/20; Batch:175/468; Training loss:0.105171\n",
      "Epoch: 7/20; Batch:176/468; Training loss:0.101112\n",
      "Epoch: 7/20; Batch:177/468; Training loss:0.100609\n",
      "Epoch: 7/20; Batch:178/468; Training loss:0.102717\n",
      "Epoch: 7/20; Batch:179/468; Training loss:0.101573\n",
      "Epoch: 7/20; Batch:180/468; Training loss:0.101482\n",
      "Epoch: 7/20; Batch:181/468; Training loss:0.102502\n",
      "Epoch: 7/20; Batch:182/468; Training loss:0.101309\n",
      "Epoch: 7/20; Batch:183/468; Training loss:0.0974577\n",
      "Epoch: 7/20; Batch:184/468; Training loss:0.0994826\n",
      "Epoch: 7/20; Batch:185/468; Training loss:0.105917\n",
      "Epoch: 7/20; Batch:186/468; Training loss:0.103593\n",
      "Epoch: 7/20; Batch:187/468; Training loss:0.104226\n",
      "Epoch: 7/20; Batch:188/468; Training loss:0.103955\n",
      "Epoch: 7/20; Batch:189/468; Training loss:0.101673\n",
      "Epoch: 7/20; Batch:190/468; Training loss:0.101471\n",
      "Epoch: 7/20; Batch:191/468; Training loss:0.104033\n",
      "Epoch: 7/20; Batch:192/468; Training loss:0.102598\n",
      "Epoch: 7/20; Batch:193/468; Training loss:0.102462\n",
      "Epoch: 7/20; Batch:194/468; Training loss:0.10189\n",
      "Epoch: 7/20; Batch:195/468; Training loss:0.108207\n",
      "Epoch: 7/20; Batch:196/468; Training loss:0.108733\n",
      "Epoch: 7/20; Batch:197/468; Training loss:0.102117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20; Batch:198/468; Training loss:0.101629\n",
      "Epoch: 7/20; Batch:199/468; Training loss:0.101986\n",
      "Epoch: 7/20; Batch:200/468; Training loss:0.102574\n",
      "Epoch: 7/20; Batch:201/468; Training loss:0.103809\n",
      "Epoch: 7/20; Batch:202/468; Training loss:0.103373\n",
      "Epoch: 7/20; Batch:203/468; Training loss:0.101808\n",
      "Epoch: 7/20; Batch:204/468; Training loss:0.0982976\n",
      "Epoch: 7/20; Batch:205/468; Training loss:0.102183\n",
      "Epoch: 7/20; Batch:206/468; Training loss:0.101766\n",
      "Epoch: 7/20; Batch:207/468; Training loss:0.102972\n",
      "Epoch: 7/20; Batch:208/468; Training loss:0.0972537\n",
      "Epoch: 7/20; Batch:209/468; Training loss:0.100264\n",
      "Epoch: 7/20; Batch:210/468; Training loss:0.0987348\n",
      "Epoch: 7/20; Batch:211/468; Training loss:0.100397\n",
      "Epoch: 7/20; Batch:212/468; Training loss:0.102062\n",
      "Epoch: 7/20; Batch:213/468; Training loss:0.103828\n",
      "Epoch: 7/20; Batch:214/468; Training loss:0.10363\n",
      "Epoch: 7/20; Batch:215/468; Training loss:0.101603\n",
      "Epoch: 7/20; Batch:216/468; Training loss:0.104927\n",
      "Epoch: 7/20; Batch:217/468; Training loss:0.10253\n",
      "Epoch: 7/20; Batch:218/468; Training loss:0.100358\n",
      "Epoch: 7/20; Batch:219/468; Training loss:0.106763\n",
      "Epoch: 7/20; Batch:220/468; Training loss:0.105128\n",
      "Epoch: 7/20; Batch:221/468; Training loss:0.102126\n",
      "Epoch: 7/20; Batch:222/468; Training loss:0.101889\n",
      "Epoch: 7/20; Batch:223/468; Training loss:0.105346\n",
      "Epoch: 7/20; Batch:224/468; Training loss:0.103505\n",
      "Epoch: 7/20; Batch:225/468; Training loss:0.101371\n",
      "Epoch: 7/20; Batch:226/468; Training loss:0.101491\n",
      "Epoch: 7/20; Batch:227/468; Training loss:0.10457\n",
      "Epoch: 7/20; Batch:228/468; Training loss:0.100659\n",
      "Epoch: 7/20; Batch:229/468; Training loss:0.103866\n",
      "Epoch: 7/20; Batch:230/468; Training loss:0.0994788\n",
      "Epoch: 7/20; Batch:231/468; Training loss:0.109622\n",
      "Epoch: 7/20; Batch:232/468; Training loss:0.101498\n",
      "Epoch: 7/20; Batch:233/468; Training loss:0.105151\n",
      "Epoch: 7/20; Batch:234/468; Training loss:0.10335\n",
      "Epoch: 7/20; Batch:235/468; Training loss:0.106874\n",
      "Epoch: 7/20; Batch:236/468; Training loss:0.105148\n",
      "Epoch: 7/20; Batch:237/468; Training loss:0.102496\n",
      "Epoch: 7/20; Batch:238/468; Training loss:0.0987996\n",
      "Epoch: 7/20; Batch:239/468; Training loss:0.103726\n",
      "Epoch: 7/20; Batch:240/468; Training loss:0.102708\n",
      "Epoch: 7/20; Batch:241/468; Training loss:0.103976\n",
      "Epoch: 7/20; Batch:242/468; Training loss:0.1017\n",
      "Epoch: 7/20; Batch:243/468; Training loss:0.1051\n",
      "Epoch: 7/20; Batch:244/468; Training loss:0.0975103\n",
      "Epoch: 7/20; Batch:245/468; Training loss:0.103069\n",
      "Epoch: 7/20; Batch:246/468; Training loss:0.102259\n",
      "Epoch: 7/20; Batch:247/468; Training loss:0.105741\n",
      "Epoch: 7/20; Batch:248/468; Training loss:0.100542\n",
      "Epoch: 7/20; Batch:249/468; Training loss:0.0993608\n",
      "Epoch: 7/20; Batch:250/468; Training loss:0.100921\n",
      "Epoch: 7/20; Batch:251/468; Training loss:0.107013\n",
      "Epoch: 7/20; Batch:252/468; Training loss:0.096017\n",
      "Epoch: 7/20; Batch:253/468; Training loss:0.104904\n",
      "Epoch: 7/20; Batch:254/468; Training loss:0.106316\n",
      "Epoch: 7/20; Batch:255/468; Training loss:0.103742\n",
      "Epoch: 7/20; Batch:256/468; Training loss:0.10246\n",
      "Epoch: 7/20; Batch:257/468; Training loss:0.0965517\n",
      "Epoch: 7/20; Batch:258/468; Training loss:0.0970219\n",
      "Epoch: 7/20; Batch:259/468; Training loss:0.10072\n",
      "Epoch: 7/20; Batch:260/468; Training loss:0.103354\n",
      "Epoch: 7/20; Batch:261/468; Training loss:0.0988459\n",
      "Epoch: 7/20; Batch:262/468; Training loss:0.100577\n",
      "Epoch: 7/20; Batch:263/468; Training loss:0.10168\n",
      "Epoch: 7/20; Batch:264/468; Training loss:0.102098\n",
      "Epoch: 7/20; Batch:265/468; Training loss:0.10492\n",
      "Epoch: 7/20; Batch:266/468; Training loss:0.101367\n",
      "Epoch: 7/20; Batch:267/468; Training loss:0.106452\n",
      "Epoch: 7/20; Batch:268/468; Training loss:0.105871\n",
      "Epoch: 7/20; Batch:269/468; Training loss:0.106905\n",
      "Epoch: 7/20; Batch:270/468; Training loss:0.101657\n",
      "Epoch: 7/20; Batch:271/468; Training loss:0.107168\n",
      "Epoch: 7/20; Batch:272/468; Training loss:0.099758\n",
      "Epoch: 7/20; Batch:273/468; Training loss:0.101433\n",
      "Epoch: 7/20; Batch:274/468; Training loss:0.101247\n",
      "Epoch: 7/20; Batch:275/468; Training loss:0.102283\n",
      "Epoch: 7/20; Batch:276/468; Training loss:0.098514\n",
      "Epoch: 7/20; Batch:277/468; Training loss:0.098805\n",
      "Epoch: 7/20; Batch:278/468; Training loss:0.103622\n",
      "Epoch: 7/20; Batch:279/468; Training loss:0.106472\n",
      "Epoch: 7/20; Batch:280/468; Training loss:0.100943\n",
      "Epoch: 7/20; Batch:281/468; Training loss:0.0998989\n",
      "Epoch: 7/20; Batch:282/468; Training loss:0.104615\n",
      "Epoch: 7/20; Batch:283/468; Training loss:0.101967\n",
      "Epoch: 7/20; Batch:284/468; Training loss:0.101352\n",
      "Epoch: 7/20; Batch:285/468; Training loss:0.0988356\n",
      "Epoch: 7/20; Batch:286/468; Training loss:0.106237\n",
      "Epoch: 7/20; Batch:287/468; Training loss:0.10255\n",
      "Epoch: 7/20; Batch:288/468; Training loss:0.0983617\n",
      "Epoch: 7/20; Batch:289/468; Training loss:0.103147\n",
      "Epoch: 7/20; Batch:290/468; Training loss:0.0991649\n",
      "Epoch: 7/20; Batch:291/468; Training loss:0.0993592\n",
      "Epoch: 7/20; Batch:292/468; Training loss:0.0976013\n",
      "Epoch: 7/20; Batch:293/468; Training loss:0.100986\n",
      "Epoch: 7/20; Batch:294/468; Training loss:0.0974403\n",
      "Epoch: 7/20; Batch:295/468; Training loss:0.0984238\n",
      "Epoch: 7/20; Batch:296/468; Training loss:0.102924\n",
      "Epoch: 7/20; Batch:297/468; Training loss:0.102073\n",
      "Epoch: 7/20; Batch:298/468; Training loss:0.100196\n",
      "Epoch: 7/20; Batch:299/468; Training loss:0.10053\n",
      "Epoch: 7/20; Batch:300/468; Training loss:0.0986442\n",
      "Epoch: 7/20; Batch:301/468; Training loss:0.106968\n",
      "Epoch: 7/20; Batch:302/468; Training loss:0.106815\n",
      "Epoch: 7/20; Batch:303/468; Training loss:0.102378\n",
      "Epoch: 7/20; Batch:304/468; Training loss:0.10162\n",
      "Epoch: 7/20; Batch:305/468; Training loss:0.099826\n",
      "Epoch: 7/20; Batch:306/468; Training loss:0.101154\n",
      "Epoch: 7/20; Batch:307/468; Training loss:0.100922\n",
      "Epoch: 7/20; Batch:308/468; Training loss:0.100079\n",
      "Epoch: 7/20; Batch:309/468; Training loss:0.0986048\n",
      "Epoch: 7/20; Batch:310/468; Training loss:0.102078\n",
      "Epoch: 7/20; Batch:311/468; Training loss:0.103043\n",
      "Epoch: 7/20; Batch:312/468; Training loss:0.0984787\n",
      "Epoch: 7/20; Batch:313/468; Training loss:0.100892\n",
      "Epoch: 7/20; Batch:314/468; Training loss:0.100583\n",
      "Epoch: 7/20; Batch:315/468; Training loss:0.101257\n",
      "Epoch: 7/20; Batch:316/468; Training loss:0.101425\n",
      "Epoch: 7/20; Batch:317/468; Training loss:0.0986725\n",
      "Epoch: 7/20; Batch:318/468; Training loss:0.103399\n",
      "Epoch: 7/20; Batch:319/468; Training loss:0.0996397\n",
      "Epoch: 7/20; Batch:320/468; Training loss:0.0971835\n",
      "Epoch: 7/20; Batch:321/468; Training loss:0.105565\n",
      "Epoch: 7/20; Batch:322/468; Training loss:0.103802\n",
      "Epoch: 7/20; Batch:323/468; Training loss:0.103101\n",
      "Epoch: 7/20; Batch:324/468; Training loss:0.100392\n",
      "Epoch: 7/20; Batch:325/468; Training loss:0.102964\n",
      "Epoch: 7/20; Batch:326/468; Training loss:0.102544\n",
      "Epoch: 7/20; Batch:327/468; Training loss:0.10033\n",
      "Epoch: 7/20; Batch:328/468; Training loss:0.101472\n",
      "Epoch: 7/20; Batch:329/468; Training loss:0.102112\n",
      "Epoch: 7/20; Batch:330/468; Training loss:0.100807\n",
      "Epoch: 7/20; Batch:331/468; Training loss:0.101053\n",
      "Epoch: 7/20; Batch:332/468; Training loss:0.104016\n",
      "Epoch: 7/20; Batch:333/468; Training loss:0.103062\n",
      "Epoch: 7/20; Batch:334/468; Training loss:0.102087\n",
      "Epoch: 7/20; Batch:335/468; Training loss:0.102999\n",
      "Epoch: 7/20; Batch:336/468; Training loss:0.102189\n",
      "Epoch: 7/20; Batch:337/468; Training loss:0.102119\n",
      "Epoch: 7/20; Batch:338/468; Training loss:0.101859\n",
      "Epoch: 7/20; Batch:339/468; Training loss:0.102043\n",
      "Epoch: 7/20; Batch:340/468; Training loss:0.100168\n",
      "Epoch: 7/20; Batch:341/468; Training loss:0.10795\n",
      "Epoch: 7/20; Batch:342/468; Training loss:0.102216\n",
      "Epoch: 7/20; Batch:343/468; Training loss:0.100484\n",
      "Epoch: 7/20; Batch:344/468; Training loss:0.10462\n",
      "Epoch: 7/20; Batch:345/468; Training loss:0.0981824\n",
      "Epoch: 7/20; Batch:346/468; Training loss:0.101704\n",
      "Epoch: 7/20; Batch:347/468; Training loss:0.102292\n",
      "Epoch: 7/20; Batch:348/468; Training loss:0.101409\n",
      "Epoch: 7/20; Batch:349/468; Training loss:0.0951076\n",
      "Epoch: 7/20; Batch:350/468; Training loss:0.0991552\n",
      "Epoch: 7/20; Batch:351/468; Training loss:0.101728\n",
      "Epoch: 7/20; Batch:352/468; Training loss:0.0996973\n",
      "Epoch: 7/20; Batch:353/468; Training loss:0.104376\n",
      "Epoch: 7/20; Batch:354/468; Training loss:0.102009\n",
      "Epoch: 7/20; Batch:355/468; Training loss:0.100047\n",
      "Epoch: 7/20; Batch:356/468; Training loss:0.0977901\n",
      "Epoch: 7/20; Batch:357/468; Training loss:0.105463\n",
      "Epoch: 7/20; Batch:358/468; Training loss:0.102736\n",
      "Epoch: 7/20; Batch:359/468; Training loss:0.102797\n",
      "Epoch: 7/20; Batch:360/468; Training loss:0.104554\n",
      "Epoch: 7/20; Batch:361/468; Training loss:0.101773\n",
      "Epoch: 7/20; Batch:362/468; Training loss:0.106341\n",
      "Epoch: 7/20; Batch:363/468; Training loss:0.102301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20; Batch:364/468; Training loss:0.102891\n",
      "Epoch: 7/20; Batch:365/468; Training loss:0.108725\n",
      "Epoch: 7/20; Batch:366/468; Training loss:0.0991717\n",
      "Epoch: 7/20; Batch:367/468; Training loss:0.102268\n",
      "Epoch: 7/20; Batch:368/468; Training loss:0.103101\n",
      "Epoch: 7/20; Batch:369/468; Training loss:0.10058\n",
      "Epoch: 7/20; Batch:370/468; Training loss:0.103462\n",
      "Epoch: 7/20; Batch:371/468; Training loss:0.10165\n",
      "Epoch: 7/20; Batch:372/468; Training loss:0.109544\n",
      "Epoch: 7/20; Batch:373/468; Training loss:0.108332\n",
      "Epoch: 7/20; Batch:374/468; Training loss:0.102872\n",
      "Epoch: 7/20; Batch:375/468; Training loss:0.104278\n",
      "Epoch: 7/20; Batch:376/468; Training loss:0.0984413\n",
      "Epoch: 7/20; Batch:377/468; Training loss:0.101276\n",
      "Epoch: 7/20; Batch:378/468; Training loss:0.103217\n",
      "Epoch: 7/20; Batch:379/468; Training loss:0.0989433\n",
      "Epoch: 7/20; Batch:380/468; Training loss:0.101483\n",
      "Epoch: 7/20; Batch:381/468; Training loss:0.102906\n",
      "Epoch: 7/20; Batch:382/468; Training loss:0.102667\n",
      "Epoch: 7/20; Batch:383/468; Training loss:0.0989981\n",
      "Epoch: 7/20; Batch:384/468; Training loss:0.102562\n",
      "Epoch: 7/20; Batch:385/468; Training loss:0.101261\n",
      "Epoch: 7/20; Batch:386/468; Training loss:0.101495\n",
      "Epoch: 7/20; Batch:387/468; Training loss:0.0988018\n",
      "Epoch: 7/20; Batch:388/468; Training loss:0.100965\n",
      "Epoch: 7/20; Batch:389/468; Training loss:0.103602\n",
      "Epoch: 7/20; Batch:390/468; Training loss:0.101192\n",
      "Epoch: 7/20; Batch:391/468; Training loss:0.105\n",
      "Epoch: 7/20; Batch:392/468; Training loss:0.100546\n",
      "Epoch: 7/20; Batch:393/468; Training loss:0.104034\n",
      "Epoch: 7/20; Batch:394/468; Training loss:0.098212\n",
      "Epoch: 7/20; Batch:395/468; Training loss:0.101744\n",
      "Epoch: 7/20; Batch:396/468; Training loss:0.103672\n",
      "Epoch: 7/20; Batch:397/468; Training loss:0.102132\n",
      "Epoch: 7/20; Batch:398/468; Training loss:0.0994281\n",
      "Epoch: 7/20; Batch:399/468; Training loss:0.0970684\n",
      "Epoch: 7/20; Batch:400/468; Training loss:0.102231\n",
      "Epoch: 7/20; Batch:401/468; Training loss:0.102849\n",
      "Epoch: 7/20; Batch:402/468; Training loss:0.105973\n",
      "Epoch: 7/20; Batch:403/468; Training loss:0.101778\n",
      "Epoch: 7/20; Batch:404/468; Training loss:0.100559\n",
      "Epoch: 7/20; Batch:405/468; Training loss:0.0988864\n",
      "Epoch: 7/20; Batch:406/468; Training loss:0.106604\n",
      "Epoch: 7/20; Batch:407/468; Training loss:0.101662\n",
      "Epoch: 7/20; Batch:408/468; Training loss:0.102665\n",
      "Epoch: 7/20; Batch:409/468; Training loss:0.103467\n",
      "Epoch: 7/20; Batch:410/468; Training loss:0.100617\n",
      "Epoch: 7/20; Batch:411/468; Training loss:0.103665\n",
      "Epoch: 7/20; Batch:412/468; Training loss:0.101447\n",
      "Epoch: 7/20; Batch:413/468; Training loss:0.102582\n",
      "Epoch: 7/20; Batch:414/468; Training loss:0.101754\n",
      "Epoch: 7/20; Batch:415/468; Training loss:0.0976433\n",
      "Epoch: 7/20; Batch:416/468; Training loss:0.102632\n",
      "Epoch: 7/20; Batch:417/468; Training loss:0.101444\n",
      "Epoch: 7/20; Batch:418/468; Training loss:0.100263\n",
      "Epoch: 7/20; Batch:419/468; Training loss:0.101093\n",
      "Epoch: 7/20; Batch:420/468; Training loss:0.0999628\n",
      "Epoch: 7/20; Batch:421/468; Training loss:0.105324\n",
      "Epoch: 7/20; Batch:422/468; Training loss:0.10227\n",
      "Epoch: 7/20; Batch:423/468; Training loss:0.100981\n",
      "Epoch: 7/20; Batch:424/468; Training loss:0.0999653\n",
      "Epoch: 7/20; Batch:425/468; Training loss:0.100581\n",
      "Epoch: 7/20; Batch:426/468; Training loss:0.104642\n",
      "Epoch: 7/20; Batch:427/468; Training loss:0.0968639\n",
      "Epoch: 7/20; Batch:428/468; Training loss:0.103286\n",
      "Epoch: 7/20; Batch:429/468; Training loss:0.098308\n",
      "Epoch: 7/20; Batch:430/468; Training loss:0.102036\n",
      "Epoch: 7/20; Batch:431/468; Training loss:0.103095\n",
      "Epoch: 7/20; Batch:432/468; Training loss:0.102766\n",
      "Epoch: 7/20; Batch:433/468; Training loss:0.102963\n",
      "Epoch: 7/20; Batch:434/468; Training loss:0.0982995\n",
      "Epoch: 7/20; Batch:435/468; Training loss:0.10154\n",
      "Epoch: 7/20; Batch:436/468; Training loss:0.0972645\n",
      "Epoch: 7/20; Batch:437/468; Training loss:0.0987469\n",
      "Epoch: 7/20; Batch:438/468; Training loss:0.10134\n",
      "Epoch: 7/20; Batch:439/468; Training loss:0.10118\n",
      "Epoch: 7/20; Batch:440/468; Training loss:0.104348\n",
      "Epoch: 7/20; Batch:441/468; Training loss:0.0983689\n",
      "Epoch: 7/20; Batch:442/468; Training loss:0.103227\n",
      "Epoch: 7/20; Batch:443/468; Training loss:0.102497\n",
      "Epoch: 7/20; Batch:444/468; Training loss:0.104124\n",
      "Epoch: 7/20; Batch:445/468; Training loss:0.105607\n",
      "Epoch: 7/20; Batch:446/468; Training loss:0.0961836\n",
      "Epoch: 7/20; Batch:447/468; Training loss:0.0995087\n",
      "Epoch: 7/20; Batch:448/468; Training loss:0.101403\n",
      "Epoch: 7/20; Batch:449/468; Training loss:0.101767\n",
      "Epoch: 7/20; Batch:450/468; Training loss:0.103332\n",
      "Epoch: 7/20; Batch:451/468; Training loss:0.101208\n",
      "Epoch: 7/20; Batch:452/468; Training loss:0.102157\n",
      "Epoch: 7/20; Batch:453/468; Training loss:0.101399\n",
      "Epoch: 7/20; Batch:454/468; Training loss:0.0996646\n",
      "Epoch: 7/20; Batch:455/468; Training loss:0.103041\n",
      "Epoch: 7/20; Batch:456/468; Training loss:0.0987993\n",
      "Epoch: 7/20; Batch:457/468; Training loss:0.105434\n",
      "Epoch: 7/20; Batch:458/468; Training loss:0.102472\n",
      "Epoch: 7/20; Batch:459/468; Training loss:0.0996913\n",
      "Epoch: 7/20; Batch:460/468; Training loss:0.106222\n",
      "Epoch: 7/20; Batch:461/468; Training loss:0.104485\n",
      "Epoch: 7/20; Batch:462/468; Training loss:0.101886\n",
      "Epoch: 7/20; Batch:463/468; Training loss:0.102025\n",
      "Epoch: 7/20; Batch:464/468; Training loss:0.102228\n",
      "Epoch: 7/20; Batch:465/468; Training loss:0.0974873\n",
      "Epoch: 7/20; Batch:466/468; Training loss:0.0938485\n",
      "Epoch: 7/20; Batch:467/468; Training loss:0.103107\n",
      "Epoch: 7/20; Batch:468/468; Training loss:0.102382\n",
      "Epoch: 8/20; Batch:1/468; Training loss:0.0981228\n",
      "Epoch: 8/20; Batch:2/468; Training loss:0.102946\n",
      "Epoch: 8/20; Batch:3/468; Training loss:0.102584\n",
      "Epoch: 8/20; Batch:4/468; Training loss:0.101186\n",
      "Epoch: 8/20; Batch:5/468; Training loss:0.0999686\n",
      "Epoch: 8/20; Batch:6/468; Training loss:0.0957267\n",
      "Epoch: 8/20; Batch:7/468; Training loss:0.101703\n",
      "Epoch: 8/20; Batch:8/468; Training loss:0.102227\n",
      "Epoch: 8/20; Batch:9/468; Training loss:0.103403\n",
      "Epoch: 8/20; Batch:10/468; Training loss:0.100945\n",
      "Epoch: 8/20; Batch:11/468; Training loss:0.101777\n",
      "Epoch: 8/20; Batch:12/468; Training loss:0.104818\n",
      "Epoch: 8/20; Batch:13/468; Training loss:0.101212\n",
      "Epoch: 8/20; Batch:14/468; Training loss:0.10097\n",
      "Epoch: 8/20; Batch:15/468; Training loss:0.0983231\n",
      "Epoch: 8/20; Batch:16/468; Training loss:0.0951929\n",
      "Epoch: 8/20; Batch:17/468; Training loss:0.0980184\n",
      "Epoch: 8/20; Batch:18/468; Training loss:0.102522\n",
      "Epoch: 8/20; Batch:19/468; Training loss:0.100184\n",
      "Epoch: 8/20; Batch:20/468; Training loss:0.102022\n",
      "Epoch: 8/20; Batch:21/468; Training loss:0.101012\n",
      "Epoch: 8/20; Batch:22/468; Training loss:0.0994525\n",
      "Epoch: 8/20; Batch:23/468; Training loss:0.103935\n",
      "Epoch: 8/20; Batch:24/468; Training loss:0.102396\n",
      "Epoch: 8/20; Batch:25/468; Training loss:0.0980725\n",
      "Epoch: 8/20; Batch:26/468; Training loss:0.102833\n",
      "Epoch: 8/20; Batch:27/468; Training loss:0.100465\n",
      "Epoch: 8/20; Batch:28/468; Training loss:0.103491\n",
      "Epoch: 8/20; Batch:29/468; Training loss:0.102222\n",
      "Epoch: 8/20; Batch:30/468; Training loss:0.099723\n",
      "Epoch: 8/20; Batch:31/468; Training loss:0.100563\n",
      "Epoch: 8/20; Batch:32/468; Training loss:0.103444\n",
      "Epoch: 8/20; Batch:33/468; Training loss:0.0990708\n",
      "Epoch: 8/20; Batch:34/468; Training loss:0.0995317\n",
      "Epoch: 8/20; Batch:35/468; Training loss:0.105472\n",
      "Epoch: 8/20; Batch:36/468; Training loss:0.103942\n",
      "Epoch: 8/20; Batch:37/468; Training loss:0.100825\n",
      "Epoch: 8/20; Batch:38/468; Training loss:0.0999787\n",
      "Epoch: 8/20; Batch:39/468; Training loss:0.0983955\n",
      "Epoch: 8/20; Batch:40/468; Training loss:0.100239\n",
      "Epoch: 8/20; Batch:41/468; Training loss:0.0994494\n",
      "Epoch: 8/20; Batch:42/468; Training loss:0.0970339\n",
      "Epoch: 8/20; Batch:43/468; Training loss:0.102365\n",
      "Epoch: 8/20; Batch:44/468; Training loss:0.0993079\n",
      "Epoch: 8/20; Batch:45/468; Training loss:0.103219\n",
      "Epoch: 8/20; Batch:46/468; Training loss:0.10435\n",
      "Epoch: 8/20; Batch:47/468; Training loss:0.103965\n",
      "Epoch: 8/20; Batch:48/468; Training loss:0.102152\n",
      "Epoch: 8/20; Batch:49/468; Training loss:0.101367\n",
      "Epoch: 8/20; Batch:50/468; Training loss:0.100159\n",
      "Epoch: 8/20; Batch:51/468; Training loss:0.101586\n",
      "Epoch: 8/20; Batch:52/468; Training loss:0.102296\n",
      "Epoch: 8/20; Batch:53/468; Training loss:0.10147\n",
      "Epoch: 8/20; Batch:54/468; Training loss:0.103547\n",
      "Epoch: 8/20; Batch:55/468; Training loss:0.10075\n",
      "Epoch: 8/20; Batch:56/468; Training loss:0.102435\n",
      "Epoch: 8/20; Batch:57/468; Training loss:0.101428\n",
      "Epoch: 8/20; Batch:58/468; Training loss:0.106011\n",
      "Epoch: 8/20; Batch:59/468; Training loss:0.103448\n",
      "Epoch: 8/20; Batch:60/468; Training loss:0.100225\n",
      "Epoch: 8/20; Batch:61/468; Training loss:0.105577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20; Batch:62/468; Training loss:0.101297\n",
      "Epoch: 8/20; Batch:63/468; Training loss:0.103251\n",
      "Epoch: 8/20; Batch:64/468; Training loss:0.0988968\n",
      "Epoch: 8/20; Batch:65/468; Training loss:0.104628\n",
      "Epoch: 8/20; Batch:66/468; Training loss:0.102674\n",
      "Epoch: 8/20; Batch:67/468; Training loss:0.104801\n",
      "Epoch: 8/20; Batch:68/468; Training loss:0.0997184\n",
      "Epoch: 8/20; Batch:69/468; Training loss:0.102438\n",
      "Epoch: 8/20; Batch:70/468; Training loss:0.102105\n",
      "Epoch: 8/20; Batch:71/468; Training loss:0.103854\n",
      "Epoch: 8/20; Batch:72/468; Training loss:0.100799\n",
      "Epoch: 8/20; Batch:73/468; Training loss:0.100649\n",
      "Epoch: 8/20; Batch:74/468; Training loss:0.102651\n",
      "Epoch: 8/20; Batch:75/468; Training loss:0.10345\n",
      "Epoch: 8/20; Batch:76/468; Training loss:0.103157\n",
      "Epoch: 8/20; Batch:77/468; Training loss:0.100577\n",
      "Epoch: 8/20; Batch:78/468; Training loss:0.103722\n",
      "Epoch: 8/20; Batch:79/468; Training loss:0.101946\n",
      "Epoch: 8/20; Batch:80/468; Training loss:0.0985033\n",
      "Epoch: 8/20; Batch:81/468; Training loss:0.102718\n",
      "Epoch: 8/20; Batch:82/468; Training loss:0.103113\n",
      "Epoch: 8/20; Batch:83/468; Training loss:0.0997548\n",
      "Epoch: 8/20; Batch:84/468; Training loss:0.102718\n",
      "Epoch: 8/20; Batch:85/468; Training loss:0.102795\n",
      "Epoch: 8/20; Batch:86/468; Training loss:0.101205\n",
      "Epoch: 8/20; Batch:87/468; Training loss:0.104743\n",
      "Epoch: 8/20; Batch:88/468; Training loss:0.0984933\n",
      "Epoch: 8/20; Batch:89/468; Training loss:0.105031\n",
      "Epoch: 8/20; Batch:90/468; Training loss:0.106472\n",
      "Epoch: 8/20; Batch:91/468; Training loss:0.100737\n",
      "Epoch: 8/20; Batch:92/468; Training loss:0.107364\n",
      "Epoch: 8/20; Batch:93/468; Training loss:0.102278\n",
      "Epoch: 8/20; Batch:94/468; Training loss:0.102528\n",
      "Epoch: 8/20; Batch:95/468; Training loss:0.103486\n",
      "Epoch: 8/20; Batch:96/468; Training loss:0.105079\n",
      "Epoch: 8/20; Batch:97/468; Training loss:0.101314\n",
      "Epoch: 8/20; Batch:98/468; Training loss:0.0996077\n",
      "Epoch: 8/20; Batch:99/468; Training loss:0.10098\n",
      "Epoch: 8/20; Batch:100/468; Training loss:0.0999094\n",
      "Epoch: 8/20; Batch:101/468; Training loss:0.100952\n",
      "Epoch: 8/20; Batch:102/468; Training loss:0.0992491\n",
      "Epoch: 8/20; Batch:103/468; Training loss:0.102133\n",
      "Epoch: 8/20; Batch:104/468; Training loss:0.0993504\n",
      "Epoch: 8/20; Batch:105/468; Training loss:0.100057\n",
      "Epoch: 8/20; Batch:106/468; Training loss:0.0957169\n",
      "Epoch: 8/20; Batch:107/468; Training loss:0.104428\n",
      "Epoch: 8/20; Batch:108/468; Training loss:0.101571\n",
      "Epoch: 8/20; Batch:109/468; Training loss:0.104983\n",
      "Epoch: 8/20; Batch:110/468; Training loss:0.103523\n",
      "Epoch: 8/20; Batch:111/468; Training loss:0.104605\n",
      "Epoch: 8/20; Batch:112/468; Training loss:0.101114\n",
      "Epoch: 8/20; Batch:113/468; Training loss:0.0958201\n",
      "Epoch: 8/20; Batch:114/468; Training loss:0.103565\n",
      "Epoch: 8/20; Batch:115/468; Training loss:0.101097\n",
      "Epoch: 8/20; Batch:116/468; Training loss:0.102597\n",
      "Epoch: 8/20; Batch:117/468; Training loss:0.100484\n",
      "Epoch: 8/20; Batch:118/468; Training loss:0.101216\n",
      "Epoch: 8/20; Batch:119/468; Training loss:0.0960317\n",
      "Epoch: 8/20; Batch:120/468; Training loss:0.102041\n",
      "Epoch: 8/20; Batch:121/468; Training loss:0.0973432\n",
      "Epoch: 8/20; Batch:122/468; Training loss:0.0988361\n",
      "Epoch: 8/20; Batch:123/468; Training loss:0.103142\n",
      "Epoch: 8/20; Batch:124/468; Training loss:0.100336\n",
      "Epoch: 8/20; Batch:125/468; Training loss:0.102883\n",
      "Epoch: 8/20; Batch:126/468; Training loss:0.101855\n",
      "Epoch: 8/20; Batch:127/468; Training loss:0.101913\n",
      "Epoch: 8/20; Batch:128/468; Training loss:0.10035\n",
      "Epoch: 8/20; Batch:129/468; Training loss:0.0972347\n",
      "Epoch: 8/20; Batch:130/468; Training loss:0.103543\n",
      "Epoch: 8/20; Batch:131/468; Training loss:0.0989638\n",
      "Epoch: 8/20; Batch:132/468; Training loss:0.104819\n",
      "Epoch: 8/20; Batch:133/468; Training loss:0.101255\n",
      "Epoch: 8/20; Batch:134/468; Training loss:0.102356\n",
      "Epoch: 8/20; Batch:135/468; Training loss:0.102186\n",
      "Epoch: 8/20; Batch:136/468; Training loss:0.101276\n",
      "Epoch: 8/20; Batch:137/468; Training loss:0.0970961\n",
      "Epoch: 8/20; Batch:138/468; Training loss:0.0979297\n",
      "Epoch: 8/20; Batch:139/468; Training loss:0.10522\n",
      "Epoch: 8/20; Batch:140/468; Training loss:0.10386\n",
      "Epoch: 8/20; Batch:141/468; Training loss:0.101136\n",
      "Epoch: 8/20; Batch:142/468; Training loss:0.103346\n",
      "Epoch: 8/20; Batch:143/468; Training loss:0.099132\n",
      "Epoch: 8/20; Batch:144/468; Training loss:0.0998303\n",
      "Epoch: 8/20; Batch:145/468; Training loss:0.103557\n",
      "Epoch: 8/20; Batch:146/468; Training loss:0.100615\n",
      "Epoch: 8/20; Batch:147/468; Training loss:0.102424\n",
      "Epoch: 8/20; Batch:148/468; Training loss:0.100616\n",
      "Epoch: 8/20; Batch:149/468; Training loss:0.103177\n",
      "Epoch: 8/20; Batch:150/468; Training loss:0.100993\n",
      "Epoch: 8/20; Batch:151/468; Training loss:0.097951\n",
      "Epoch: 8/20; Batch:152/468; Training loss:0.101865\n",
      "Epoch: 8/20; Batch:153/468; Training loss:0.101957\n",
      "Epoch: 8/20; Batch:154/468; Training loss:0.103412\n",
      "Epoch: 8/20; Batch:155/468; Training loss:0.0997567\n",
      "Epoch: 8/20; Batch:156/468; Training loss:0.101258\n",
      "Epoch: 8/20; Batch:157/468; Training loss:0.104704\n",
      "Epoch: 8/20; Batch:158/468; Training loss:0.105209\n",
      "Epoch: 8/20; Batch:159/468; Training loss:0.102608\n",
      "Epoch: 8/20; Batch:160/468; Training loss:0.101432\n",
      "Epoch: 8/20; Batch:161/468; Training loss:0.105779\n",
      "Epoch: 8/20; Batch:162/468; Training loss:0.0986716\n",
      "Epoch: 8/20; Batch:163/468; Training loss:0.10011\n",
      "Epoch: 8/20; Batch:164/468; Training loss:0.103235\n",
      "Epoch: 8/20; Batch:165/468; Training loss:0.104349\n",
      "Epoch: 8/20; Batch:166/468; Training loss:0.0959616\n",
      "Epoch: 8/20; Batch:167/468; Training loss:0.101453\n",
      "Epoch: 8/20; Batch:168/468; Training loss:0.0982591\n",
      "Epoch: 8/20; Batch:169/468; Training loss:0.0980885\n",
      "Epoch: 8/20; Batch:170/468; Training loss:0.0991569\n",
      "Epoch: 8/20; Batch:171/468; Training loss:0.105496\n",
      "Epoch: 8/20; Batch:172/468; Training loss:0.105012\n",
      "Epoch: 8/20; Batch:173/468; Training loss:0.0983598\n",
      "Epoch: 8/20; Batch:174/468; Training loss:0.105013\n",
      "Epoch: 8/20; Batch:175/468; Training loss:0.0986172\n",
      "Epoch: 8/20; Batch:176/468; Training loss:0.0953881\n",
      "Epoch: 8/20; Batch:177/468; Training loss:0.101363\n",
      "Epoch: 8/20; Batch:178/468; Training loss:0.100703\n",
      "Epoch: 8/20; Batch:179/468; Training loss:0.0989066\n",
      "Epoch: 8/20; Batch:180/468; Training loss:0.0995663\n",
      "Epoch: 8/20; Batch:181/468; Training loss:0.103375\n",
      "Epoch: 8/20; Batch:182/468; Training loss:0.103101\n",
      "Epoch: 8/20; Batch:183/468; Training loss:0.0959729\n",
      "Epoch: 8/20; Batch:184/468; Training loss:0.101696\n",
      "Epoch: 8/20; Batch:185/468; Training loss:0.0987576\n",
      "Epoch: 8/20; Batch:186/468; Training loss:0.102301\n",
      "Epoch: 8/20; Batch:187/468; Training loss:0.100968\n",
      "Epoch: 8/20; Batch:188/468; Training loss:0.100253\n",
      "Epoch: 8/20; Batch:189/468; Training loss:0.101727\n",
      "Epoch: 8/20; Batch:190/468; Training loss:0.100221\n",
      "Epoch: 8/20; Batch:191/468; Training loss:0.100255\n",
      "Epoch: 8/20; Batch:192/468; Training loss:0.100153\n",
      "Epoch: 8/20; Batch:193/468; Training loss:0.105529\n",
      "Epoch: 8/20; Batch:194/468; Training loss:0.0973605\n",
      "Epoch: 8/20; Batch:195/468; Training loss:0.1007\n",
      "Epoch: 8/20; Batch:196/468; Training loss:0.0980118\n",
      "Epoch: 8/20; Batch:197/468; Training loss:0.103654\n",
      "Epoch: 8/20; Batch:198/468; Training loss:0.098305\n",
      "Epoch: 8/20; Batch:199/468; Training loss:0.102159\n",
      "Epoch: 8/20; Batch:200/468; Training loss:0.100558\n",
      "Epoch: 8/20; Batch:201/468; Training loss:0.101335\n",
      "Epoch: 8/20; Batch:202/468; Training loss:0.103147\n",
      "Epoch: 8/20; Batch:203/468; Training loss:0.103233\n",
      "Epoch: 8/20; Batch:204/468; Training loss:0.100418\n",
      "Epoch: 8/20; Batch:205/468; Training loss:0.104044\n",
      "Epoch: 8/20; Batch:206/468; Training loss:0.099971\n",
      "Epoch: 8/20; Batch:207/468; Training loss:0.106363\n",
      "Epoch: 8/20; Batch:208/468; Training loss:0.100357\n",
      "Epoch: 8/20; Batch:209/468; Training loss:0.0982566\n",
      "Epoch: 8/20; Batch:210/468; Training loss:0.102341\n",
      "Epoch: 8/20; Batch:211/468; Training loss:0.0990634\n",
      "Epoch: 8/20; Batch:212/468; Training loss:0.103703\n",
      "Epoch: 8/20; Batch:213/468; Training loss:0.102687\n",
      "Epoch: 8/20; Batch:214/468; Training loss:0.101504\n",
      "Epoch: 8/20; Batch:215/468; Training loss:0.102308\n",
      "Epoch: 8/20; Batch:216/468; Training loss:0.0973037\n",
      "Epoch: 8/20; Batch:217/468; Training loss:0.100105\n",
      "Epoch: 8/20; Batch:218/468; Training loss:0.098679\n",
      "Epoch: 8/20; Batch:219/468; Training loss:0.103046\n",
      "Epoch: 8/20; Batch:220/468; Training loss:0.105392\n",
      "Epoch: 8/20; Batch:221/468; Training loss:0.105165\n",
      "Epoch: 8/20; Batch:222/468; Training loss:0.101999\n",
      "Epoch: 8/20; Batch:223/468; Training loss:0.105527\n",
      "Epoch: 8/20; Batch:224/468; Training loss:0.102162\n",
      "Epoch: 8/20; Batch:225/468; Training loss:0.097738\n",
      "Epoch: 8/20; Batch:226/468; Training loss:0.101514\n",
      "Epoch: 8/20; Batch:227/468; Training loss:0.101043\n",
      "Epoch: 8/20; Batch:228/468; Training loss:0.0986323\n",
      "Epoch: 8/20; Batch:229/468; Training loss:0.0965596\n",
      "Epoch: 8/20; Batch:230/468; Training loss:0.10235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20; Batch:231/468; Training loss:0.0979002\n",
      "Epoch: 8/20; Batch:232/468; Training loss:0.0992036\n",
      "Epoch: 8/20; Batch:233/468; Training loss:0.101082\n",
      "Epoch: 8/20; Batch:234/468; Training loss:0.105629\n",
      "Epoch: 8/20; Batch:235/468; Training loss:0.100267\n",
      "Epoch: 8/20; Batch:236/468; Training loss:0.100013\n",
      "Epoch: 8/20; Batch:237/468; Training loss:0.104694\n",
      "Epoch: 8/20; Batch:238/468; Training loss:0.100925\n",
      "Epoch: 8/20; Batch:239/468; Training loss:0.0992277\n",
      "Epoch: 8/20; Batch:240/468; Training loss:0.103308\n",
      "Epoch: 8/20; Batch:241/468; Training loss:0.103501\n",
      "Epoch: 8/20; Batch:242/468; Training loss:0.103459\n",
      "Epoch: 8/20; Batch:243/468; Training loss:0.0969815\n",
      "Epoch: 8/20; Batch:244/468; Training loss:0.0994436\n",
      "Epoch: 8/20; Batch:245/468; Training loss:0.101929\n",
      "Epoch: 8/20; Batch:246/468; Training loss:0.100229\n",
      "Epoch: 8/20; Batch:247/468; Training loss:0.103136\n",
      "Epoch: 8/20; Batch:248/468; Training loss:0.103586\n",
      "Epoch: 8/20; Batch:249/468; Training loss:0.099399\n",
      "Epoch: 8/20; Batch:250/468; Training loss:0.101776\n",
      "Epoch: 8/20; Batch:251/468; Training loss:0.100777\n",
      "Epoch: 8/20; Batch:252/468; Training loss:0.102189\n",
      "Epoch: 8/20; Batch:253/468; Training loss:0.102621\n",
      "Epoch: 8/20; Batch:254/468; Training loss:0.103052\n",
      "Epoch: 8/20; Batch:255/468; Training loss:0.102639\n",
      "Epoch: 8/20; Batch:256/468; Training loss:0.105341\n",
      "Epoch: 8/20; Batch:257/468; Training loss:0.100943\n",
      "Epoch: 8/20; Batch:258/468; Training loss:0.10079\n",
      "Epoch: 8/20; Batch:259/468; Training loss:0.101895\n",
      "Epoch: 8/20; Batch:260/468; Training loss:0.101249\n",
      "Epoch: 8/20; Batch:261/468; Training loss:0.101934\n",
      "Epoch: 8/20; Batch:262/468; Training loss:0.10133\n",
      "Epoch: 8/20; Batch:263/468; Training loss:0.101424\n",
      "Epoch: 8/20; Batch:264/468; Training loss:0.103499\n",
      "Epoch: 8/20; Batch:265/468; Training loss:0.0977496\n",
      "Epoch: 8/20; Batch:266/468; Training loss:0.101348\n",
      "Epoch: 8/20; Batch:267/468; Training loss:0.101033\n",
      "Epoch: 8/20; Batch:268/468; Training loss:0.0994171\n",
      "Epoch: 8/20; Batch:269/468; Training loss:0.10382\n",
      "Epoch: 8/20; Batch:270/468; Training loss:0.100703\n",
      "Epoch: 8/20; Batch:271/468; Training loss:0.0963724\n",
      "Epoch: 8/20; Batch:272/468; Training loss:0.103105\n",
      "Epoch: 8/20; Batch:273/468; Training loss:0.102035\n",
      "Epoch: 8/20; Batch:274/468; Training loss:0.0992427\n",
      "Epoch: 8/20; Batch:275/468; Training loss:0.103609\n",
      "Epoch: 8/20; Batch:276/468; Training loss:0.100406\n",
      "Epoch: 8/20; Batch:277/468; Training loss:0.101982\n",
      "Epoch: 8/20; Batch:278/468; Training loss:0.102846\n",
      "Epoch: 8/20; Batch:279/468; Training loss:0.0997587\n",
      "Epoch: 8/20; Batch:280/468; Training loss:0.099982\n",
      "Epoch: 8/20; Batch:281/468; Training loss:0.104465\n",
      "Epoch: 8/20; Batch:282/468; Training loss:0.10156\n",
      "Epoch: 8/20; Batch:283/468; Training loss:0.102975\n",
      "Epoch: 8/20; Batch:284/468; Training loss:0.102383\n",
      "Epoch: 8/20; Batch:285/468; Training loss:0.105302\n",
      "Epoch: 8/20; Batch:286/468; Training loss:0.101086\n",
      "Epoch: 8/20; Batch:287/468; Training loss:0.0973786\n",
      "Epoch: 8/20; Batch:288/468; Training loss:0.105433\n",
      "Epoch: 8/20; Batch:289/468; Training loss:0.103921\n",
      "Epoch: 8/20; Batch:290/468; Training loss:0.100657\n",
      "Epoch: 8/20; Batch:291/468; Training loss:0.0980523\n",
      "Epoch: 8/20; Batch:292/468; Training loss:0.0969725\n",
      "Epoch: 8/20; Batch:293/468; Training loss:0.101579\n",
      "Epoch: 8/20; Batch:294/468; Training loss:0.0984625\n",
      "Epoch: 8/20; Batch:295/468; Training loss:0.0983226\n",
      "Epoch: 8/20; Batch:296/468; Training loss:0.102848\n",
      "Epoch: 8/20; Batch:297/468; Training loss:0.100524\n",
      "Epoch: 8/20; Batch:298/468; Training loss:0.0970856\n",
      "Epoch: 8/20; Batch:299/468; Training loss:0.102833\n",
      "Epoch: 8/20; Batch:300/468; Training loss:0.09756\n",
      "Epoch: 8/20; Batch:301/468; Training loss:0.104227\n",
      "Epoch: 8/20; Batch:302/468; Training loss:0.0998155\n",
      "Epoch: 8/20; Batch:303/468; Training loss:0.101706\n",
      "Epoch: 8/20; Batch:304/468; Training loss:0.0999882\n",
      "Epoch: 8/20; Batch:305/468; Training loss:0.0939467\n",
      "Epoch: 8/20; Batch:306/468; Training loss:0.100602\n",
      "Epoch: 8/20; Batch:307/468; Training loss:0.103929\n",
      "Epoch: 8/20; Batch:308/468; Training loss:0.0964437\n",
      "Epoch: 8/20; Batch:309/468; Training loss:0.0996622\n",
      "Epoch: 8/20; Batch:310/468; Training loss:0.105447\n",
      "Epoch: 8/20; Batch:311/468; Training loss:0.101677\n",
      "Epoch: 8/20; Batch:312/468; Training loss:0.10162\n",
      "Epoch: 8/20; Batch:313/468; Training loss:0.0969291\n",
      "Epoch: 8/20; Batch:314/468; Training loss:0.101568\n",
      "Epoch: 8/20; Batch:315/468; Training loss:0.101111\n",
      "Epoch: 8/20; Batch:316/468; Training loss:0.0996937\n",
      "Epoch: 8/20; Batch:317/468; Training loss:0.103765\n",
      "Epoch: 8/20; Batch:318/468; Training loss:0.100927\n",
      "Epoch: 8/20; Batch:319/468; Training loss:0.0985538\n",
      "Epoch: 8/20; Batch:320/468; Training loss:0.100164\n",
      "Epoch: 8/20; Batch:321/468; Training loss:0.103147\n",
      "Epoch: 8/20; Batch:322/468; Training loss:0.0983007\n",
      "Epoch: 8/20; Batch:323/468; Training loss:0.0943157\n",
      "Epoch: 8/20; Batch:324/468; Training loss:0.102601\n",
      "Epoch: 8/20; Batch:325/468; Training loss:0.0983488\n",
      "Epoch: 8/20; Batch:326/468; Training loss:0.0990308\n",
      "Epoch: 8/20; Batch:327/468; Training loss:0.0985986\n",
      "Epoch: 8/20; Batch:328/468; Training loss:0.0990367\n",
      "Epoch: 8/20; Batch:329/468; Training loss:0.100937\n",
      "Epoch: 8/20; Batch:330/468; Training loss:0.101047\n",
      "Epoch: 8/20; Batch:331/468; Training loss:0.100671\n",
      "Epoch: 8/20; Batch:332/468; Training loss:0.102986\n",
      "Epoch: 8/20; Batch:333/468; Training loss:0.101472\n",
      "Epoch: 8/20; Batch:334/468; Training loss:0.0994026\n",
      "Epoch: 8/20; Batch:335/468; Training loss:0.104623\n",
      "Epoch: 8/20; Batch:336/468; Training loss:0.0998885\n",
      "Epoch: 8/20; Batch:337/468; Training loss:0.104569\n",
      "Epoch: 8/20; Batch:338/468; Training loss:0.100655\n",
      "Epoch: 8/20; Batch:339/468; Training loss:0.0992145\n",
      "Epoch: 8/20; Batch:340/468; Training loss:0.101417\n",
      "Epoch: 8/20; Batch:341/468; Training loss:0.0993346\n",
      "Epoch: 8/20; Batch:342/468; Training loss:0.101083\n",
      "Epoch: 8/20; Batch:343/468; Training loss:0.100579\n",
      "Epoch: 8/20; Batch:344/468; Training loss:0.0989104\n",
      "Epoch: 8/20; Batch:345/468; Training loss:0.100491\n",
      "Epoch: 8/20; Batch:346/468; Training loss:0.101024\n",
      "Epoch: 8/20; Batch:347/468; Training loss:0.103514\n",
      "Epoch: 8/20; Batch:348/468; Training loss:0.099656\n",
      "Epoch: 8/20; Batch:349/468; Training loss:0.100169\n",
      "Epoch: 8/20; Batch:350/468; Training loss:0.0962696\n",
      "Epoch: 8/20; Batch:351/468; Training loss:0.105008\n",
      "Epoch: 8/20; Batch:352/468; Training loss:0.100301\n",
      "Epoch: 8/20; Batch:353/468; Training loss:0.0965651\n",
      "Epoch: 8/20; Batch:354/468; Training loss:0.102871\n",
      "Epoch: 8/20; Batch:355/468; Training loss:0.101075\n",
      "Epoch: 8/20; Batch:356/468; Training loss:0.102127\n",
      "Epoch: 8/20; Batch:357/468; Training loss:0.107117\n",
      "Epoch: 8/20; Batch:358/468; Training loss:0.101147\n",
      "Epoch: 8/20; Batch:359/468; Training loss:0.0996511\n",
      "Epoch: 8/20; Batch:360/468; Training loss:0.0995379\n",
      "Epoch: 8/20; Batch:361/468; Training loss:0.10065\n",
      "Epoch: 8/20; Batch:362/468; Training loss:0.105232\n",
      "Epoch: 8/20; Batch:363/468; Training loss:0.103791\n",
      "Epoch: 8/20; Batch:364/468; Training loss:0.10337\n",
      "Epoch: 8/20; Batch:365/468; Training loss:0.101018\n",
      "Epoch: 8/20; Batch:366/468; Training loss:0.103178\n",
      "Epoch: 8/20; Batch:367/468; Training loss:0.0975242\n",
      "Epoch: 8/20; Batch:368/468; Training loss:0.10196\n",
      "Epoch: 8/20; Batch:369/468; Training loss:0.0995751\n",
      "Epoch: 8/20; Batch:370/468; Training loss:0.10243\n",
      "Epoch: 8/20; Batch:371/468; Training loss:0.100258\n",
      "Epoch: 8/20; Batch:372/468; Training loss:0.102517\n",
      "Epoch: 8/20; Batch:373/468; Training loss:0.104751\n",
      "Epoch: 8/20; Batch:374/468; Training loss:0.0993645\n",
      "Epoch: 8/20; Batch:375/468; Training loss:0.0997514\n",
      "Epoch: 8/20; Batch:376/468; Training loss:0.0971525\n",
      "Epoch: 8/20; Batch:377/468; Training loss:0.0984317\n",
      "Epoch: 8/20; Batch:378/468; Training loss:0.0982488\n",
      "Epoch: 8/20; Batch:379/468; Training loss:0.100638\n",
      "Epoch: 8/20; Batch:380/468; Training loss:0.0987248\n",
      "Epoch: 8/20; Batch:381/468; Training loss:0.103212\n",
      "Epoch: 8/20; Batch:382/468; Training loss:0.1053\n",
      "Epoch: 8/20; Batch:383/468; Training loss:0.0976753\n",
      "Epoch: 8/20; Batch:384/468; Training loss:0.096915\n",
      "Epoch: 8/20; Batch:385/468; Training loss:0.0988739\n",
      "Epoch: 8/20; Batch:386/468; Training loss:0.102485\n",
      "Epoch: 8/20; Batch:387/468; Training loss:0.103316\n",
      "Epoch: 8/20; Batch:388/468; Training loss:0.102142\n",
      "Epoch: 8/20; Batch:389/468; Training loss:0.098274\n",
      "Epoch: 8/20; Batch:390/468; Training loss:0.1006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20; Batch:391/468; Training loss:0.100818\n",
      "Epoch: 8/20; Batch:392/468; Training loss:0.100297\n",
      "Epoch: 8/20; Batch:393/468; Training loss:0.103006\n",
      "Epoch: 8/20; Batch:394/468; Training loss:0.0967823\n",
      "Epoch: 8/20; Batch:395/468; Training loss:0.0995537\n",
      "Epoch: 8/20; Batch:396/468; Training loss:0.102425\n",
      "Epoch: 8/20; Batch:397/468; Training loss:0.102575\n",
      "Epoch: 8/20; Batch:398/468; Training loss:0.100302\n",
      "Epoch: 8/20; Batch:399/468; Training loss:0.10113\n",
      "Epoch: 8/20; Batch:400/468; Training loss:0.0991793\n",
      "Epoch: 8/20; Batch:401/468; Training loss:0.105492\n",
      "Epoch: 8/20; Batch:402/468; Training loss:0.0971792\n",
      "Epoch: 8/20; Batch:403/468; Training loss:0.100003\n",
      "Epoch: 8/20; Batch:404/468; Training loss:0.104442\n",
      "Epoch: 8/20; Batch:405/468; Training loss:0.104255\n",
      "Epoch: 8/20; Batch:406/468; Training loss:0.10244\n",
      "Epoch: 8/20; Batch:407/468; Training loss:0.102385\n",
      "Epoch: 8/20; Batch:408/468; Training loss:0.0999835\n",
      "Epoch: 8/20; Batch:409/468; Training loss:0.100569\n",
      "Epoch: 8/20; Batch:410/468; Training loss:0.100698\n",
      "Epoch: 8/20; Batch:411/468; Training loss:0.0986824\n",
      "Epoch: 8/20; Batch:412/468; Training loss:0.104725\n",
      "Epoch: 8/20; Batch:413/468; Training loss:0.101933\n",
      "Epoch: 8/20; Batch:414/468; Training loss:0.101534\n",
      "Epoch: 8/20; Batch:415/468; Training loss:0.0991841\n",
      "Epoch: 8/20; Batch:416/468; Training loss:0.0997687\n",
      "Epoch: 8/20; Batch:417/468; Training loss:0.102354\n",
      "Epoch: 8/20; Batch:418/468; Training loss:0.100708\n",
      "Epoch: 8/20; Batch:419/468; Training loss:0.0996357\n",
      "Epoch: 8/20; Batch:420/468; Training loss:0.0996763\n",
      "Epoch: 8/20; Batch:421/468; Training loss:0.103492\n",
      "Epoch: 8/20; Batch:422/468; Training loss:0.0987617\n",
      "Epoch: 8/20; Batch:423/468; Training loss:0.101222\n",
      "Epoch: 8/20; Batch:424/468; Training loss:0.102053\n",
      "Epoch: 8/20; Batch:425/468; Training loss:0.0998846\n",
      "Epoch: 8/20; Batch:426/468; Training loss:0.0956273\n",
      "Epoch: 8/20; Batch:427/468; Training loss:0.100803\n",
      "Epoch: 8/20; Batch:428/468; Training loss:0.100596\n",
      "Epoch: 8/20; Batch:429/468; Training loss:0.0998681\n",
      "Epoch: 8/20; Batch:430/468; Training loss:0.102097\n",
      "Epoch: 8/20; Batch:431/468; Training loss:0.103609\n",
      "Epoch: 8/20; Batch:432/468; Training loss:0.101011\n",
      "Epoch: 8/20; Batch:433/468; Training loss:0.101247\n",
      "Epoch: 8/20; Batch:434/468; Training loss:0.101668\n",
      "Epoch: 8/20; Batch:435/468; Training loss:0.0984291\n",
      "Epoch: 8/20; Batch:436/468; Training loss:0.100087\n",
      "Epoch: 8/20; Batch:437/468; Training loss:0.100348\n",
      "Epoch: 8/20; Batch:438/468; Training loss:0.0961488\n",
      "Epoch: 8/20; Batch:439/468; Training loss:0.102678\n",
      "Epoch: 8/20; Batch:440/468; Training loss:0.101821\n",
      "Epoch: 8/20; Batch:441/468; Training loss:0.103551\n",
      "Epoch: 8/20; Batch:442/468; Training loss:0.101297\n",
      "Epoch: 8/20; Batch:443/468; Training loss:0.100716\n",
      "Epoch: 8/20; Batch:444/468; Training loss:0.100366\n",
      "Epoch: 8/20; Batch:445/468; Training loss:0.0978106\n",
      "Epoch: 8/20; Batch:446/468; Training loss:0.101457\n",
      "Epoch: 8/20; Batch:447/468; Training loss:0.0941106\n",
      "Epoch: 8/20; Batch:448/468; Training loss:0.10075\n",
      "Epoch: 8/20; Batch:449/468; Training loss:0.100737\n",
      "Epoch: 8/20; Batch:450/468; Training loss:0.10475\n",
      "Epoch: 8/20; Batch:451/468; Training loss:0.101722\n",
      "Epoch: 8/20; Batch:452/468; Training loss:0.0996858\n",
      "Epoch: 8/20; Batch:453/468; Training loss:0.101005\n",
      "Epoch: 8/20; Batch:454/468; Training loss:0.100423\n",
      "Epoch: 8/20; Batch:455/468; Training loss:0.0985389\n",
      "Epoch: 8/20; Batch:456/468; Training loss:0.0998986\n",
      "Epoch: 8/20; Batch:457/468; Training loss:0.104291\n",
      "Epoch: 8/20; Batch:458/468; Training loss:0.0989695\n",
      "Epoch: 8/20; Batch:459/468; Training loss:0.101117\n",
      "Epoch: 8/20; Batch:460/468; Training loss:0.104065\n",
      "Epoch: 8/20; Batch:461/468; Training loss:0.102865\n",
      "Epoch: 8/20; Batch:462/468; Training loss:0.0954333\n",
      "Epoch: 8/20; Batch:463/468; Training loss:0.0986946\n",
      "Epoch: 8/20; Batch:464/468; Training loss:0.101588\n",
      "Epoch: 8/20; Batch:465/468; Training loss:0.0943696\n",
      "Epoch: 8/20; Batch:466/468; Training loss:0.0992961\n",
      "Epoch: 8/20; Batch:467/468; Training loss:0.0987656\n",
      "Epoch: 8/20; Batch:468/468; Training loss:0.100179\n",
      "Epoch: 9/20; Batch:1/468; Training loss:0.095667\n",
      "Epoch: 9/20; Batch:2/468; Training loss:0.100717\n",
      "Epoch: 9/20; Batch:3/468; Training loss:0.0948351\n",
      "Epoch: 9/20; Batch:4/468; Training loss:0.0962627\n",
      "Epoch: 9/20; Batch:5/468; Training loss:0.100506\n",
      "Epoch: 9/20; Batch:6/468; Training loss:0.102619\n",
      "Epoch: 9/20; Batch:7/468; Training loss:0.0973624\n",
      "Epoch: 9/20; Batch:8/468; Training loss:0.0971658\n",
      "Epoch: 9/20; Batch:9/468; Training loss:0.0992241\n",
      "Epoch: 9/20; Batch:10/468; Training loss:0.100197\n",
      "Epoch: 9/20; Batch:11/468; Training loss:0.10202\n",
      "Epoch: 9/20; Batch:12/468; Training loss:0.101801\n",
      "Epoch: 9/20; Batch:13/468; Training loss:0.10386\n",
      "Epoch: 9/20; Batch:14/468; Training loss:0.101098\n",
      "Epoch: 9/20; Batch:15/468; Training loss:0.0971348\n",
      "Epoch: 9/20; Batch:16/468; Training loss:0.0996843\n",
      "Epoch: 9/20; Batch:17/468; Training loss:0.101996\n",
      "Epoch: 9/20; Batch:18/468; Training loss:0.102844\n",
      "Epoch: 9/20; Batch:19/468; Training loss:0.100945\n",
      "Epoch: 9/20; Batch:20/468; Training loss:0.103403\n",
      "Epoch: 9/20; Batch:21/468; Training loss:0.0999705\n",
      "Epoch: 9/20; Batch:22/468; Training loss:0.104231\n",
      "Epoch: 9/20; Batch:23/468; Training loss:0.0988912\n",
      "Epoch: 9/20; Batch:24/468; Training loss:0.10059\n",
      "Epoch: 9/20; Batch:25/468; Training loss:0.0972748\n",
      "Epoch: 9/20; Batch:26/468; Training loss:0.101561\n",
      "Epoch: 9/20; Batch:27/468; Training loss:0.101604\n",
      "Epoch: 9/20; Batch:28/468; Training loss:0.103116\n",
      "Epoch: 9/20; Batch:29/468; Training loss:0.0969807\n",
      "Epoch: 9/20; Batch:30/468; Training loss:0.102493\n",
      "Epoch: 9/20; Batch:31/468; Training loss:0.101283\n",
      "Epoch: 9/20; Batch:32/468; Training loss:0.098637\n",
      "Epoch: 9/20; Batch:33/468; Training loss:0.0994519\n",
      "Epoch: 9/20; Batch:34/468; Training loss:0.102435\n",
      "Epoch: 9/20; Batch:35/468; Training loss:0.104333\n",
      "Epoch: 9/20; Batch:36/468; Training loss:0.100624\n",
      "Epoch: 9/20; Batch:37/468; Training loss:0.101608\n",
      "Epoch: 9/20; Batch:38/468; Training loss:0.100887\n",
      "Epoch: 9/20; Batch:39/468; Training loss:0.0975301\n",
      "Epoch: 9/20; Batch:40/468; Training loss:0.0996754\n",
      "Epoch: 9/20; Batch:41/468; Training loss:0.0996528\n",
      "Epoch: 9/20; Batch:42/468; Training loss:0.09927\n",
      "Epoch: 9/20; Batch:43/468; Training loss:0.100447\n",
      "Epoch: 9/20; Batch:44/468; Training loss:0.102964\n",
      "Epoch: 9/20; Batch:45/468; Training loss:0.0980572\n",
      "Epoch: 9/20; Batch:46/468; Training loss:0.098498\n",
      "Epoch: 9/20; Batch:47/468; Training loss:0.101399\n",
      "Epoch: 9/20; Batch:48/468; Training loss:0.106675\n",
      "Epoch: 9/20; Batch:49/468; Training loss:0.100109\n",
      "Epoch: 9/20; Batch:50/468; Training loss:0.0982922\n",
      "Epoch: 9/20; Batch:51/468; Training loss:0.0985276\n",
      "Epoch: 9/20; Batch:52/468; Training loss:0.0979962\n",
      "Epoch: 9/20; Batch:53/468; Training loss:0.0975493\n",
      "Epoch: 9/20; Batch:54/468; Training loss:0.0984617\n",
      "Epoch: 9/20; Batch:55/468; Training loss:0.106171\n",
      "Epoch: 9/20; Batch:56/468; Training loss:0.0998186\n",
      "Epoch: 9/20; Batch:57/468; Training loss:0.101721\n",
      "Epoch: 9/20; Batch:58/468; Training loss:0.103634\n",
      "Epoch: 9/20; Batch:59/468; Training loss:0.0998692\n",
      "Epoch: 9/20; Batch:60/468; Training loss:0.0982922\n",
      "Epoch: 9/20; Batch:61/468; Training loss:0.100584\n",
      "Epoch: 9/20; Batch:62/468; Training loss:0.100238\n",
      "Epoch: 9/20; Batch:63/468; Training loss:0.0983231\n",
      "Epoch: 9/20; Batch:64/468; Training loss:0.101649\n",
      "Epoch: 9/20; Batch:65/468; Training loss:0.103091\n",
      "Epoch: 9/20; Batch:66/468; Training loss:0.100043\n",
      "Epoch: 9/20; Batch:67/468; Training loss:0.0987348\n",
      "Epoch: 9/20; Batch:68/468; Training loss:0.0989331\n",
      "Epoch: 9/20; Batch:69/468; Training loss:0.102148\n",
      "Epoch: 9/20; Batch:70/468; Training loss:0.101171\n",
      "Epoch: 9/20; Batch:71/468; Training loss:0.101504\n",
      "Epoch: 9/20; Batch:72/468; Training loss:0.0976781\n",
      "Epoch: 9/20; Batch:73/468; Training loss:0.100493\n",
      "Epoch: 9/20; Batch:74/468; Training loss:0.10261\n",
      "Epoch: 9/20; Batch:75/468; Training loss:0.0985419\n",
      "Epoch: 9/20; Batch:76/468; Training loss:0.0971424\n",
      "Epoch: 9/20; Batch:77/468; Training loss:0.0994497\n",
      "Epoch: 9/20; Batch:78/468; Training loss:0.099328\n",
      "Epoch: 9/20; Batch:79/468; Training loss:0.0966611\n",
      "Epoch: 9/20; Batch:80/468; Training loss:0.100499\n",
      "Epoch: 9/20; Batch:81/468; Training loss:0.102082\n",
      "Epoch: 9/20; Batch:82/468; Training loss:0.101456\n",
      "Epoch: 9/20; Batch:83/468; Training loss:0.0945643\n",
      "Epoch: 9/20; Batch:84/468; Training loss:0.0973514\n",
      "Epoch: 9/20; Batch:85/468; Training loss:0.104828\n",
      "Epoch: 9/20; Batch:86/468; Training loss:0.0958446\n",
      "Epoch: 9/20; Batch:87/468; Training loss:0.098877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20; Batch:88/468; Training loss:0.0990843\n",
      "Epoch: 9/20; Batch:89/468; Training loss:0.0995431\n",
      "Epoch: 9/20; Batch:90/468; Training loss:0.101811\n",
      "Epoch: 9/20; Batch:91/468; Training loss:0.100332\n",
      "Epoch: 9/20; Batch:92/468; Training loss:0.101276\n",
      "Epoch: 9/20; Batch:93/468; Training loss:0.0999375\n",
      "Epoch: 9/20; Batch:94/468; Training loss:0.100423\n",
      "Epoch: 9/20; Batch:95/468; Training loss:0.0988209\n",
      "Epoch: 9/20; Batch:96/468; Training loss:0.0986368\n",
      "Epoch: 9/20; Batch:97/468; Training loss:0.0984089\n",
      "Epoch: 9/20; Batch:98/468; Training loss:0.0991734\n",
      "Epoch: 9/20; Batch:99/468; Training loss:0.101711\n",
      "Epoch: 9/20; Batch:100/468; Training loss:0.106028\n",
      "Epoch: 9/20; Batch:101/468; Training loss:0.0978687\n",
      "Epoch: 9/20; Batch:102/468; Training loss:0.099258\n",
      "Epoch: 9/20; Batch:103/468; Training loss:0.0948469\n",
      "Epoch: 9/20; Batch:104/468; Training loss:0.0994003\n",
      "Epoch: 9/20; Batch:105/468; Training loss:0.102892\n",
      "Epoch: 9/20; Batch:106/468; Training loss:0.098644\n",
      "Epoch: 9/20; Batch:107/468; Training loss:0.100116\n",
      "Epoch: 9/20; Batch:108/468; Training loss:0.0984151\n",
      "Epoch: 9/20; Batch:109/468; Training loss:0.097082\n",
      "Epoch: 9/20; Batch:110/468; Training loss:0.0990194\n",
      "Epoch: 9/20; Batch:111/468; Training loss:0.100982\n",
      "Epoch: 9/20; Batch:112/468; Training loss:0.100202\n",
      "Epoch: 9/20; Batch:113/468; Training loss:0.0990676\n",
      "Epoch: 9/20; Batch:114/468; Training loss:0.103897\n",
      "Epoch: 9/20; Batch:115/468; Training loss:0.0997819\n",
      "Epoch: 9/20; Batch:116/468; Training loss:0.103229\n",
      "Epoch: 9/20; Batch:117/468; Training loss:0.103041\n",
      "Epoch: 9/20; Batch:118/468; Training loss:0.0974161\n",
      "Epoch: 9/20; Batch:119/468; Training loss:0.0992423\n",
      "Epoch: 9/20; Batch:120/468; Training loss:0.0989703\n",
      "Epoch: 9/20; Batch:121/468; Training loss:0.101112\n",
      "Epoch: 9/20; Batch:122/468; Training loss:0.0994355\n",
      "Epoch: 9/20; Batch:123/468; Training loss:0.0984246\n",
      "Epoch: 9/20; Batch:124/468; Training loss:0.0990294\n",
      "Epoch: 9/20; Batch:125/468; Training loss:0.0998929\n",
      "Epoch: 9/20; Batch:126/468; Training loss:0.102781\n",
      "Epoch: 9/20; Batch:127/468; Training loss:0.0979059\n",
      "Epoch: 9/20; Batch:128/468; Training loss:0.0967537\n",
      "Epoch: 9/20; Batch:129/468; Training loss:0.102524\n",
      "Epoch: 9/20; Batch:130/468; Training loss:0.103138\n",
      "Epoch: 9/20; Batch:131/468; Training loss:0.0991844\n",
      "Epoch: 9/20; Batch:132/468; Training loss:0.0991052\n",
      "Epoch: 9/20; Batch:133/468; Training loss:0.0954174\n",
      "Epoch: 9/20; Batch:134/468; Training loss:0.0983103\n",
      "Epoch: 9/20; Batch:135/468; Training loss:0.10108\n",
      "Epoch: 9/20; Batch:136/468; Training loss:0.0983563\n",
      "Epoch: 9/20; Batch:137/468; Training loss:0.100933\n",
      "Epoch: 9/20; Batch:138/468; Training loss:0.100826\n",
      "Epoch: 9/20; Batch:139/468; Training loss:0.0954955\n",
      "Epoch: 9/20; Batch:140/468; Training loss:0.099865\n",
      "Epoch: 9/20; Batch:141/468; Training loss:0.0970519\n",
      "Epoch: 9/20; Batch:142/468; Training loss:0.101501\n",
      "Epoch: 9/20; Batch:143/468; Training loss:0.102577\n",
      "Epoch: 9/20; Batch:144/468; Training loss:0.0999109\n",
      "Epoch: 9/20; Batch:145/468; Training loss:0.102408\n",
      "Epoch: 9/20; Batch:146/468; Training loss:0.103784\n",
      "Epoch: 9/20; Batch:147/468; Training loss:0.101054\n",
      "Epoch: 9/20; Batch:148/468; Training loss:0.0988365\n",
      "Epoch: 9/20; Batch:149/468; Training loss:0.0982986\n",
      "Epoch: 9/20; Batch:150/468; Training loss:0.0994715\n",
      "Epoch: 9/20; Batch:151/468; Training loss:0.101706\n",
      "Epoch: 9/20; Batch:152/468; Training loss:0.100686\n",
      "Epoch: 9/20; Batch:153/468; Training loss:0.0948065\n",
      "Epoch: 9/20; Batch:154/468; Training loss:0.101341\n",
      "Epoch: 9/20; Batch:155/468; Training loss:0.101611\n",
      "Epoch: 9/20; Batch:156/468; Training loss:0.0993707\n",
      "Epoch: 9/20; Batch:157/468; Training loss:0.100319\n",
      "Epoch: 9/20; Batch:158/468; Training loss:0.10081\n",
      "Epoch: 9/20; Batch:159/468; Training loss:0.0986547\n",
      "Epoch: 9/20; Batch:160/468; Training loss:0.0967884\n",
      "Epoch: 9/20; Batch:161/468; Training loss:0.0961862\n",
      "Epoch: 9/20; Batch:162/468; Training loss:0.0994692\n",
      "Epoch: 9/20; Batch:163/468; Training loss:0.100226\n",
      "Epoch: 9/20; Batch:164/468; Training loss:0.101525\n",
      "Epoch: 9/20; Batch:165/468; Training loss:0.101199\n",
      "Epoch: 9/20; Batch:166/468; Training loss:0.0996813\n",
      "Epoch: 9/20; Batch:167/468; Training loss:0.102013\n",
      "Epoch: 9/20; Batch:168/468; Training loss:0.0940561\n",
      "Epoch: 9/20; Batch:169/468; Training loss:0.0966519\n",
      "Epoch: 9/20; Batch:170/468; Training loss:0.0979367\n",
      "Epoch: 9/20; Batch:171/468; Training loss:0.10072\n",
      "Epoch: 9/20; Batch:172/468; Training loss:0.0988259\n",
      "Epoch: 9/20; Batch:173/468; Training loss:0.100724\n",
      "Epoch: 9/20; Batch:174/468; Training loss:0.101737\n",
      "Epoch: 9/20; Batch:175/468; Training loss:0.09894\n",
      "Epoch: 9/20; Batch:176/468; Training loss:0.104785\n",
      "Epoch: 9/20; Batch:177/468; Training loss:0.1026\n",
      "Epoch: 9/20; Batch:178/468; Training loss:0.100612\n",
      "Epoch: 9/20; Batch:179/468; Training loss:0.101227\n",
      "Epoch: 9/20; Batch:180/468; Training loss:0.0985568\n",
      "Epoch: 9/20; Batch:181/468; Training loss:0.103381\n",
      "Epoch: 9/20; Batch:182/468; Training loss:0.0960123\n",
      "Epoch: 9/20; Batch:183/468; Training loss:0.100673\n",
      "Epoch: 9/20; Batch:184/468; Training loss:0.0999252\n",
      "Epoch: 9/20; Batch:185/468; Training loss:0.102939\n",
      "Epoch: 9/20; Batch:186/468; Training loss:0.100827\n",
      "Epoch: 9/20; Batch:187/468; Training loss:0.10151\n",
      "Epoch: 9/20; Batch:188/468; Training loss:0.100423\n",
      "Epoch: 9/20; Batch:189/468; Training loss:0.0989438\n",
      "Epoch: 9/20; Batch:190/468; Training loss:0.0991255\n",
      "Epoch: 9/20; Batch:191/468; Training loss:0.100569\n",
      "Epoch: 9/20; Batch:192/468; Training loss:0.0982501\n",
      "Epoch: 9/20; Batch:193/468; Training loss:0.10192\n",
      "Epoch: 9/20; Batch:194/468; Training loss:0.0984551\n",
      "Epoch: 9/20; Batch:195/468; Training loss:0.101934\n",
      "Epoch: 9/20; Batch:196/468; Training loss:0.102426\n",
      "Epoch: 9/20; Batch:197/468; Training loss:0.0996759\n",
      "Epoch: 9/20; Batch:198/468; Training loss:0.0984885\n",
      "Epoch: 9/20; Batch:199/468; Training loss:0.0966313\n",
      "Epoch: 9/20; Batch:200/468; Training loss:0.100261\n",
      "Epoch: 9/20; Batch:201/468; Training loss:0.101054\n",
      "Epoch: 9/20; Batch:202/468; Training loss:0.101152\n",
      "Epoch: 9/20; Batch:203/468; Training loss:0.103312\n",
      "Epoch: 9/20; Batch:204/468; Training loss:0.0993749\n",
      "Epoch: 9/20; Batch:205/468; Training loss:0.0978551\n",
      "Epoch: 9/20; Batch:206/468; Training loss:0.100838\n",
      "Epoch: 9/20; Batch:207/468; Training loss:0.101705\n",
      "Epoch: 9/20; Batch:208/468; Training loss:0.103974\n",
      "Epoch: 9/20; Batch:209/468; Training loss:0.104987\n",
      "Epoch: 9/20; Batch:210/468; Training loss:0.0990952\n",
      "Epoch: 9/20; Batch:211/468; Training loss:0.100331\n",
      "Epoch: 9/20; Batch:212/468; Training loss:0.103536\n",
      "Epoch: 9/20; Batch:213/468; Training loss:0.0975687\n",
      "Epoch: 9/20; Batch:214/468; Training loss:0.10249\n",
      "Epoch: 9/20; Batch:215/468; Training loss:0.0965958\n",
      "Epoch: 9/20; Batch:216/468; Training loss:0.099709\n",
      "Epoch: 9/20; Batch:217/468; Training loss:0.0969078\n",
      "Epoch: 9/20; Batch:218/468; Training loss:0.099508\n",
      "Epoch: 9/20; Batch:219/468; Training loss:0.101707\n",
      "Epoch: 9/20; Batch:220/468; Training loss:0.103966\n",
      "Epoch: 9/20; Batch:221/468; Training loss:0.101786\n",
      "Epoch: 9/20; Batch:222/468; Training loss:0.107572\n",
      "Epoch: 9/20; Batch:223/468; Training loss:0.10131\n",
      "Epoch: 9/20; Batch:224/468; Training loss:0.101204\n",
      "Epoch: 9/20; Batch:225/468; Training loss:0.102847\n",
      "Epoch: 9/20; Batch:226/468; Training loss:0.0950224\n",
      "Epoch: 9/20; Batch:227/468; Training loss:0.100526\n",
      "Epoch: 9/20; Batch:228/468; Training loss:0.100551\n",
      "Epoch: 9/20; Batch:229/468; Training loss:0.0984473\n",
      "Epoch: 9/20; Batch:230/468; Training loss:0.101261\n",
      "Epoch: 9/20; Batch:231/468; Training loss:0.0999225\n",
      "Epoch: 9/20; Batch:232/468; Training loss:0.100082\n",
      "Epoch: 9/20; Batch:233/468; Training loss:0.0990901\n",
      "Epoch: 9/20; Batch:234/468; Training loss:0.101598\n",
      "Epoch: 9/20; Batch:235/468; Training loss:0.101978\n",
      "Epoch: 9/20; Batch:236/468; Training loss:0.101107\n",
      "Epoch: 9/20; Batch:237/468; Training loss:0.0995307\n",
      "Epoch: 9/20; Batch:238/468; Training loss:0.103117\n",
      "Epoch: 9/20; Batch:239/468; Training loss:0.0969217\n",
      "Epoch: 9/20; Batch:240/468; Training loss:0.0987082\n",
      "Epoch: 9/20; Batch:241/468; Training loss:0.0998901\n",
      "Epoch: 9/20; Batch:242/468; Training loss:0.104682\n",
      "Epoch: 9/20; Batch:243/468; Training loss:0.100467\n",
      "Epoch: 9/20; Batch:244/468; Training loss:0.102241\n",
      "Epoch: 9/20; Batch:245/468; Training loss:0.10189\n",
      "Epoch: 9/20; Batch:246/468; Training loss:0.102457\n",
      "Epoch: 9/20; Batch:247/468; Training loss:0.100526\n",
      "Epoch: 9/20; Batch:248/468; Training loss:0.0980569\n",
      "Epoch: 9/20; Batch:249/468; Training loss:0.102439\n",
      "Epoch: 9/20; Batch:250/468; Training loss:0.0988078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20; Batch:251/468; Training loss:0.101218\n",
      "Epoch: 9/20; Batch:252/468; Training loss:0.0959674\n",
      "Epoch: 9/20; Batch:253/468; Training loss:0.0988009\n",
      "Epoch: 9/20; Batch:254/468; Training loss:0.0958618\n",
      "Epoch: 9/20; Batch:255/468; Training loss:0.100945\n",
      "Epoch: 9/20; Batch:256/468; Training loss:0.101175\n",
      "Epoch: 9/20; Batch:257/468; Training loss:0.0994988\n",
      "Epoch: 9/20; Batch:258/468; Training loss:0.100802\n",
      "Epoch: 9/20; Batch:259/468; Training loss:0.0997348\n",
      "Epoch: 9/20; Batch:260/468; Training loss:0.101397\n",
      "Epoch: 9/20; Batch:261/468; Training loss:0.095887\n",
      "Epoch: 9/20; Batch:262/468; Training loss:0.09573\n",
      "Epoch: 9/20; Batch:263/468; Training loss:0.101244\n",
      "Epoch: 9/20; Batch:264/468; Training loss:0.0999163\n",
      "Epoch: 9/20; Batch:265/468; Training loss:0.0964333\n",
      "Epoch: 9/20; Batch:266/468; Training loss:0.100673\n",
      "Epoch: 9/20; Batch:267/468; Training loss:0.100347\n",
      "Epoch: 9/20; Batch:268/468; Training loss:0.0975263\n",
      "Epoch: 9/20; Batch:269/468; Training loss:0.099932\n",
      "Epoch: 9/20; Batch:270/468; Training loss:0.1014\n",
      "Epoch: 9/20; Batch:271/468; Training loss:0.0985975\n",
      "Epoch: 9/20; Batch:272/468; Training loss:0.100277\n",
      "Epoch: 9/20; Batch:273/468; Training loss:0.101894\n",
      "Epoch: 9/20; Batch:274/468; Training loss:0.0987518\n",
      "Epoch: 9/20; Batch:275/468; Training loss:0.0975007\n",
      "Epoch: 9/20; Batch:276/468; Training loss:0.0975393\n",
      "Epoch: 9/20; Batch:277/468; Training loss:0.0964688\n",
      "Epoch: 9/20; Batch:278/468; Training loss:0.100054\n",
      "Epoch: 9/20; Batch:279/468; Training loss:0.0956068\n",
      "Epoch: 9/20; Batch:280/468; Training loss:0.100177\n",
      "Epoch: 9/20; Batch:281/468; Training loss:0.100649\n",
      "Epoch: 9/20; Batch:282/468; Training loss:0.0961522\n",
      "Epoch: 9/20; Batch:283/468; Training loss:0.100858\n",
      "Epoch: 9/20; Batch:284/468; Training loss:0.102182\n",
      "Epoch: 9/20; Batch:285/468; Training loss:0.0965565\n",
      "Epoch: 9/20; Batch:286/468; Training loss:0.0982138\n",
      "Epoch: 9/20; Batch:287/468; Training loss:0.0996844\n",
      "Epoch: 9/20; Batch:288/468; Training loss:0.100499\n",
      "Epoch: 9/20; Batch:289/468; Training loss:0.101812\n",
      "Epoch: 9/20; Batch:290/468; Training loss:0.099057\n",
      "Epoch: 9/20; Batch:291/468; Training loss:0.101982\n",
      "Epoch: 9/20; Batch:292/468; Training loss:0.099513\n",
      "Epoch: 9/20; Batch:293/468; Training loss:0.102566\n",
      "Epoch: 9/20; Batch:294/468; Training loss:0.0982405\n",
      "Epoch: 9/20; Batch:295/468; Training loss:0.097623\n",
      "Epoch: 9/20; Batch:296/468; Training loss:0.10106\n",
      "Epoch: 9/20; Batch:297/468; Training loss:0.101414\n",
      "Epoch: 9/20; Batch:298/468; Training loss:0.0982956\n",
      "Epoch: 9/20; Batch:299/468; Training loss:0.0983213\n",
      "Epoch: 9/20; Batch:300/468; Training loss:0.104979\n",
      "Epoch: 9/20; Batch:301/468; Training loss:0.0971505\n",
      "Epoch: 9/20; Batch:302/468; Training loss:0.103209\n",
      "Epoch: 9/20; Batch:303/468; Training loss:0.0953962\n",
      "Epoch: 9/20; Batch:304/468; Training loss:0.100797\n",
      "Epoch: 9/20; Batch:305/468; Training loss:0.100025\n",
      "Epoch: 9/20; Batch:306/468; Training loss:0.0989774\n",
      "Epoch: 9/20; Batch:307/468; Training loss:0.0998488\n",
      "Epoch: 9/20; Batch:308/468; Training loss:0.0987207\n",
      "Epoch: 9/20; Batch:309/468; Training loss:0.101647\n",
      "Epoch: 9/20; Batch:310/468; Training loss:0.0947231\n",
      "Epoch: 9/20; Batch:311/468; Training loss:0.103021\n",
      "Epoch: 9/20; Batch:312/468; Training loss:0.1045\n",
      "Epoch: 9/20; Batch:313/468; Training loss:0.100469\n",
      "Epoch: 9/20; Batch:314/468; Training loss:0.100874\n",
      "Epoch: 9/20; Batch:315/468; Training loss:0.100201\n",
      "Epoch: 9/20; Batch:316/468; Training loss:0.101894\n",
      "Epoch: 9/20; Batch:317/468; Training loss:0.100498\n",
      "Epoch: 9/20; Batch:318/468; Training loss:0.0949913\n",
      "Epoch: 9/20; Batch:319/468; Training loss:0.100849\n",
      "Epoch: 9/20; Batch:320/468; Training loss:0.100838\n",
      "Epoch: 9/20; Batch:321/468; Training loss:0.0986376\n",
      "Epoch: 9/20; Batch:322/468; Training loss:0.0992925\n",
      "Epoch: 9/20; Batch:323/468; Training loss:0.0997285\n",
      "Epoch: 9/20; Batch:324/468; Training loss:0.0971962\n",
      "Epoch: 9/20; Batch:325/468; Training loss:0.102332\n",
      "Epoch: 9/20; Batch:326/468; Training loss:0.0962893\n",
      "Epoch: 9/20; Batch:327/468; Training loss:0.10198\n",
      "Epoch: 9/20; Batch:328/468; Training loss:0.103863\n",
      "Epoch: 9/20; Batch:329/468; Training loss:0.1022\n",
      "Epoch: 9/20; Batch:330/468; Training loss:0.10075\n",
      "Epoch: 9/20; Batch:331/468; Training loss:0.0989857\n",
      "Epoch: 9/20; Batch:332/468; Training loss:0.102107\n",
      "Epoch: 9/20; Batch:333/468; Training loss:0.0989062\n",
      "Epoch: 9/20; Batch:334/468; Training loss:0.103107\n",
      "Epoch: 9/20; Batch:335/468; Training loss:0.101965\n",
      "Epoch: 9/20; Batch:336/468; Training loss:0.1017\n",
      "Epoch: 9/20; Batch:337/468; Training loss:0.101828\n",
      "Epoch: 9/20; Batch:338/468; Training loss:0.0966846\n",
      "Epoch: 9/20; Batch:339/468; Training loss:0.102083\n",
      "Epoch: 9/20; Batch:340/468; Training loss:0.100624\n",
      "Epoch: 9/20; Batch:341/468; Training loss:0.0985461\n",
      "Epoch: 9/20; Batch:342/468; Training loss:0.0994356\n",
      "Epoch: 9/20; Batch:343/468; Training loss:0.0999325\n",
      "Epoch: 9/20; Batch:344/468; Training loss:0.100534\n",
      "Epoch: 9/20; Batch:345/468; Training loss:0.102716\n",
      "Epoch: 9/20; Batch:346/468; Training loss:0.0998592\n",
      "Epoch: 9/20; Batch:347/468; Training loss:0.101478\n",
      "Epoch: 9/20; Batch:348/468; Training loss:0.101257\n",
      "Epoch: 9/20; Batch:349/468; Training loss:0.0996995\n",
      "Epoch: 9/20; Batch:350/468; Training loss:0.0989813\n",
      "Epoch: 9/20; Batch:351/468; Training loss:0.0999452\n",
      "Epoch: 9/20; Batch:352/468; Training loss:0.101518\n",
      "Epoch: 9/20; Batch:353/468; Training loss:0.0961085\n",
      "Epoch: 9/20; Batch:354/468; Training loss:0.100161\n",
      "Epoch: 9/20; Batch:355/468; Training loss:0.0981734\n",
      "Epoch: 9/20; Batch:356/468; Training loss:0.0986703\n",
      "Epoch: 9/20; Batch:357/468; Training loss:0.0991977\n",
      "Epoch: 9/20; Batch:358/468; Training loss:0.102315\n",
      "Epoch: 9/20; Batch:359/468; Training loss:0.101563\n",
      "Epoch: 9/20; Batch:360/468; Training loss:0.100839\n",
      "Epoch: 9/20; Batch:361/468; Training loss:0.102521\n",
      "Epoch: 9/20; Batch:362/468; Training loss:0.100001\n",
      "Epoch: 9/20; Batch:363/468; Training loss:0.0985209\n",
      "Epoch: 9/20; Batch:364/468; Training loss:0.0987835\n",
      "Epoch: 9/20; Batch:365/468; Training loss:0.100092\n",
      "Epoch: 9/20; Batch:366/468; Training loss:0.0986522\n",
      "Epoch: 9/20; Batch:367/468; Training loss:0.0993036\n",
      "Epoch: 9/20; Batch:368/468; Training loss:0.103471\n",
      "Epoch: 9/20; Batch:369/468; Training loss:0.10033\n",
      "Epoch: 9/20; Batch:370/468; Training loss:0.0961824\n",
      "Epoch: 9/20; Batch:371/468; Training loss:0.10014\n",
      "Epoch: 9/20; Batch:372/468; Training loss:0.0992029\n",
      "Epoch: 9/20; Batch:373/468; Training loss:0.0982076\n",
      "Epoch: 9/20; Batch:374/468; Training loss:0.101611\n",
      "Epoch: 9/20; Batch:375/468; Training loss:0.0991119\n",
      "Epoch: 9/20; Batch:376/468; Training loss:0.101673\n",
      "Epoch: 9/20; Batch:377/468; Training loss:0.0982058\n",
      "Epoch: 9/20; Batch:378/468; Training loss:0.0971147\n",
      "Epoch: 9/20; Batch:379/468; Training loss:0.0991505\n",
      "Epoch: 9/20; Batch:380/468; Training loss:0.0986503\n",
      "Epoch: 9/20; Batch:381/468; Training loss:0.101281\n",
      "Epoch: 9/20; Batch:382/468; Training loss:0.102282\n",
      "Epoch: 9/20; Batch:383/468; Training loss:0.101911\n",
      "Epoch: 9/20; Batch:384/468; Training loss:0.0997126\n",
      "Epoch: 9/20; Batch:385/468; Training loss:0.101096\n",
      "Epoch: 9/20; Batch:386/468; Training loss:0.1004\n",
      "Epoch: 9/20; Batch:387/468; Training loss:0.0985663\n",
      "Epoch: 9/20; Batch:388/468; Training loss:0.0971488\n",
      "Epoch: 9/20; Batch:389/468; Training loss:0.101884\n",
      "Epoch: 9/20; Batch:390/468; Training loss:0.0986182\n",
      "Epoch: 9/20; Batch:391/468; Training loss:0.100624\n",
      "Epoch: 9/20; Batch:392/468; Training loss:0.0983306\n",
      "Epoch: 9/20; Batch:393/468; Training loss:0.0974933\n",
      "Epoch: 9/20; Batch:394/468; Training loss:0.100458\n",
      "Epoch: 9/20; Batch:395/468; Training loss:0.101985\n",
      "Epoch: 9/20; Batch:396/468; Training loss:0.103829\n",
      "Epoch: 9/20; Batch:397/468; Training loss:0.096922\n",
      "Epoch: 9/20; Batch:398/468; Training loss:0.0961261\n",
      "Epoch: 9/20; Batch:399/468; Training loss:0.0992959\n",
      "Epoch: 9/20; Batch:400/468; Training loss:0.10114\n",
      "Epoch: 9/20; Batch:401/468; Training loss:0.100151\n",
      "Epoch: 9/20; Batch:402/468; Training loss:0.101362\n",
      "Epoch: 9/20; Batch:403/468; Training loss:0.102483\n",
      "Epoch: 9/20; Batch:404/468; Training loss:0.105093\n",
      "Epoch: 9/20; Batch:405/468; Training loss:0.100103\n",
      "Epoch: 9/20; Batch:406/468; Training loss:0.103985\n",
      "Epoch: 9/20; Batch:407/468; Training loss:0.100204\n",
      "Epoch: 9/20; Batch:408/468; Training loss:0.101533\n",
      "Epoch: 9/20; Batch:409/468; Training loss:0.0988334\n",
      "Epoch: 9/20; Batch:410/468; Training loss:0.100527\n",
      "Epoch: 9/20; Batch:411/468; Training loss:0.095924\n",
      "Epoch: 9/20; Batch:412/468; Training loss:0.104656\n",
      "Epoch: 9/20; Batch:413/468; Training loss:0.0984131\n",
      "Epoch: 9/20; Batch:414/468; Training loss:0.10184\n",
      "Epoch: 9/20; Batch:415/468; Training loss:0.0974716\n",
      "Epoch: 9/20; Batch:416/468; Training loss:0.0985553\n",
      "Epoch: 9/20; Batch:417/468; Training loss:0.102216\n",
      "Epoch: 9/20; Batch:418/468; Training loss:0.102136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20; Batch:419/468; Training loss:0.10334\n",
      "Epoch: 9/20; Batch:420/468; Training loss:0.10177\n",
      "Epoch: 9/20; Batch:421/468; Training loss:0.101335\n",
      "Epoch: 9/20; Batch:422/468; Training loss:0.0986389\n",
      "Epoch: 9/20; Batch:423/468; Training loss:0.0977923\n",
      "Epoch: 9/20; Batch:424/468; Training loss:0.0993233\n",
      "Epoch: 9/20; Batch:425/468; Training loss:0.102728\n",
      "Epoch: 9/20; Batch:426/468; Training loss:0.0976829\n",
      "Epoch: 9/20; Batch:427/468; Training loss:0.100907\n",
      "Epoch: 9/20; Batch:428/468; Training loss:0.104405\n",
      "Epoch: 9/20; Batch:429/468; Training loss:0.099057\n",
      "Epoch: 9/20; Batch:430/468; Training loss:0.0970755\n",
      "Epoch: 9/20; Batch:431/468; Training loss:0.102601\n",
      "Epoch: 9/20; Batch:432/468; Training loss:0.0995949\n",
      "Epoch: 9/20; Batch:433/468; Training loss:0.107051\n",
      "Epoch: 9/20; Batch:434/468; Training loss:0.0964695\n",
      "Epoch: 9/20; Batch:435/468; Training loss:0.0979265\n",
      "Epoch: 9/20; Batch:436/468; Training loss:0.0977438\n",
      "Epoch: 9/20; Batch:437/468; Training loss:0.100969\n",
      "Epoch: 9/20; Batch:438/468; Training loss:0.101586\n",
      "Epoch: 9/20; Batch:439/468; Training loss:0.101396\n",
      "Epoch: 9/20; Batch:440/468; Training loss:0.102204\n",
      "Epoch: 9/20; Batch:441/468; Training loss:0.099209\n",
      "Epoch: 9/20; Batch:442/468; Training loss:0.106118\n",
      "Epoch: 9/20; Batch:443/468; Training loss:0.0963657\n",
      "Epoch: 9/20; Batch:444/468; Training loss:0.0983326\n",
      "Epoch: 9/20; Batch:445/468; Training loss:0.0989631\n",
      "Epoch: 9/20; Batch:446/468; Training loss:0.0994311\n",
      "Epoch: 9/20; Batch:447/468; Training loss:0.0980441\n",
      "Epoch: 9/20; Batch:448/468; Training loss:0.102204\n",
      "Epoch: 9/20; Batch:449/468; Training loss:0.100981\n",
      "Epoch: 9/20; Batch:450/468; Training loss:0.104016\n",
      "Epoch: 9/20; Batch:451/468; Training loss:0.0982098\n",
      "Epoch: 9/20; Batch:452/468; Training loss:0.0973127\n",
      "Epoch: 9/20; Batch:453/468; Training loss:0.0998185\n",
      "Epoch: 9/20; Batch:454/468; Training loss:0.0984705\n",
      "Epoch: 9/20; Batch:455/468; Training loss:0.104383\n",
      "Epoch: 9/20; Batch:456/468; Training loss:0.10118\n",
      "Epoch: 9/20; Batch:457/468; Training loss:0.0964973\n",
      "Epoch: 9/20; Batch:458/468; Training loss:0.101695\n",
      "Epoch: 9/20; Batch:459/468; Training loss:0.100463\n",
      "Epoch: 9/20; Batch:460/468; Training loss:0.100387\n",
      "Epoch: 9/20; Batch:461/468; Training loss:0.100918\n",
      "Epoch: 9/20; Batch:462/468; Training loss:0.0993572\n",
      "Epoch: 9/20; Batch:463/468; Training loss:0.0960499\n",
      "Epoch: 9/20; Batch:464/468; Training loss:0.103401\n",
      "Epoch: 9/20; Batch:465/468; Training loss:0.100709\n",
      "Epoch: 9/20; Batch:466/468; Training loss:0.0961149\n",
      "Epoch: 9/20; Batch:467/468; Training loss:0.100208\n",
      "Epoch: 9/20; Batch:468/468; Training loss:0.0998795\n",
      "Epoch: 10/20; Batch:1/468; Training loss:0.100126\n",
      "Epoch: 10/20; Batch:2/468; Training loss:0.101914\n",
      "Epoch: 10/20; Batch:3/468; Training loss:0.101678\n",
      "Epoch: 10/20; Batch:4/468; Training loss:0.100951\n",
      "Epoch: 10/20; Batch:5/468; Training loss:0.0996623\n",
      "Epoch: 10/20; Batch:6/468; Training loss:0.0991561\n",
      "Epoch: 10/20; Batch:7/468; Training loss:0.0982735\n",
      "Epoch: 10/20; Batch:8/468; Training loss:0.0978787\n",
      "Epoch: 10/20; Batch:9/468; Training loss:0.0986875\n",
      "Epoch: 10/20; Batch:10/468; Training loss:0.0980798\n",
      "Epoch: 10/20; Batch:11/468; Training loss:0.103695\n",
      "Epoch: 10/20; Batch:12/468; Training loss:0.0978732\n",
      "Epoch: 10/20; Batch:13/468; Training loss:0.102572\n",
      "Epoch: 10/20; Batch:14/468; Training loss:0.099743\n",
      "Epoch: 10/20; Batch:15/468; Training loss:0.100024\n",
      "Epoch: 10/20; Batch:16/468; Training loss:0.097251\n",
      "Epoch: 10/20; Batch:17/468; Training loss:0.101967\n",
      "Epoch: 10/20; Batch:18/468; Training loss:0.101825\n",
      "Epoch: 10/20; Batch:19/468; Training loss:0.0994525\n",
      "Epoch: 10/20; Batch:20/468; Training loss:0.0993726\n",
      "Epoch: 10/20; Batch:21/468; Training loss:0.0975101\n",
      "Epoch: 10/20; Batch:22/468; Training loss:0.0990506\n",
      "Epoch: 10/20; Batch:23/468; Training loss:0.0964167\n",
      "Epoch: 10/20; Batch:24/468; Training loss:0.099141\n",
      "Epoch: 10/20; Batch:25/468; Training loss:0.100566\n",
      "Epoch: 10/20; Batch:26/468; Training loss:0.0998221\n",
      "Epoch: 10/20; Batch:27/468; Training loss:0.0978913\n",
      "Epoch: 10/20; Batch:28/468; Training loss:0.0970813\n",
      "Epoch: 10/20; Batch:29/468; Training loss:0.097995\n",
      "Epoch: 10/20; Batch:30/468; Training loss:0.0968495\n",
      "Epoch: 10/20; Batch:31/468; Training loss:0.0975809\n",
      "Epoch: 10/20; Batch:32/468; Training loss:0.0992685\n",
      "Epoch: 10/20; Batch:33/468; Training loss:0.0969098\n",
      "Epoch: 10/20; Batch:34/468; Training loss:0.102195\n",
      "Epoch: 10/20; Batch:35/468; Training loss:0.0962432\n",
      "Epoch: 10/20; Batch:36/468; Training loss:0.102776\n",
      "Epoch: 10/20; Batch:37/468; Training loss:0.0995938\n",
      "Epoch: 10/20; Batch:38/468; Training loss:0.099279\n",
      "Epoch: 10/20; Batch:39/468; Training loss:0.101044\n",
      "Epoch: 10/20; Batch:40/468; Training loss:0.101778\n",
      "Epoch: 10/20; Batch:41/468; Training loss:0.0990299\n",
      "Epoch: 10/20; Batch:42/468; Training loss:0.0986221\n",
      "Epoch: 10/20; Batch:43/468; Training loss:0.0957379\n",
      "Epoch: 10/20; Batch:44/468; Training loss:0.100351\n",
      "Epoch: 10/20; Batch:45/468; Training loss:0.0995145\n",
      "Epoch: 10/20; Batch:46/468; Training loss:0.0987287\n",
      "Epoch: 10/20; Batch:47/468; Training loss:0.09613\n",
      "Epoch: 10/20; Batch:48/468; Training loss:0.0999875\n",
      "Epoch: 10/20; Batch:49/468; Training loss:0.10013\n",
      "Epoch: 10/20; Batch:50/468; Training loss:0.0980075\n",
      "Epoch: 10/20; Batch:51/468; Training loss:0.0969654\n",
      "Epoch: 10/20; Batch:52/468; Training loss:0.0990777\n",
      "Epoch: 10/20; Batch:53/468; Training loss:0.0971835\n",
      "Epoch: 10/20; Batch:54/468; Training loss:0.0989789\n",
      "Epoch: 10/20; Batch:55/468; Training loss:0.102363\n",
      "Epoch: 10/20; Batch:56/468; Training loss:0.101899\n",
      "Epoch: 10/20; Batch:57/468; Training loss:0.098658\n",
      "Epoch: 10/20; Batch:58/468; Training loss:0.101975\n",
      "Epoch: 10/20; Batch:59/468; Training loss:0.101773\n",
      "Epoch: 10/20; Batch:60/468; Training loss:0.0975424\n",
      "Epoch: 10/20; Batch:61/468; Training loss:0.0991709\n",
      "Epoch: 10/20; Batch:62/468; Training loss:0.100457\n",
      "Epoch: 10/20; Batch:63/468; Training loss:0.102988\n",
      "Epoch: 10/20; Batch:64/468; Training loss:0.0995542\n",
      "Epoch: 10/20; Batch:65/468; Training loss:0.101138\n",
      "Epoch: 10/20; Batch:66/468; Training loss:0.101259\n",
      "Epoch: 10/20; Batch:67/468; Training loss:0.0974156\n",
      "Epoch: 10/20; Batch:68/468; Training loss:0.0977294\n",
      "Epoch: 10/20; Batch:69/468; Training loss:0.0967177\n",
      "Epoch: 10/20; Batch:70/468; Training loss:0.0982265\n",
      "Epoch: 10/20; Batch:71/468; Training loss:0.101495\n",
      "Epoch: 10/20; Batch:72/468; Training loss:0.101342\n",
      "Epoch: 10/20; Batch:73/468; Training loss:0.0998241\n",
      "Epoch: 10/20; Batch:74/468; Training loss:0.101292\n",
      "Epoch: 10/20; Batch:75/468; Training loss:0.0999027\n",
      "Epoch: 10/20; Batch:76/468; Training loss:0.0980941\n",
      "Epoch: 10/20; Batch:77/468; Training loss:0.104714\n",
      "Epoch: 10/20; Batch:78/468; Training loss:0.100767\n",
      "Epoch: 10/20; Batch:79/468; Training loss:0.103901\n",
      "Epoch: 10/20; Batch:80/468; Training loss:0.0974128\n",
      "Epoch: 10/20; Batch:81/468; Training loss:0.101396\n",
      "Epoch: 10/20; Batch:82/468; Training loss:0.094473\n",
      "Epoch: 10/20; Batch:83/468; Training loss:0.0973233\n",
      "Epoch: 10/20; Batch:84/468; Training loss:0.0978133\n",
      "Epoch: 10/20; Batch:85/468; Training loss:0.0985021\n",
      "Epoch: 10/20; Batch:86/468; Training loss:0.096637\n",
      "Epoch: 10/20; Batch:87/468; Training loss:0.0998003\n",
      "Epoch: 10/20; Batch:88/468; Training loss:0.103227\n",
      "Epoch: 10/20; Batch:89/468; Training loss:0.100253\n",
      "Epoch: 10/20; Batch:90/468; Training loss:0.102323\n",
      "Epoch: 10/20; Batch:91/468; Training loss:0.102585\n",
      "Epoch: 10/20; Batch:92/468; Training loss:0.101159\n",
      "Epoch: 10/20; Batch:93/468; Training loss:0.0998098\n",
      "Epoch: 10/20; Batch:94/468; Training loss:0.102527\n",
      "Epoch: 10/20; Batch:95/468; Training loss:0.10399\n",
      "Epoch: 10/20; Batch:96/468; Training loss:0.0996204\n",
      "Epoch: 10/20; Batch:97/468; Training loss:0.0955323\n",
      "Epoch: 10/20; Batch:98/468; Training loss:0.0958494\n",
      "Epoch: 10/20; Batch:99/468; Training loss:0.100395\n",
      "Epoch: 10/20; Batch:100/468; Training loss:0.098349\n",
      "Epoch: 10/20; Batch:101/468; Training loss:0.101786\n",
      "Epoch: 10/20; Batch:102/468; Training loss:0.102893\n",
      "Epoch: 10/20; Batch:103/468; Training loss:0.0966231\n",
      "Epoch: 10/20; Batch:104/468; Training loss:0.0998188\n",
      "Epoch: 10/20; Batch:105/468; Training loss:0.104765\n",
      "Epoch: 10/20; Batch:106/468; Training loss:0.0991912\n",
      "Epoch: 10/20; Batch:107/468; Training loss:0.102249\n",
      "Epoch: 10/20; Batch:108/468; Training loss:0.103267\n",
      "Epoch: 10/20; Batch:109/468; Training loss:0.100774\n",
      "Epoch: 10/20; Batch:110/468; Training loss:0.0994903\n",
      "Epoch: 10/20; Batch:111/468; Training loss:0.104408\n",
      "Epoch: 10/20; Batch:112/468; Training loss:0.0959014\n",
      "Epoch: 10/20; Batch:113/468; Training loss:0.101979\n",
      "Epoch: 10/20; Batch:114/468; Training loss:0.104745\n",
      "Epoch: 10/20; Batch:115/468; Training loss:0.100982\n",
      "Epoch: 10/20; Batch:116/468; Training loss:0.103895\n",
      "Epoch: 10/20; Batch:117/468; Training loss:0.101332\n",
      "Epoch: 10/20; Batch:118/468; Training loss:0.0989617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20; Batch:119/468; Training loss:0.101277\n",
      "Epoch: 10/20; Batch:120/468; Training loss:0.102419\n",
      "Epoch: 10/20; Batch:121/468; Training loss:0.0977879\n",
      "Epoch: 10/20; Batch:122/468; Training loss:0.100457\n",
      "Epoch: 10/20; Batch:123/468; Training loss:0.0991029\n",
      "Epoch: 10/20; Batch:124/468; Training loss:0.101255\n",
      "Epoch: 10/20; Batch:125/468; Training loss:0.0998117\n",
      "Epoch: 10/20; Batch:126/468; Training loss:0.102207\n",
      "Epoch: 10/20; Batch:127/468; Training loss:0.0948693\n",
      "Epoch: 10/20; Batch:128/468; Training loss:0.102153\n",
      "Epoch: 10/20; Batch:129/468; Training loss:0.0996892\n",
      "Epoch: 10/20; Batch:130/468; Training loss:0.0996289\n",
      "Epoch: 10/20; Batch:131/468; Training loss:0.0996049\n",
      "Epoch: 10/20; Batch:132/468; Training loss:0.0985643\n",
      "Epoch: 10/20; Batch:133/468; Training loss:0.0992752\n",
      "Epoch: 10/20; Batch:134/468; Training loss:0.100649\n",
      "Epoch: 10/20; Batch:135/468; Training loss:0.100911\n",
      "Epoch: 10/20; Batch:136/468; Training loss:0.0957012\n",
      "Epoch: 10/20; Batch:137/468; Training loss:0.099132\n",
      "Epoch: 10/20; Batch:138/468; Training loss:0.0964458\n",
      "Epoch: 10/20; Batch:139/468; Training loss:0.0984646\n",
      "Epoch: 10/20; Batch:140/468; Training loss:0.0959826\n",
      "Epoch: 10/20; Batch:141/468; Training loss:0.0946286\n",
      "Epoch: 10/20; Batch:142/468; Training loss:0.100086\n",
      "Epoch: 10/20; Batch:143/468; Training loss:0.0996609\n",
      "Epoch: 10/20; Batch:144/468; Training loss:0.100104\n",
      "Epoch: 10/20; Batch:145/468; Training loss:0.099049\n",
      "Epoch: 10/20; Batch:146/468; Training loss:0.0962063\n",
      "Epoch: 10/20; Batch:147/468; Training loss:0.0981485\n",
      "Epoch: 10/20; Batch:148/468; Training loss:0.102739\n",
      "Epoch: 10/20; Batch:149/468; Training loss:0.0958958\n",
      "Epoch: 10/20; Batch:150/468; Training loss:0.103458\n",
      "Epoch: 10/20; Batch:151/468; Training loss:0.10026\n",
      "Epoch: 10/20; Batch:152/468; Training loss:0.0990899\n",
      "Epoch: 10/20; Batch:153/468; Training loss:0.104419\n",
      "Epoch: 10/20; Batch:154/468; Training loss:0.10137\n",
      "Epoch: 10/20; Batch:155/468; Training loss:0.0972511\n",
      "Epoch: 10/20; Batch:156/468; Training loss:0.101058\n",
      "Epoch: 10/20; Batch:157/468; Training loss:0.101976\n",
      "Epoch: 10/20; Batch:158/468; Training loss:0.100066\n",
      "Epoch: 10/20; Batch:159/468; Training loss:0.0939942\n",
      "Epoch: 10/20; Batch:160/468; Training loss:0.100447\n",
      "Epoch: 10/20; Batch:161/468; Training loss:0.0964487\n",
      "Epoch: 10/20; Batch:162/468; Training loss:0.0970212\n",
      "Epoch: 10/20; Batch:163/468; Training loss:0.100974\n",
      "Epoch: 10/20; Batch:164/468; Training loss:0.0963695\n",
      "Epoch: 10/20; Batch:165/468; Training loss:0.0969341\n",
      "Epoch: 10/20; Batch:166/468; Training loss:0.0994484\n",
      "Epoch: 10/20; Batch:167/468; Training loss:0.0994324\n",
      "Epoch: 10/20; Batch:168/468; Training loss:0.098922\n",
      "Epoch: 10/20; Batch:169/468; Training loss:0.101142\n",
      "Epoch: 10/20; Batch:170/468; Training loss:0.0970526\n",
      "Epoch: 10/20; Batch:171/468; Training loss:0.0953239\n",
      "Epoch: 10/20; Batch:172/468; Training loss:0.0998025\n",
      "Epoch: 10/20; Batch:173/468; Training loss:0.0963646\n",
      "Epoch: 10/20; Batch:174/468; Training loss:0.100417\n",
      "Epoch: 10/20; Batch:175/468; Training loss:0.0991787\n",
      "Epoch: 10/20; Batch:176/468; Training loss:0.0987747\n",
      "Epoch: 10/20; Batch:177/468; Training loss:0.0998858\n",
      "Epoch: 10/20; Batch:178/468; Training loss:0.0997101\n",
      "Epoch: 10/20; Batch:179/468; Training loss:0.098691\n",
      "Epoch: 10/20; Batch:180/468; Training loss:0.0998802\n",
      "Epoch: 10/20; Batch:181/468; Training loss:0.0978147\n",
      "Epoch: 10/20; Batch:182/468; Training loss:0.0972232\n",
      "Epoch: 10/20; Batch:183/468; Training loss:0.0996195\n",
      "Epoch: 10/20; Batch:184/468; Training loss:0.0973722\n",
      "Epoch: 10/20; Batch:185/468; Training loss:0.100512\n",
      "Epoch: 10/20; Batch:186/468; Training loss:0.0989571\n",
      "Epoch: 10/20; Batch:187/468; Training loss:0.0960519\n",
      "Epoch: 10/20; Batch:188/468; Training loss:0.0962257\n",
      "Epoch: 10/20; Batch:189/468; Training loss:0.101506\n",
      "Epoch: 10/20; Batch:190/468; Training loss:0.0991024\n",
      "Epoch: 10/20; Batch:191/468; Training loss:0.0982786\n",
      "Epoch: 10/20; Batch:192/468; Training loss:0.0993667\n",
      "Epoch: 10/20; Batch:193/468; Training loss:0.099238\n",
      "Epoch: 10/20; Batch:194/468; Training loss:0.0961869\n",
      "Epoch: 10/20; Batch:195/468; Training loss:0.0983322\n",
      "Epoch: 10/20; Batch:196/468; Training loss:0.100898\n",
      "Epoch: 10/20; Batch:197/468; Training loss:0.0990431\n",
      "Epoch: 10/20; Batch:198/468; Training loss:0.100837\n",
      "Epoch: 10/20; Batch:199/468; Training loss:0.101134\n",
      "Epoch: 10/20; Batch:200/468; Training loss:0.0996347\n",
      "Epoch: 10/20; Batch:201/468; Training loss:0.0988767\n",
      "Epoch: 10/20; Batch:202/468; Training loss:0.100689\n",
      "Epoch: 10/20; Batch:203/468; Training loss:0.097544\n",
      "Epoch: 10/20; Batch:204/468; Training loss:0.104888\n",
      "Epoch: 10/20; Batch:205/468; Training loss:0.096278\n",
      "Epoch: 10/20; Batch:206/468; Training loss:0.104706\n",
      "Epoch: 10/20; Batch:207/468; Training loss:0.100483\n",
      "Epoch: 10/20; Batch:208/468; Training loss:0.095103\n",
      "Epoch: 10/20; Batch:209/468; Training loss:0.100001\n",
      "Epoch: 10/20; Batch:210/468; Training loss:0.0993656\n",
      "Epoch: 10/20; Batch:211/468; Training loss:0.0983894\n",
      "Epoch: 10/20; Batch:212/468; Training loss:0.100175\n",
      "Epoch: 10/20; Batch:213/468; Training loss:0.0958859\n",
      "Epoch: 10/20; Batch:214/468; Training loss:0.0995009\n",
      "Epoch: 10/20; Batch:215/468; Training loss:0.0991629\n",
      "Epoch: 10/20; Batch:216/468; Training loss:0.10234\n",
      "Epoch: 10/20; Batch:217/468; Training loss:0.101033\n",
      "Epoch: 10/20; Batch:218/468; Training loss:0.0992891\n",
      "Epoch: 10/20; Batch:219/468; Training loss:0.100828\n",
      "Epoch: 10/20; Batch:220/468; Training loss:0.101081\n",
      "Epoch: 10/20; Batch:221/468; Training loss:0.100627\n",
      "Epoch: 10/20; Batch:222/468; Training loss:0.0976603\n",
      "Epoch: 10/20; Batch:223/468; Training loss:0.100213\n",
      "Epoch: 10/20; Batch:224/468; Training loss:0.103763\n",
      "Epoch: 10/20; Batch:225/468; Training loss:0.101958\n",
      "Epoch: 10/20; Batch:226/468; Training loss:0.0998333\n",
      "Epoch: 10/20; Batch:227/468; Training loss:0.0961572\n",
      "Epoch: 10/20; Batch:228/468; Training loss:0.0993432\n",
      "Epoch: 10/20; Batch:229/468; Training loss:0.10323\n",
      "Epoch: 10/20; Batch:230/468; Training loss:0.0981452\n",
      "Epoch: 10/20; Batch:231/468; Training loss:0.100452\n",
      "Epoch: 10/20; Batch:232/468; Training loss:0.0971745\n",
      "Epoch: 10/20; Batch:233/468; Training loss:0.097648\n",
      "Epoch: 10/20; Batch:234/468; Training loss:0.100548\n",
      "Epoch: 10/20; Batch:235/468; Training loss:0.0982232\n",
      "Epoch: 10/20; Batch:236/468; Training loss:0.0978491\n",
      "Epoch: 10/20; Batch:237/468; Training loss:0.0930106\n",
      "Epoch: 10/20; Batch:238/468; Training loss:0.100419\n",
      "Epoch: 10/20; Batch:239/468; Training loss:0.099333\n",
      "Epoch: 10/20; Batch:240/468; Training loss:0.0997306\n",
      "Epoch: 10/20; Batch:241/468; Training loss:0.0988393\n",
      "Epoch: 10/20; Batch:242/468; Training loss:0.0968776\n",
      "Epoch: 10/20; Batch:243/468; Training loss:0.099515\n",
      "Epoch: 10/20; Batch:244/468; Training loss:0.101602\n",
      "Epoch: 10/20; Batch:245/468; Training loss:0.0973502\n",
      "Epoch: 10/20; Batch:246/468; Training loss:0.103647\n",
      "Epoch: 10/20; Batch:247/468; Training loss:0.102153\n",
      "Epoch: 10/20; Batch:248/468; Training loss:0.0975946\n",
      "Epoch: 10/20; Batch:249/468; Training loss:0.0958553\n",
      "Epoch: 10/20; Batch:250/468; Training loss:0.101109\n",
      "Epoch: 10/20; Batch:251/468; Training loss:0.0955361\n",
      "Epoch: 10/20; Batch:252/468; Training loss:0.103071\n",
      "Epoch: 10/20; Batch:253/468; Training loss:0.101641\n",
      "Epoch: 10/20; Batch:254/468; Training loss:0.102974\n",
      "Epoch: 10/20; Batch:255/468; Training loss:0.103045\n",
      "Epoch: 10/20; Batch:256/468; Training loss:0.0950595\n",
      "Epoch: 10/20; Batch:257/468; Training loss:0.104048\n",
      "Epoch: 10/20; Batch:258/468; Training loss:0.0962679\n",
      "Epoch: 10/20; Batch:259/468; Training loss:0.0984242\n",
      "Epoch: 10/20; Batch:260/468; Training loss:0.0989777\n",
      "Epoch: 10/20; Batch:261/468; Training loss:0.099554\n",
      "Epoch: 10/20; Batch:262/468; Training loss:0.101806\n",
      "Epoch: 10/20; Batch:263/468; Training loss:0.100261\n",
      "Epoch: 10/20; Batch:264/468; Training loss:0.103691\n",
      "Epoch: 10/20; Batch:265/468; Training loss:0.10245\n",
      "Epoch: 10/20; Batch:266/468; Training loss:0.0996756\n",
      "Epoch: 10/20; Batch:267/468; Training loss:0.093317\n",
      "Epoch: 10/20; Batch:268/468; Training loss:0.101167\n",
      "Epoch: 10/20; Batch:269/468; Training loss:0.0998629\n",
      "Epoch: 10/20; Batch:270/468; Training loss:0.0993798\n",
      "Epoch: 10/20; Batch:271/468; Training loss:0.101642\n",
      "Epoch: 10/20; Batch:272/468; Training loss:0.0974708\n",
      "Epoch: 10/20; Batch:273/468; Training loss:0.0964919\n",
      "Epoch: 10/20; Batch:274/468; Training loss:0.0994664\n",
      "Epoch: 10/20; Batch:275/468; Training loss:0.101849\n",
      "Epoch: 10/20; Batch:276/468; Training loss:0.0980322\n",
      "Epoch: 10/20; Batch:277/468; Training loss:0.0991498\n",
      "Epoch: 10/20; Batch:278/468; Training loss:0.0971789\n",
      "Epoch: 10/20; Batch:279/468; Training loss:0.100112\n",
      "Epoch: 10/20; Batch:280/468; Training loss:0.102156\n",
      "Epoch: 10/20; Batch:281/468; Training loss:0.0968071\n",
      "Epoch: 10/20; Batch:282/468; Training loss:0.0998184\n",
      "Epoch: 10/20; Batch:283/468; Training loss:0.0998246\n",
      "Epoch: 10/20; Batch:284/468; Training loss:0.0973292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20; Batch:285/468; Training loss:0.102005\n",
      "Epoch: 10/20; Batch:286/468; Training loss:0.0968995\n",
      "Epoch: 10/20; Batch:287/468; Training loss:0.101537\n",
      "Epoch: 10/20; Batch:288/468; Training loss:0.100658\n",
      "Epoch: 10/20; Batch:289/468; Training loss:0.0942915\n",
      "Epoch: 10/20; Batch:290/468; Training loss:0.101451\n",
      "Epoch: 10/20; Batch:291/468; Training loss:0.101369\n",
      "Epoch: 10/20; Batch:292/468; Training loss:0.0990537\n",
      "Epoch: 10/20; Batch:293/468; Training loss:0.0982563\n",
      "Epoch: 10/20; Batch:294/468; Training loss:0.0976103\n",
      "Epoch: 10/20; Batch:295/468; Training loss:0.0966176\n",
      "Epoch: 10/20; Batch:296/468; Training loss:0.0992016\n",
      "Epoch: 10/20; Batch:297/468; Training loss:0.101991\n",
      "Epoch: 10/20; Batch:298/468; Training loss:0.0995521\n",
      "Epoch: 10/20; Batch:299/468; Training loss:0.0972609\n",
      "Epoch: 10/20; Batch:300/468; Training loss:0.0984103\n",
      "Epoch: 10/20; Batch:301/468; Training loss:0.0993784\n",
      "Epoch: 10/20; Batch:302/468; Training loss:0.100738\n",
      "Epoch: 10/20; Batch:303/468; Training loss:0.0980644\n",
      "Epoch: 10/20; Batch:304/468; Training loss:0.0979605\n",
      "Epoch: 10/20; Batch:305/468; Training loss:0.0973558\n",
      "Epoch: 10/20; Batch:306/468; Training loss:0.0975263\n",
      "Epoch: 10/20; Batch:307/468; Training loss:0.102094\n",
      "Epoch: 10/20; Batch:308/468; Training loss:0.0981225\n",
      "Epoch: 10/20; Batch:309/468; Training loss:0.104093\n",
      "Epoch: 10/20; Batch:310/468; Training loss:0.0941081\n",
      "Epoch: 10/20; Batch:311/468; Training loss:0.0985223\n",
      "Epoch: 10/20; Batch:312/468; Training loss:0.100851\n",
      "Epoch: 10/20; Batch:313/468; Training loss:0.0979522\n",
      "Epoch: 10/20; Batch:314/468; Training loss:0.0980612\n",
      "Epoch: 10/20; Batch:315/468; Training loss:0.102415\n",
      "Epoch: 10/20; Batch:316/468; Training loss:0.0981516\n",
      "Epoch: 10/20; Batch:317/468; Training loss:0.0996322\n",
      "Epoch: 10/20; Batch:318/468; Training loss:0.0972353\n",
      "Epoch: 10/20; Batch:319/468; Training loss:0.0953107\n",
      "Epoch: 10/20; Batch:320/468; Training loss:0.0981837\n",
      "Epoch: 10/20; Batch:321/468; Training loss:0.0966984\n",
      "Epoch: 10/20; Batch:322/468; Training loss:0.0998356\n",
      "Epoch: 10/20; Batch:323/468; Training loss:0.0980142\n",
      "Epoch: 10/20; Batch:324/468; Training loss:0.0965985\n",
      "Epoch: 10/20; Batch:325/468; Training loss:0.0978146\n",
      "Epoch: 10/20; Batch:326/468; Training loss:0.100117\n",
      "Epoch: 10/20; Batch:327/468; Training loss:0.0982869\n",
      "Epoch: 10/20; Batch:328/468; Training loss:0.0922479\n",
      "Epoch: 10/20; Batch:329/468; Training loss:0.0987179\n",
      "Epoch: 10/20; Batch:330/468; Training loss:0.0987449\n",
      "Epoch: 10/20; Batch:331/468; Training loss:0.0986647\n",
      "Epoch: 10/20; Batch:332/468; Training loss:0.0961784\n",
      "Epoch: 10/20; Batch:333/468; Training loss:0.0979439\n",
      "Epoch: 10/20; Batch:334/468; Training loss:0.0996245\n",
      "Epoch: 10/20; Batch:335/468; Training loss:0.0990297\n",
      "Epoch: 10/20; Batch:336/468; Training loss:0.100324\n",
      "Epoch: 10/20; Batch:337/468; Training loss:0.0999607\n",
      "Epoch: 10/20; Batch:338/468; Training loss:0.0983966\n",
      "Epoch: 10/20; Batch:339/468; Training loss:0.0952659\n",
      "Epoch: 10/20; Batch:340/468; Training loss:0.0932933\n",
      "Epoch: 10/20; Batch:341/468; Training loss:0.0980595\n",
      "Epoch: 10/20; Batch:342/468; Training loss:0.0995494\n",
      "Epoch: 10/20; Batch:343/468; Training loss:0.096216\n",
      "Epoch: 10/20; Batch:344/468; Training loss:0.0991373\n",
      "Epoch: 10/20; Batch:345/468; Training loss:0.0964691\n",
      "Epoch: 10/20; Batch:346/468; Training loss:0.098568\n",
      "Epoch: 10/20; Batch:347/468; Training loss:0.0990614\n",
      "Epoch: 10/20; Batch:348/468; Training loss:0.0999231\n",
      "Epoch: 10/20; Batch:349/468; Training loss:0.0962561\n",
      "Epoch: 10/20; Batch:350/468; Training loss:0.100601\n",
      "Epoch: 10/20; Batch:351/468; Training loss:0.100705\n",
      "Epoch: 10/20; Batch:352/468; Training loss:0.0987638\n",
      "Epoch: 10/20; Batch:353/468; Training loss:0.102381\n",
      "Epoch: 10/20; Batch:354/468; Training loss:0.0964298\n",
      "Epoch: 10/20; Batch:355/468; Training loss:0.0998208\n",
      "Epoch: 10/20; Batch:356/468; Training loss:0.0945526\n",
      "Epoch: 10/20; Batch:357/468; Training loss:0.0993589\n",
      "Epoch: 10/20; Batch:358/468; Training loss:0.101081\n",
      "Epoch: 10/20; Batch:359/468; Training loss:0.0994281\n",
      "Epoch: 10/20; Batch:360/468; Training loss:0.0982763\n",
      "Epoch: 10/20; Batch:361/468; Training loss:0.0983105\n",
      "Epoch: 10/20; Batch:362/468; Training loss:0.10037\n",
      "Epoch: 10/20; Batch:363/468; Training loss:0.100133\n",
      "Epoch: 10/20; Batch:364/468; Training loss:0.0986808\n",
      "Epoch: 10/20; Batch:365/468; Training loss:0.0989572\n",
      "Epoch: 10/20; Batch:366/468; Training loss:0.101717\n",
      "Epoch: 10/20; Batch:367/468; Training loss:0.10137\n",
      "Epoch: 10/20; Batch:368/468; Training loss:0.0976459\n",
      "Epoch: 10/20; Batch:369/468; Training loss:0.099782\n",
      "Epoch: 10/20; Batch:370/468; Training loss:0.101951\n",
      "Epoch: 10/20; Batch:371/468; Training loss:0.10106\n",
      "Epoch: 10/20; Batch:372/468; Training loss:0.103451\n",
      "Epoch: 10/20; Batch:373/468; Training loss:0.103635\n",
      "Epoch: 10/20; Batch:374/468; Training loss:0.102043\n",
      "Epoch: 10/20; Batch:375/468; Training loss:0.103331\n",
      "Epoch: 10/20; Batch:376/468; Training loss:0.101138\n",
      "Epoch: 10/20; Batch:377/468; Training loss:0.100503\n",
      "Epoch: 10/20; Batch:378/468; Training loss:0.0974167\n",
      "Epoch: 10/20; Batch:379/468; Training loss:0.100462\n",
      "Epoch: 10/20; Batch:380/468; Training loss:0.0946943\n",
      "Epoch: 10/20; Batch:381/468; Training loss:0.0999454\n",
      "Epoch: 10/20; Batch:382/468; Training loss:0.0981699\n",
      "Epoch: 10/20; Batch:383/468; Training loss:0.0979365\n",
      "Epoch: 10/20; Batch:384/468; Training loss:0.0998916\n",
      "Epoch: 10/20; Batch:385/468; Training loss:0.0965866\n",
      "Epoch: 10/20; Batch:386/468; Training loss:0.100158\n",
      "Epoch: 10/20; Batch:387/468; Training loss:0.101517\n",
      "Epoch: 10/20; Batch:388/468; Training loss:0.101181\n",
      "Epoch: 10/20; Batch:389/468; Training loss:0.102711\n",
      "Epoch: 10/20; Batch:390/468; Training loss:0.100145\n",
      "Epoch: 10/20; Batch:391/468; Training loss:0.0999149\n",
      "Epoch: 10/20; Batch:392/468; Training loss:0.0999337\n",
      "Epoch: 10/20; Batch:393/468; Training loss:0.101296\n",
      "Epoch: 10/20; Batch:394/468; Training loss:0.0993095\n",
      "Epoch: 10/20; Batch:395/468; Training loss:0.102099\n",
      "Epoch: 10/20; Batch:396/468; Training loss:0.0988719\n",
      "Epoch: 10/20; Batch:397/468; Training loss:0.096521\n",
      "Epoch: 10/20; Batch:398/468; Training loss:0.100363\n",
      "Epoch: 10/20; Batch:399/468; Training loss:0.0991199\n",
      "Epoch: 10/20; Batch:400/468; Training loss:0.0971338\n",
      "Epoch: 10/20; Batch:401/468; Training loss:0.101651\n",
      "Epoch: 10/20; Batch:402/468; Training loss:0.0990088\n",
      "Epoch: 10/20; Batch:403/468; Training loss:0.0977657\n",
      "Epoch: 10/20; Batch:404/468; Training loss:0.0979685\n",
      "Epoch: 10/20; Batch:405/468; Training loss:0.0991502\n",
      "Epoch: 10/20; Batch:406/468; Training loss:0.10162\n",
      "Epoch: 10/20; Batch:407/468; Training loss:0.0994213\n",
      "Epoch: 10/20; Batch:408/468; Training loss:0.0959865\n",
      "Epoch: 10/20; Batch:409/468; Training loss:0.10459\n",
      "Epoch: 10/20; Batch:410/468; Training loss:0.098864\n",
      "Epoch: 10/20; Batch:411/468; Training loss:0.0993759\n",
      "Epoch: 10/20; Batch:412/468; Training loss:0.102081\n",
      "Epoch: 10/20; Batch:413/468; Training loss:0.0986676\n",
      "Epoch: 10/20; Batch:414/468; Training loss:0.100113\n",
      "Epoch: 10/20; Batch:415/468; Training loss:0.0979143\n",
      "Epoch: 10/20; Batch:416/468; Training loss:0.0968218\n",
      "Epoch: 10/20; Batch:417/468; Training loss:0.0980236\n",
      "Epoch: 10/20; Batch:418/468; Training loss:0.102563\n",
      "Epoch: 10/20; Batch:419/468; Training loss:0.100082\n",
      "Epoch: 10/20; Batch:420/468; Training loss:0.0997445\n",
      "Epoch: 10/20; Batch:421/468; Training loss:0.0967648\n",
      "Epoch: 10/20; Batch:422/468; Training loss:0.0984091\n",
      "Epoch: 10/20; Batch:423/468; Training loss:0.0997889\n",
      "Epoch: 10/20; Batch:424/468; Training loss:0.0945218\n",
      "Epoch: 10/20; Batch:425/468; Training loss:0.0979165\n",
      "Epoch: 10/20; Batch:426/468; Training loss:0.0975404\n",
      "Epoch: 10/20; Batch:427/468; Training loss:0.10048\n",
      "Epoch: 10/20; Batch:428/468; Training loss:0.100224\n",
      "Epoch: 10/20; Batch:429/468; Training loss:0.0994938\n",
      "Epoch: 10/20; Batch:430/468; Training loss:0.0957901\n",
      "Epoch: 10/20; Batch:431/468; Training loss:0.0991153\n",
      "Epoch: 10/20; Batch:432/468; Training loss:0.0994318\n",
      "Epoch: 10/20; Batch:433/468; Training loss:0.0980435\n",
      "Epoch: 10/20; Batch:434/468; Training loss:0.0958707\n",
      "Epoch: 10/20; Batch:435/468; Training loss:0.0981593\n",
      "Epoch: 10/20; Batch:436/468; Training loss:0.0945043\n",
      "Epoch: 10/20; Batch:437/468; Training loss:0.101207\n",
      "Epoch: 10/20; Batch:438/468; Training loss:0.098151\n",
      "Epoch: 10/20; Batch:439/468; Training loss:0.0990171\n",
      "Epoch: 10/20; Batch:440/468; Training loss:0.100819\n",
      "Epoch: 10/20; Batch:441/468; Training loss:0.0989869\n",
      "Epoch: 10/20; Batch:442/468; Training loss:0.0964055\n",
      "Epoch: 10/20; Batch:443/468; Training loss:0.0978029\n",
      "Epoch: 10/20; Batch:444/468; Training loss:0.0986351\n",
      "Epoch: 10/20; Batch:445/468; Training loss:0.099101\n",
      "Epoch: 10/20; Batch:446/468; Training loss:0.102282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20; Batch:447/468; Training loss:0.100625\n",
      "Epoch: 10/20; Batch:448/468; Training loss:0.0972208\n",
      "Epoch: 10/20; Batch:449/468; Training loss:0.0989645\n",
      "Epoch: 10/20; Batch:450/468; Training loss:0.0999739\n",
      "Epoch: 10/20; Batch:451/468; Training loss:0.0943611\n",
      "Epoch: 10/20; Batch:452/468; Training loss:0.0994326\n",
      "Epoch: 10/20; Batch:453/468; Training loss:0.0978554\n",
      "Epoch: 10/20; Batch:454/468; Training loss:0.0991229\n",
      "Epoch: 10/20; Batch:455/468; Training loss:0.100863\n",
      "Epoch: 10/20; Batch:456/468; Training loss:0.100255\n",
      "Epoch: 10/20; Batch:457/468; Training loss:0.0987318\n",
      "Epoch: 10/20; Batch:458/468; Training loss:0.100896\n",
      "Epoch: 10/20; Batch:459/468; Training loss:0.0997433\n",
      "Epoch: 10/20; Batch:460/468; Training loss:0.0993594\n",
      "Epoch: 10/20; Batch:461/468; Training loss:0.102057\n",
      "Epoch: 10/20; Batch:462/468; Training loss:0.0998792\n",
      "Epoch: 10/20; Batch:463/468; Training loss:0.0991438\n",
      "Epoch: 10/20; Batch:464/468; Training loss:0.100613\n",
      "Epoch: 10/20; Batch:465/468; Training loss:0.095963\n",
      "Epoch: 10/20; Batch:466/468; Training loss:0.0994161\n",
      "Epoch: 10/20; Batch:467/468; Training loss:0.0983921\n",
      "Epoch: 10/20; Batch:468/468; Training loss:0.102575\n",
      "Epoch: 11/20; Batch:1/468; Training loss:0.0951682\n",
      "Epoch: 11/20; Batch:2/468; Training loss:0.101749\n",
      "Epoch: 11/20; Batch:3/468; Training loss:0.096275\n",
      "Epoch: 11/20; Batch:4/468; Training loss:0.0984085\n",
      "Epoch: 11/20; Batch:5/468; Training loss:0.097344\n",
      "Epoch: 11/20; Batch:6/468; Training loss:0.0970505\n",
      "Epoch: 11/20; Batch:7/468; Training loss:0.0980378\n",
      "Epoch: 11/20; Batch:8/468; Training loss:0.0962868\n",
      "Epoch: 11/20; Batch:9/468; Training loss:0.0987029\n",
      "Epoch: 11/20; Batch:10/468; Training loss:0.100164\n",
      "Epoch: 11/20; Batch:11/468; Training loss:0.0988655\n",
      "Epoch: 11/20; Batch:12/468; Training loss:0.0922847\n",
      "Epoch: 11/20; Batch:13/468; Training loss:0.0992159\n",
      "Epoch: 11/20; Batch:14/468; Training loss:0.0966994\n",
      "Epoch: 11/20; Batch:15/468; Training loss:0.100439\n",
      "Epoch: 11/20; Batch:16/468; Training loss:0.0989741\n",
      "Epoch: 11/20; Batch:17/468; Training loss:0.0982012\n",
      "Epoch: 11/20; Batch:18/468; Training loss:0.101249\n",
      "Epoch: 11/20; Batch:19/468; Training loss:0.0973675\n",
      "Epoch: 11/20; Batch:20/468; Training loss:0.0991369\n",
      "Epoch: 11/20; Batch:21/468; Training loss:0.09872\n",
      "Epoch: 11/20; Batch:22/468; Training loss:0.0987682\n",
      "Epoch: 11/20; Batch:23/468; Training loss:0.0994265\n",
      "Epoch: 11/20; Batch:24/468; Training loss:0.0977569\n",
      "Epoch: 11/20; Batch:25/468; Training loss:0.0994375\n",
      "Epoch: 11/20; Batch:26/468; Training loss:0.102156\n",
      "Epoch: 11/20; Batch:27/468; Training loss:0.104151\n",
      "Epoch: 11/20; Batch:28/468; Training loss:0.0988039\n",
      "Epoch: 11/20; Batch:29/468; Training loss:0.0972069\n",
      "Epoch: 11/20; Batch:30/468; Training loss:0.100179\n",
      "Epoch: 11/20; Batch:31/468; Training loss:0.100664\n",
      "Epoch: 11/20; Batch:32/468; Training loss:0.103306\n",
      "Epoch: 11/20; Batch:33/468; Training loss:0.105048\n",
      "Epoch: 11/20; Batch:34/468; Training loss:0.0963035\n",
      "Epoch: 11/20; Batch:35/468; Training loss:0.0998354\n",
      "Epoch: 11/20; Batch:36/468; Training loss:0.095185\n",
      "Epoch: 11/20; Batch:37/468; Training loss:0.102024\n",
      "Epoch: 11/20; Batch:38/468; Training loss:0.101142\n",
      "Epoch: 11/20; Batch:39/468; Training loss:0.10239\n",
      "Epoch: 11/20; Batch:40/468; Training loss:0.101011\n",
      "Epoch: 11/20; Batch:41/468; Training loss:0.104938\n",
      "Epoch: 11/20; Batch:42/468; Training loss:0.0962753\n",
      "Epoch: 11/20; Batch:43/468; Training loss:0.101585\n",
      "Epoch: 11/20; Batch:44/468; Training loss:0.0978378\n",
      "Epoch: 11/20; Batch:45/468; Training loss:0.10186\n",
      "Epoch: 11/20; Batch:46/468; Training loss:0.0977228\n",
      "Epoch: 11/20; Batch:47/468; Training loss:0.100517\n",
      "Epoch: 11/20; Batch:48/468; Training loss:0.100111\n",
      "Epoch: 11/20; Batch:49/468; Training loss:0.0956607\n",
      "Epoch: 11/20; Batch:50/468; Training loss:0.0970509\n",
      "Epoch: 11/20; Batch:51/468; Training loss:0.100058\n",
      "Epoch: 11/20; Batch:52/468; Training loss:0.097324\n",
      "Epoch: 11/20; Batch:53/468; Training loss:0.0958605\n",
      "Epoch: 11/20; Batch:54/468; Training loss:0.0990042\n",
      "Epoch: 11/20; Batch:55/468; Training loss:0.0938575\n",
      "Epoch: 11/20; Batch:56/468; Training loss:0.100101\n",
      "Epoch: 11/20; Batch:57/468; Training loss:0.100054\n",
      "Epoch: 11/20; Batch:58/468; Training loss:0.0975495\n",
      "Epoch: 11/20; Batch:59/468; Training loss:0.0982298\n",
      "Epoch: 11/20; Batch:60/468; Training loss:0.100925\n",
      "Epoch: 11/20; Batch:61/468; Training loss:0.0981221\n",
      "Epoch: 11/20; Batch:62/468; Training loss:0.102241\n",
      "Epoch: 11/20; Batch:63/468; Training loss:0.0999622\n",
      "Epoch: 11/20; Batch:64/468; Training loss:0.0981961\n",
      "Epoch: 11/20; Batch:65/468; Training loss:0.0977849\n",
      "Epoch: 11/20; Batch:66/468; Training loss:0.100313\n",
      "Epoch: 11/20; Batch:67/468; Training loss:0.100004\n",
      "Epoch: 11/20; Batch:68/468; Training loss:0.0985434\n",
      "Epoch: 11/20; Batch:69/468; Training loss:0.101365\n",
      "Epoch: 11/20; Batch:70/468; Training loss:0.0968514\n",
      "Epoch: 11/20; Batch:71/468; Training loss:0.0995413\n",
      "Epoch: 11/20; Batch:72/468; Training loss:0.101563\n",
      "Epoch: 11/20; Batch:73/468; Training loss:0.0967908\n",
      "Epoch: 11/20; Batch:74/468; Training loss:0.0956074\n",
      "Epoch: 11/20; Batch:75/468; Training loss:0.0939519\n",
      "Epoch: 11/20; Batch:76/468; Training loss:0.102794\n",
      "Epoch: 11/20; Batch:77/468; Training loss:0.101473\n",
      "Epoch: 11/20; Batch:78/468; Training loss:0.1008\n",
      "Epoch: 11/20; Batch:79/468; Training loss:0.0969394\n",
      "Epoch: 11/20; Batch:80/468; Training loss:0.095476\n",
      "Epoch: 11/20; Batch:81/468; Training loss:0.0961083\n",
      "Epoch: 11/20; Batch:82/468; Training loss:0.0982433\n",
      "Epoch: 11/20; Batch:83/468; Training loss:0.0987249\n",
      "Epoch: 11/20; Batch:84/468; Training loss:0.0997119\n",
      "Epoch: 11/20; Batch:85/468; Training loss:0.0971508\n",
      "Epoch: 11/20; Batch:86/468; Training loss:0.0959977\n",
      "Epoch: 11/20; Batch:87/468; Training loss:0.0951172\n",
      "Epoch: 11/20; Batch:88/468; Training loss:0.0975959\n",
      "Epoch: 11/20; Batch:89/468; Training loss:0.100385\n",
      "Epoch: 11/20; Batch:90/468; Training loss:0.0994709\n",
      "Epoch: 11/20; Batch:91/468; Training loss:0.100158\n",
      "Epoch: 11/20; Batch:92/468; Training loss:0.0982092\n",
      "Epoch: 11/20; Batch:93/468; Training loss:0.0985861\n",
      "Epoch: 11/20; Batch:94/468; Training loss:0.0965027\n",
      "Epoch: 11/20; Batch:95/468; Training loss:0.100825\n",
      "Epoch: 11/20; Batch:96/468; Training loss:0.0957586\n",
      "Epoch: 11/20; Batch:97/468; Training loss:0.101123\n",
      "Epoch: 11/20; Batch:98/468; Training loss:0.0961633\n",
      "Epoch: 11/20; Batch:99/468; Training loss:0.0978509\n",
      "Epoch: 11/20; Batch:100/468; Training loss:0.102294\n",
      "Epoch: 11/20; Batch:101/468; Training loss:0.101364\n",
      "Epoch: 11/20; Batch:102/468; Training loss:0.0997583\n",
      "Epoch: 11/20; Batch:103/468; Training loss:0.0981676\n",
      "Epoch: 11/20; Batch:104/468; Training loss:0.10156\n",
      "Epoch: 11/20; Batch:105/468; Training loss:0.0997253\n",
      "Epoch: 11/20; Batch:106/468; Training loss:0.100863\n",
      "Epoch: 11/20; Batch:107/468; Training loss:0.0974833\n",
      "Epoch: 11/20; Batch:108/468; Training loss:0.100233\n",
      "Epoch: 11/20; Batch:109/468; Training loss:0.0963524\n",
      "Epoch: 11/20; Batch:110/468; Training loss:0.0968297\n",
      "Epoch: 11/20; Batch:111/468; Training loss:0.0999195\n",
      "Epoch: 11/20; Batch:112/468; Training loss:0.100651\n",
      "Epoch: 11/20; Batch:113/468; Training loss:0.101257\n",
      "Epoch: 11/20; Batch:114/468; Training loss:0.0997538\n",
      "Epoch: 11/20; Batch:115/468; Training loss:0.100877\n",
      "Epoch: 11/20; Batch:116/468; Training loss:0.0985682\n",
      "Epoch: 11/20; Batch:117/468; Training loss:0.0987298\n",
      "Epoch: 11/20; Batch:118/468; Training loss:0.10018\n",
      "Epoch: 11/20; Batch:119/468; Training loss:0.0982135\n",
      "Epoch: 11/20; Batch:120/468; Training loss:0.0993538\n",
      "Epoch: 11/20; Batch:121/468; Training loss:0.0953973\n",
      "Epoch: 11/20; Batch:122/468; Training loss:0.0973158\n",
      "Epoch: 11/20; Batch:123/468; Training loss:0.101979\n",
      "Epoch: 11/20; Batch:124/468; Training loss:0.0964517\n",
      "Epoch: 11/20; Batch:125/468; Training loss:0.0998268\n",
      "Epoch: 11/20; Batch:126/468; Training loss:0.102077\n",
      "Epoch: 11/20; Batch:127/468; Training loss:0.0982\n",
      "Epoch: 11/20; Batch:128/468; Training loss:0.100699\n",
      "Epoch: 11/20; Batch:129/468; Training loss:0.0981095\n",
      "Epoch: 11/20; Batch:130/468; Training loss:0.0960995\n",
      "Epoch: 11/20; Batch:131/468; Training loss:0.098146\n",
      "Epoch: 11/20; Batch:132/468; Training loss:0.101536\n",
      "Epoch: 11/20; Batch:133/468; Training loss:0.0977811\n",
      "Epoch: 11/20; Batch:134/468; Training loss:0.102325\n",
      "Epoch: 11/20; Batch:135/468; Training loss:0.100486\n",
      "Epoch: 11/20; Batch:136/468; Training loss:0.099114\n",
      "Epoch: 11/20; Batch:137/468; Training loss:0.10357\n",
      "Epoch: 11/20; Batch:138/468; Training loss:0.0943363\n",
      "Epoch: 11/20; Batch:139/468; Training loss:0.100642\n",
      "Epoch: 11/20; Batch:140/468; Training loss:0.101698\n",
      "Epoch: 11/20; Batch:141/468; Training loss:0.100743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20; Batch:142/468; Training loss:0.095883\n",
      "Epoch: 11/20; Batch:143/468; Training loss:0.102316\n",
      "Epoch: 11/20; Batch:144/468; Training loss:0.096868\n",
      "Epoch: 11/20; Batch:145/468; Training loss:0.0981409\n",
      "Epoch: 11/20; Batch:146/468; Training loss:0.0997695\n",
      "Epoch: 11/20; Batch:147/468; Training loss:0.100896\n",
      "Epoch: 11/20; Batch:148/468; Training loss:0.0995677\n",
      "Epoch: 11/20; Batch:149/468; Training loss:0.0993206\n",
      "Epoch: 11/20; Batch:150/468; Training loss:0.0993329\n",
      "Epoch: 11/20; Batch:151/468; Training loss:0.0975086\n",
      "Epoch: 11/20; Batch:152/468; Training loss:0.0979381\n",
      "Epoch: 11/20; Batch:153/468; Training loss:0.0953817\n",
      "Epoch: 11/20; Batch:154/468; Training loss:0.0999849\n",
      "Epoch: 11/20; Batch:155/468; Training loss:0.0967866\n",
      "Epoch: 11/20; Batch:156/468; Training loss:0.0972335\n",
      "Epoch: 11/20; Batch:157/468; Training loss:0.104988\n",
      "Epoch: 11/20; Batch:158/468; Training loss:0.0975333\n",
      "Epoch: 11/20; Batch:159/468; Training loss:0.0986526\n",
      "Epoch: 11/20; Batch:160/468; Training loss:0.0978713\n",
      "Epoch: 11/20; Batch:161/468; Training loss:0.100669\n",
      "Epoch: 11/20; Batch:162/468; Training loss:0.0946354\n",
      "Epoch: 11/20; Batch:163/468; Training loss:0.100314\n",
      "Epoch: 11/20; Batch:164/468; Training loss:0.0999331\n",
      "Epoch: 11/20; Batch:165/468; Training loss:0.0997159\n",
      "Epoch: 11/20; Batch:166/468; Training loss:0.0973201\n",
      "Epoch: 11/20; Batch:167/468; Training loss:0.0991191\n",
      "Epoch: 11/20; Batch:168/468; Training loss:0.100556\n",
      "Epoch: 11/20; Batch:169/468; Training loss:0.0979233\n",
      "Epoch: 11/20; Batch:170/468; Training loss:0.10079\n",
      "Epoch: 11/20; Batch:171/468; Training loss:0.0994123\n",
      "Epoch: 11/20; Batch:172/468; Training loss:0.0991972\n",
      "Epoch: 11/20; Batch:173/468; Training loss:0.0978715\n",
      "Epoch: 11/20; Batch:174/468; Training loss:0.0988584\n",
      "Epoch: 11/20; Batch:175/468; Training loss:0.0937027\n",
      "Epoch: 11/20; Batch:176/468; Training loss:0.0996592\n",
      "Epoch: 11/20; Batch:177/468; Training loss:0.0969269\n",
      "Epoch: 11/20; Batch:178/468; Training loss:0.0945911\n",
      "Epoch: 11/20; Batch:179/468; Training loss:0.0981617\n",
      "Epoch: 11/20; Batch:180/468; Training loss:0.0951977\n",
      "Epoch: 11/20; Batch:181/468; Training loss:0.0962635\n",
      "Epoch: 11/20; Batch:182/468; Training loss:0.0991299\n",
      "Epoch: 11/20; Batch:183/468; Training loss:0.0955808\n",
      "Epoch: 11/20; Batch:184/468; Training loss:0.0930213\n",
      "Epoch: 11/20; Batch:185/468; Training loss:0.0977737\n",
      "Epoch: 11/20; Batch:186/468; Training loss:0.0991029\n",
      "Epoch: 11/20; Batch:187/468; Training loss:0.0988605\n",
      "Epoch: 11/20; Batch:188/468; Training loss:0.100342\n",
      "Epoch: 11/20; Batch:189/468; Training loss:0.0985525\n",
      "Epoch: 11/20; Batch:190/468; Training loss:0.0974422\n",
      "Epoch: 11/20; Batch:191/468; Training loss:0.101731\n",
      "Epoch: 11/20; Batch:192/468; Training loss:0.100744\n",
      "Epoch: 11/20; Batch:193/468; Training loss:0.0994945\n",
      "Epoch: 11/20; Batch:194/468; Training loss:0.0970945\n",
      "Epoch: 11/20; Batch:195/468; Training loss:0.0993151\n",
      "Epoch: 11/20; Batch:196/468; Training loss:0.10139\n",
      "Epoch: 11/20; Batch:197/468; Training loss:0.0971092\n",
      "Epoch: 11/20; Batch:198/468; Training loss:0.0986761\n",
      "Epoch: 11/20; Batch:199/468; Training loss:0.104281\n",
      "Epoch: 11/20; Batch:200/468; Training loss:0.102849\n",
      "Epoch: 11/20; Batch:201/468; Training loss:0.0969631\n",
      "Epoch: 11/20; Batch:202/468; Training loss:0.102513\n",
      "Epoch: 11/20; Batch:203/468; Training loss:0.0968178\n",
      "Epoch: 11/20; Batch:204/468; Training loss:0.0959514\n",
      "Epoch: 11/20; Batch:205/468; Training loss:0.101106\n",
      "Epoch: 11/20; Batch:206/468; Training loss:0.0934292\n",
      "Epoch: 11/20; Batch:207/468; Training loss:0.100374\n",
      "Epoch: 11/20; Batch:208/468; Training loss:0.0971584\n",
      "Epoch: 11/20; Batch:209/468; Training loss:0.0995744\n",
      "Epoch: 11/20; Batch:210/468; Training loss:0.0974438\n",
      "Epoch: 11/20; Batch:211/468; Training loss:0.0989323\n",
      "Epoch: 11/20; Batch:212/468; Training loss:0.0960955\n",
      "Epoch: 11/20; Batch:213/468; Training loss:0.0990458\n",
      "Epoch: 11/20; Batch:214/468; Training loss:0.0937785\n",
      "Epoch: 11/20; Batch:215/468; Training loss:0.0976531\n",
      "Epoch: 11/20; Batch:216/468; Training loss:0.0962495\n",
      "Epoch: 11/20; Batch:217/468; Training loss:0.0960347\n",
      "Epoch: 11/20; Batch:218/468; Training loss:0.0973383\n",
      "Epoch: 11/20; Batch:219/468; Training loss:0.0986073\n",
      "Epoch: 11/20; Batch:220/468; Training loss:0.0940218\n",
      "Epoch: 11/20; Batch:221/468; Training loss:0.102933\n",
      "Epoch: 11/20; Batch:222/468; Training loss:0.0998897\n",
      "Epoch: 11/20; Batch:223/468; Training loss:0.0992201\n",
      "Epoch: 11/20; Batch:224/468; Training loss:0.0990494\n",
      "Epoch: 11/20; Batch:225/468; Training loss:0.0972458\n",
      "Epoch: 11/20; Batch:226/468; Training loss:0.0999724\n",
      "Epoch: 11/20; Batch:227/468; Training loss:0.097546\n",
      "Epoch: 11/20; Batch:228/468; Training loss:0.0961647\n",
      "Epoch: 11/20; Batch:229/468; Training loss:0.0969102\n",
      "Epoch: 11/20; Batch:230/468; Training loss:0.0973951\n",
      "Epoch: 11/20; Batch:231/468; Training loss:0.0964377\n",
      "Epoch: 11/20; Batch:232/468; Training loss:0.100883\n",
      "Epoch: 11/20; Batch:233/468; Training loss:0.0939581\n",
      "Epoch: 11/20; Batch:234/468; Training loss:0.0987556\n",
      "Epoch: 11/20; Batch:235/468; Training loss:0.0983268\n",
      "Epoch: 11/20; Batch:236/468; Training loss:0.0992693\n",
      "Epoch: 11/20; Batch:237/468; Training loss:0.10227\n",
      "Epoch: 11/20; Batch:238/468; Training loss:0.104198\n",
      "Epoch: 11/20; Batch:239/468; Training loss:0.0997154\n",
      "Epoch: 11/20; Batch:240/468; Training loss:0.100753\n",
      "Epoch: 11/20; Batch:241/468; Training loss:0.0930793\n",
      "Epoch: 11/20; Batch:242/468; Training loss:0.101589\n",
      "Epoch: 11/20; Batch:243/468; Training loss:0.0913762\n",
      "Epoch: 11/20; Batch:244/468; Training loss:0.0955665\n",
      "Epoch: 11/20; Batch:245/468; Training loss:0.0997019\n",
      "Epoch: 11/20; Batch:246/468; Training loss:0.102158\n",
      "Epoch: 11/20; Batch:247/468; Training loss:0.0973077\n",
      "Epoch: 11/20; Batch:248/468; Training loss:0.0971626\n",
      "Epoch: 11/20; Batch:249/468; Training loss:0.099484\n",
      "Epoch: 11/20; Batch:250/468; Training loss:0.0995687\n",
      "Epoch: 11/20; Batch:251/468; Training loss:0.0993074\n",
      "Epoch: 11/20; Batch:252/468; Training loss:0.0984943\n",
      "Epoch: 11/20; Batch:253/468; Training loss:0.0992065\n",
      "Epoch: 11/20; Batch:254/468; Training loss:0.10022\n",
      "Epoch: 11/20; Batch:255/468; Training loss:0.0947108\n",
      "Epoch: 11/20; Batch:256/468; Training loss:0.0979782\n",
      "Epoch: 11/20; Batch:257/468; Training loss:0.101351\n",
      "Epoch: 11/20; Batch:258/468; Training loss:0.0946697\n",
      "Epoch: 11/20; Batch:259/468; Training loss:0.0996473\n",
      "Epoch: 11/20; Batch:260/468; Training loss:0.0957319\n",
      "Epoch: 11/20; Batch:261/468; Training loss:0.0958562\n",
      "Epoch: 11/20; Batch:262/468; Training loss:0.0983622\n",
      "Epoch: 11/20; Batch:263/468; Training loss:0.102645\n",
      "Epoch: 11/20; Batch:264/468; Training loss:0.0986708\n",
      "Epoch: 11/20; Batch:265/468; Training loss:0.100409\n",
      "Epoch: 11/20; Batch:266/468; Training loss:0.0951664\n",
      "Epoch: 11/20; Batch:267/468; Training loss:0.101688\n",
      "Epoch: 11/20; Batch:268/468; Training loss:0.0999021\n",
      "Epoch: 11/20; Batch:269/468; Training loss:0.0980434\n",
      "Epoch: 11/20; Batch:270/468; Training loss:0.0987411\n",
      "Epoch: 11/20; Batch:271/468; Training loss:0.102317\n",
      "Epoch: 11/20; Batch:272/468; Training loss:0.0982204\n",
      "Epoch: 11/20; Batch:273/468; Training loss:0.102258\n",
      "Epoch: 11/20; Batch:274/468; Training loss:0.0994713\n",
      "Epoch: 11/20; Batch:275/468; Training loss:0.100578\n",
      "Epoch: 11/20; Batch:276/468; Training loss:0.0983178\n",
      "Epoch: 11/20; Batch:277/468; Training loss:0.100677\n",
      "Epoch: 11/20; Batch:278/468; Training loss:0.100729\n",
      "Epoch: 11/20; Batch:279/468; Training loss:0.0962306\n",
      "Epoch: 11/20; Batch:280/468; Training loss:0.0998458\n",
      "Epoch: 11/20; Batch:281/468; Training loss:0.0989548\n",
      "Epoch: 11/20; Batch:282/468; Training loss:0.102341\n",
      "Epoch: 11/20; Batch:283/468; Training loss:0.0998154\n",
      "Epoch: 11/20; Batch:284/468; Training loss:0.101034\n",
      "Epoch: 11/20; Batch:285/468; Training loss:0.101418\n",
      "Epoch: 11/20; Batch:286/468; Training loss:0.100577\n",
      "Epoch: 11/20; Batch:287/468; Training loss:0.0971402\n",
      "Epoch: 11/20; Batch:288/468; Training loss:0.100625\n",
      "Epoch: 11/20; Batch:289/468; Training loss:0.0953045\n",
      "Epoch: 11/20; Batch:290/468; Training loss:0.101158\n",
      "Epoch: 11/20; Batch:291/468; Training loss:0.101273\n",
      "Epoch: 11/20; Batch:292/468; Training loss:0.100837\n",
      "Epoch: 11/20; Batch:293/468; Training loss:0.101912\n",
      "Epoch: 11/20; Batch:294/468; Training loss:0.0993502\n",
      "Epoch: 11/20; Batch:295/468; Training loss:0.099851\n",
      "Epoch: 11/20; Batch:296/468; Training loss:0.101355\n",
      "Epoch: 11/20; Batch:297/468; Training loss:0.0981506\n",
      "Epoch: 11/20; Batch:298/468; Training loss:0.102074\n",
      "Epoch: 11/20; Batch:299/468; Training loss:0.101436\n",
      "Epoch: 11/20; Batch:300/468; Training loss:0.0948975\n",
      "Epoch: 11/20; Batch:301/468; Training loss:0.0974032\n",
      "Epoch: 11/20; Batch:302/468; Training loss:0.10029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20; Batch:303/468; Training loss:0.100042\n",
      "Epoch: 11/20; Batch:304/468; Training loss:0.0977769\n",
      "Epoch: 11/20; Batch:305/468; Training loss:0.0932051\n",
      "Epoch: 11/20; Batch:306/468; Training loss:0.0984278\n",
      "Epoch: 11/20; Batch:307/468; Training loss:0.0971429\n",
      "Epoch: 11/20; Batch:308/468; Training loss:0.100185\n",
      "Epoch: 11/20; Batch:309/468; Training loss:0.100372\n",
      "Epoch: 11/20; Batch:310/468; Training loss:0.0962674\n",
      "Epoch: 11/20; Batch:311/468; Training loss:0.0947002\n",
      "Epoch: 11/20; Batch:312/468; Training loss:0.0973027\n",
      "Epoch: 11/20; Batch:313/468; Training loss:0.0945948\n",
      "Epoch: 11/20; Batch:314/468; Training loss:0.102019\n",
      "Epoch: 11/20; Batch:315/468; Training loss:0.0995577\n",
      "Epoch: 11/20; Batch:316/468; Training loss:0.0960475\n",
      "Epoch: 11/20; Batch:317/468; Training loss:0.0998751\n",
      "Epoch: 11/20; Batch:318/468; Training loss:0.101521\n",
      "Epoch: 11/20; Batch:319/468; Training loss:0.102407\n",
      "Epoch: 11/20; Batch:320/468; Training loss:0.0999072\n",
      "Epoch: 11/20; Batch:321/468; Training loss:0.0977387\n",
      "Epoch: 11/20; Batch:322/468; Training loss:0.0976443\n",
      "Epoch: 11/20; Batch:323/468; Training loss:0.096399\n",
      "Epoch: 11/20; Batch:324/468; Training loss:0.100395\n",
      "Epoch: 11/20; Batch:325/468; Training loss:0.0955321\n",
      "Epoch: 11/20; Batch:326/468; Training loss:0.0977156\n",
      "Epoch: 11/20; Batch:327/468; Training loss:0.101891\n",
      "Epoch: 11/20; Batch:328/468; Training loss:0.0999622\n",
      "Epoch: 11/20; Batch:329/468; Training loss:0.099264\n",
      "Epoch: 11/20; Batch:330/468; Training loss:0.0967033\n",
      "Epoch: 11/20; Batch:331/468; Training loss:0.0935688\n",
      "Epoch: 11/20; Batch:332/468; Training loss:0.096508\n",
      "Epoch: 11/20; Batch:333/468; Training loss:0.101991\n",
      "Epoch: 11/20; Batch:334/468; Training loss:0.10231\n",
      "Epoch: 11/20; Batch:335/468; Training loss:0.102246\n",
      "Epoch: 11/20; Batch:336/468; Training loss:0.0949172\n",
      "Epoch: 11/20; Batch:337/468; Training loss:0.0989732\n",
      "Epoch: 11/20; Batch:338/468; Training loss:0.0982493\n",
      "Epoch: 11/20; Batch:339/468; Training loss:0.0955202\n",
      "Epoch: 11/20; Batch:340/468; Training loss:0.0975173\n",
      "Epoch: 11/20; Batch:341/468; Training loss:0.0957575\n",
      "Epoch: 11/20; Batch:342/468; Training loss:0.0969402\n",
      "Epoch: 11/20; Batch:343/468; Training loss:0.0980444\n",
      "Epoch: 11/20; Batch:344/468; Training loss:0.0956645\n",
      "Epoch: 11/20; Batch:345/468; Training loss:0.0984535\n",
      "Epoch: 11/20; Batch:346/468; Training loss:0.0948999\n",
      "Epoch: 11/20; Batch:347/468; Training loss:0.0993712\n",
      "Epoch: 11/20; Batch:348/468; Training loss:0.0988624\n",
      "Epoch: 11/20; Batch:349/468; Training loss:0.0986536\n",
      "Epoch: 11/20; Batch:350/468; Training loss:0.0977981\n",
      "Epoch: 11/20; Batch:351/468; Training loss:0.100021\n",
      "Epoch: 11/20; Batch:352/468; Training loss:0.101384\n",
      "Epoch: 11/20; Batch:353/468; Training loss:0.100504\n",
      "Epoch: 11/20; Batch:354/468; Training loss:0.0976754\n",
      "Epoch: 11/20; Batch:355/468; Training loss:0.101001\n",
      "Epoch: 11/20; Batch:356/468; Training loss:0.0995292\n",
      "Epoch: 11/20; Batch:357/468; Training loss:0.100596\n",
      "Epoch: 11/20; Batch:358/468; Training loss:0.0985313\n",
      "Epoch: 11/20; Batch:359/468; Training loss:0.0993931\n",
      "Epoch: 11/20; Batch:360/468; Training loss:0.0981269\n",
      "Epoch: 11/20; Batch:361/468; Training loss:0.0972676\n",
      "Epoch: 11/20; Batch:362/468; Training loss:0.103206\n",
      "Epoch: 11/20; Batch:363/468; Training loss:0.0992562\n",
      "Epoch: 11/20; Batch:364/468; Training loss:0.0957667\n",
      "Epoch: 11/20; Batch:365/468; Training loss:0.10132\n",
      "Epoch: 11/20; Batch:366/468; Training loss:0.0955567\n",
      "Epoch: 11/20; Batch:367/468; Training loss:0.101079\n",
      "Epoch: 11/20; Batch:368/468; Training loss:0.0963545\n",
      "Epoch: 11/20; Batch:369/468; Training loss:0.0980033\n",
      "Epoch: 11/20; Batch:370/468; Training loss:0.101064\n",
      "Epoch: 11/20; Batch:371/468; Training loss:0.0965293\n",
      "Epoch: 11/20; Batch:372/468; Training loss:0.100982\n",
      "Epoch: 11/20; Batch:373/468; Training loss:0.0995152\n",
      "Epoch: 11/20; Batch:374/468; Training loss:0.0974794\n",
      "Epoch: 11/20; Batch:375/468; Training loss:0.0964562\n",
      "Epoch: 11/20; Batch:376/468; Training loss:0.0998923\n",
      "Epoch: 11/20; Batch:377/468; Training loss:0.100459\n",
      "Epoch: 11/20; Batch:378/468; Training loss:0.101156\n",
      "Epoch: 11/20; Batch:379/468; Training loss:0.0961089\n",
      "Epoch: 11/20; Batch:380/468; Training loss:0.10122\n",
      "Epoch: 11/20; Batch:381/468; Training loss:0.0976244\n",
      "Epoch: 11/20; Batch:382/468; Training loss:0.0952743\n",
      "Epoch: 11/20; Batch:383/468; Training loss:0.103196\n",
      "Epoch: 11/20; Batch:384/468; Training loss:0.102797\n",
      "Epoch: 11/20; Batch:385/468; Training loss:0.0941982\n",
      "Epoch: 11/20; Batch:386/468; Training loss:0.101538\n",
      "Epoch: 11/20; Batch:387/468; Training loss:0.0994782\n",
      "Epoch: 11/20; Batch:388/468; Training loss:0.094773\n",
      "Epoch: 11/20; Batch:389/468; Training loss:0.101313\n",
      "Epoch: 11/20; Batch:390/468; Training loss:0.0949758\n",
      "Epoch: 11/20; Batch:391/468; Training loss:0.0970208\n",
      "Epoch: 11/20; Batch:392/468; Training loss:0.097925\n",
      "Epoch: 11/20; Batch:393/468; Training loss:0.0974141\n",
      "Epoch: 11/20; Batch:394/468; Training loss:0.100869\n",
      "Epoch: 11/20; Batch:395/468; Training loss:0.098556\n",
      "Epoch: 11/20; Batch:396/468; Training loss:0.094825\n",
      "Epoch: 11/20; Batch:397/468; Training loss:0.0980032\n",
      "Epoch: 11/20; Batch:398/468; Training loss:0.0991666\n",
      "Epoch: 11/20; Batch:399/468; Training loss:0.097779\n",
      "Epoch: 11/20; Batch:400/468; Training loss:0.0958238\n",
      "Epoch: 11/20; Batch:401/468; Training loss:0.0964216\n",
      "Epoch: 11/20; Batch:402/468; Training loss:0.101378\n",
      "Epoch: 11/20; Batch:403/468; Training loss:0.0989751\n",
      "Epoch: 11/20; Batch:404/468; Training loss:0.0975394\n",
      "Epoch: 11/20; Batch:405/468; Training loss:0.0997803\n",
      "Epoch: 11/20; Batch:406/468; Training loss:0.0992778\n",
      "Epoch: 11/20; Batch:407/468; Training loss:0.0999487\n",
      "Epoch: 11/20; Batch:408/468; Training loss:0.0962349\n",
      "Epoch: 11/20; Batch:409/468; Training loss:0.0986003\n",
      "Epoch: 11/20; Batch:410/468; Training loss:0.0981626\n",
      "Epoch: 11/20; Batch:411/468; Training loss:0.0986839\n",
      "Epoch: 11/20; Batch:412/468; Training loss:0.100204\n",
      "Epoch: 11/20; Batch:413/468; Training loss:0.0992315\n",
      "Epoch: 11/20; Batch:414/468; Training loss:0.0987318\n",
      "Epoch: 11/20; Batch:415/468; Training loss:0.0998634\n",
      "Epoch: 11/20; Batch:416/468; Training loss:0.100426\n",
      "Epoch: 11/20; Batch:417/468; Training loss:0.0995923\n",
      "Epoch: 11/20; Batch:418/468; Training loss:0.0971273\n",
      "Epoch: 11/20; Batch:419/468; Training loss:0.0978986\n",
      "Epoch: 11/20; Batch:420/468; Training loss:0.0998873\n",
      "Epoch: 11/20; Batch:421/468; Training loss:0.0948803\n",
      "Epoch: 11/20; Batch:422/468; Training loss:0.0964287\n",
      "Epoch: 11/20; Batch:423/468; Training loss:0.0988555\n",
      "Epoch: 11/20; Batch:424/468; Training loss:0.10041\n",
      "Epoch: 11/20; Batch:425/468; Training loss:0.0992541\n",
      "Epoch: 11/20; Batch:426/468; Training loss:0.0980634\n",
      "Epoch: 11/20; Batch:427/468; Training loss:0.0943536\n",
      "Epoch: 11/20; Batch:428/468; Training loss:0.100916\n",
      "Epoch: 11/20; Batch:429/468; Training loss:0.0975888\n",
      "Epoch: 11/20; Batch:430/468; Training loss:0.0975466\n",
      "Epoch: 11/20; Batch:431/468; Training loss:0.098407\n",
      "Epoch: 11/20; Batch:432/468; Training loss:0.0912729\n",
      "Epoch: 11/20; Batch:433/468; Training loss:0.0988254\n",
      "Epoch: 11/20; Batch:434/468; Training loss:0.0979903\n",
      "Epoch: 11/20; Batch:435/468; Training loss:0.0969015\n",
      "Epoch: 11/20; Batch:436/468; Training loss:0.0995566\n",
      "Epoch: 11/20; Batch:437/468; Training loss:0.0946527\n",
      "Epoch: 11/20; Batch:438/468; Training loss:0.0968754\n",
      "Epoch: 11/20; Batch:439/468; Training loss:0.0934318\n",
      "Epoch: 11/20; Batch:440/468; Training loss:0.0960727\n",
      "Epoch: 11/20; Batch:441/468; Training loss:0.0963906\n",
      "Epoch: 11/20; Batch:442/468; Training loss:0.101524\n",
      "Epoch: 11/20; Batch:443/468; Training loss:0.098895\n",
      "Epoch: 11/20; Batch:444/468; Training loss:0.094074\n",
      "Epoch: 11/20; Batch:445/468; Training loss:0.0960587\n",
      "Epoch: 11/20; Batch:446/468; Training loss:0.100245\n",
      "Epoch: 11/20; Batch:447/468; Training loss:0.09994\n",
      "Epoch: 11/20; Batch:448/468; Training loss:0.0995338\n",
      "Epoch: 11/20; Batch:449/468; Training loss:0.0994928\n",
      "Epoch: 11/20; Batch:450/468; Training loss:0.0963754\n",
      "Epoch: 11/20; Batch:451/468; Training loss:0.0936036\n",
      "Epoch: 11/20; Batch:452/468; Training loss:0.0958338\n",
      "Epoch: 11/20; Batch:453/468; Training loss:0.0995935\n",
      "Epoch: 11/20; Batch:454/468; Training loss:0.0971003\n",
      "Epoch: 11/20; Batch:455/468; Training loss:0.0987256\n",
      "Epoch: 11/20; Batch:456/468; Training loss:0.100077\n",
      "Epoch: 11/20; Batch:457/468; Training loss:0.0989388\n",
      "Epoch: 11/20; Batch:458/468; Training loss:0.0962515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20; Batch:459/468; Training loss:0.0995066\n",
      "Epoch: 11/20; Batch:460/468; Training loss:0.0970455\n",
      "Epoch: 11/20; Batch:461/468; Training loss:0.0949114\n",
      "Epoch: 11/20; Batch:462/468; Training loss:0.0986629\n",
      "Epoch: 11/20; Batch:463/468; Training loss:0.0956887\n",
      "Epoch: 11/20; Batch:464/468; Training loss:0.0994243\n",
      "Epoch: 11/20; Batch:465/468; Training loss:0.0985051\n",
      "Epoch: 11/20; Batch:466/468; Training loss:0.0996421\n",
      "Epoch: 11/20; Batch:467/468; Training loss:0.0988296\n",
      "Epoch: 11/20; Batch:468/468; Training loss:0.100521\n",
      "Epoch: 12/20; Batch:1/468; Training loss:0.096579\n",
      "Epoch: 12/20; Batch:2/468; Training loss:0.0942908\n",
      "Epoch: 12/20; Batch:3/468; Training loss:0.0963289\n",
      "Epoch: 12/20; Batch:4/468; Training loss:0.09474\n",
      "Epoch: 12/20; Batch:5/468; Training loss:0.0987862\n",
      "Epoch: 12/20; Batch:6/468; Training loss:0.0971289\n",
      "Epoch: 12/20; Batch:7/468; Training loss:0.0993678\n",
      "Epoch: 12/20; Batch:8/468; Training loss:0.0979119\n",
      "Epoch: 12/20; Batch:9/468; Training loss:0.0998435\n",
      "Epoch: 12/20; Batch:10/468; Training loss:0.100243\n",
      "Epoch: 12/20; Batch:11/468; Training loss:0.103353\n",
      "Epoch: 12/20; Batch:12/468; Training loss:0.099844\n",
      "Epoch: 12/20; Batch:13/468; Training loss:0.0980718\n",
      "Epoch: 12/20; Batch:14/468; Training loss:0.0996491\n",
      "Epoch: 12/20; Batch:15/468; Training loss:0.0971986\n",
      "Epoch: 12/20; Batch:16/468; Training loss:0.0962158\n",
      "Epoch: 12/20; Batch:17/468; Training loss:0.0998891\n",
      "Epoch: 12/20; Batch:18/468; Training loss:0.0955965\n",
      "Epoch: 12/20; Batch:19/468; Training loss:0.0981784\n",
      "Epoch: 12/20; Batch:20/468; Training loss:0.0966349\n",
      "Epoch: 12/20; Batch:21/468; Training loss:0.0967547\n",
      "Epoch: 12/20; Batch:22/468; Training loss:0.0985387\n",
      "Epoch: 12/20; Batch:23/468; Training loss:0.0990849\n",
      "Epoch: 12/20; Batch:24/468; Training loss:0.0976809\n",
      "Epoch: 12/20; Batch:25/468; Training loss:0.0959367\n",
      "Epoch: 12/20; Batch:26/468; Training loss:0.0987071\n",
      "Epoch: 12/20; Batch:27/468; Training loss:0.102821\n",
      "Epoch: 12/20; Batch:28/468; Training loss:0.103787\n",
      "Epoch: 12/20; Batch:29/468; Training loss:0.0980448\n",
      "Epoch: 12/20; Batch:30/468; Training loss:0.100354\n",
      "Epoch: 12/20; Batch:31/468; Training loss:0.10211\n",
      "Epoch: 12/20; Batch:32/468; Training loss:0.0965259\n",
      "Epoch: 12/20; Batch:33/468; Training loss:0.0988857\n",
      "Epoch: 12/20; Batch:34/468; Training loss:0.0975254\n",
      "Epoch: 12/20; Batch:35/468; Training loss:0.10213\n",
      "Epoch: 12/20; Batch:36/468; Training loss:0.0982603\n",
      "Epoch: 12/20; Batch:37/468; Training loss:0.0989681\n",
      "Epoch: 12/20; Batch:38/468; Training loss:0.0997648\n",
      "Epoch: 12/20; Batch:39/468; Training loss:0.0983741\n",
      "Epoch: 12/20; Batch:40/468; Training loss:0.0994892\n",
      "Epoch: 12/20; Batch:41/468; Training loss:0.10137\n",
      "Epoch: 12/20; Batch:42/468; Training loss:0.0961219\n",
      "Epoch: 12/20; Batch:43/468; Training loss:0.0987191\n",
      "Epoch: 12/20; Batch:44/468; Training loss:0.0943998\n",
      "Epoch: 12/20; Batch:45/468; Training loss:0.101378\n",
      "Epoch: 12/20; Batch:46/468; Training loss:0.0969172\n",
      "Epoch: 12/20; Batch:47/468; Training loss:0.0972384\n",
      "Epoch: 12/20; Batch:48/468; Training loss:0.0978751\n",
      "Epoch: 12/20; Batch:49/468; Training loss:0.0960026\n",
      "Epoch: 12/20; Batch:50/468; Training loss:0.101273\n",
      "Epoch: 12/20; Batch:51/468; Training loss:0.100785\n",
      "Epoch: 12/20; Batch:52/468; Training loss:0.09864\n",
      "Epoch: 12/20; Batch:53/468; Training loss:0.0981619\n",
      "Epoch: 12/20; Batch:54/468; Training loss:0.0989173\n",
      "Epoch: 12/20; Batch:55/468; Training loss:0.101839\n",
      "Epoch: 12/20; Batch:56/468; Training loss:0.094674\n",
      "Epoch: 12/20; Batch:57/468; Training loss:0.10094\n",
      "Epoch: 12/20; Batch:58/468; Training loss:0.0952991\n",
      "Epoch: 12/20; Batch:59/468; Training loss:0.09732\n",
      "Epoch: 12/20; Batch:60/468; Training loss:0.0997152\n",
      "Epoch: 12/20; Batch:61/468; Training loss:0.0996429\n",
      "Epoch: 12/20; Batch:62/468; Training loss:0.0956732\n",
      "Epoch: 12/20; Batch:63/468; Training loss:0.101741\n",
      "Epoch: 12/20; Batch:64/468; Training loss:0.102371\n",
      "Epoch: 12/20; Batch:65/468; Training loss:0.0920928\n",
      "Epoch: 12/20; Batch:66/468; Training loss:0.102092\n",
      "Epoch: 12/20; Batch:67/468; Training loss:0.0963508\n",
      "Epoch: 12/20; Batch:68/468; Training loss:0.0985278\n",
      "Epoch: 12/20; Batch:69/468; Training loss:0.0965262\n",
      "Epoch: 12/20; Batch:70/468; Training loss:0.101447\n",
      "Epoch: 12/20; Batch:71/468; Training loss:0.096765\n",
      "Epoch: 12/20; Batch:72/468; Training loss:0.0966389\n",
      "Epoch: 12/20; Batch:73/468; Training loss:0.0991814\n",
      "Epoch: 12/20; Batch:74/468; Training loss:0.0958993\n",
      "Epoch: 12/20; Batch:75/468; Training loss:0.0976193\n",
      "Epoch: 12/20; Batch:76/468; Training loss:0.100134\n",
      "Epoch: 12/20; Batch:77/468; Training loss:0.0992469\n",
      "Epoch: 12/20; Batch:78/468; Training loss:0.0984863\n",
      "Epoch: 12/20; Batch:79/468; Training loss:0.0973413\n",
      "Epoch: 12/20; Batch:80/468; Training loss:0.0980684\n",
      "Epoch: 12/20; Batch:81/468; Training loss:0.0976759\n",
      "Epoch: 12/20; Batch:82/468; Training loss:0.0996076\n",
      "Epoch: 12/20; Batch:83/468; Training loss:0.0984123\n",
      "Epoch: 12/20; Batch:84/468; Training loss:0.100061\n",
      "Epoch: 12/20; Batch:85/468; Training loss:0.0969312\n",
      "Epoch: 12/20; Batch:86/468; Training loss:0.0998296\n",
      "Epoch: 12/20; Batch:87/468; Training loss:0.0905154\n",
      "Epoch: 12/20; Batch:88/468; Training loss:0.0994921\n",
      "Epoch: 12/20; Batch:89/468; Training loss:0.0938561\n",
      "Epoch: 12/20; Batch:90/468; Training loss:0.0973113\n",
      "Epoch: 12/20; Batch:91/468; Training loss:0.0956449\n",
      "Epoch: 12/20; Batch:92/468; Training loss:0.0965817\n",
      "Epoch: 12/20; Batch:93/468; Training loss:0.0977132\n",
      "Epoch: 12/20; Batch:94/468; Training loss:0.0948173\n",
      "Epoch: 12/20; Batch:95/468; Training loss:0.100951\n",
      "Epoch: 12/20; Batch:96/468; Training loss:0.0998713\n",
      "Epoch: 12/20; Batch:97/468; Training loss:0.0922384\n",
      "Epoch: 12/20; Batch:98/468; Training loss:0.0939259\n",
      "Epoch: 12/20; Batch:99/468; Training loss:0.098308\n",
      "Epoch: 12/20; Batch:100/468; Training loss:0.0993364\n",
      "Epoch: 12/20; Batch:101/468; Training loss:0.0949768\n",
      "Epoch: 12/20; Batch:102/468; Training loss:0.0991633\n",
      "Epoch: 12/20; Batch:103/468; Training loss:0.0984309\n",
      "Epoch: 12/20; Batch:104/468; Training loss:0.094016\n",
      "Epoch: 12/20; Batch:105/468; Training loss:0.0971111\n",
      "Epoch: 12/20; Batch:106/468; Training loss:0.095241\n",
      "Epoch: 12/20; Batch:107/468; Training loss:0.100916\n",
      "Epoch: 12/20; Batch:108/468; Training loss:0.0934377\n",
      "Epoch: 12/20; Batch:109/468; Training loss:0.0968375\n",
      "Epoch: 12/20; Batch:110/468; Training loss:0.0965266\n",
      "Epoch: 12/20; Batch:111/468; Training loss:0.0977937\n",
      "Epoch: 12/20; Batch:112/468; Training loss:0.0952182\n",
      "Epoch: 12/20; Batch:113/468; Training loss:0.0934379\n",
      "Epoch: 12/20; Batch:114/468; Training loss:0.098605\n",
      "Epoch: 12/20; Batch:115/468; Training loss:0.101175\n",
      "Epoch: 12/20; Batch:116/468; Training loss:0.0966009\n",
      "Epoch: 12/20; Batch:117/468; Training loss:0.0943116\n",
      "Epoch: 12/20; Batch:118/468; Training loss:0.101475\n",
      "Epoch: 12/20; Batch:119/468; Training loss:0.0962404\n",
      "Epoch: 12/20; Batch:120/468; Training loss:0.099599\n",
      "Epoch: 12/20; Batch:121/468; Training loss:0.0962053\n",
      "Epoch: 12/20; Batch:122/468; Training loss:0.0963763\n",
      "Epoch: 12/20; Batch:123/468; Training loss:0.099166\n",
      "Epoch: 12/20; Batch:124/468; Training loss:0.101335\n",
      "Epoch: 12/20; Batch:125/468; Training loss:0.0965165\n",
      "Epoch: 12/20; Batch:126/468; Training loss:0.0985992\n",
      "Epoch: 12/20; Batch:127/468; Training loss:0.0977511\n",
      "Epoch: 12/20; Batch:128/468; Training loss:0.0975563\n",
      "Epoch: 12/20; Batch:129/468; Training loss:0.0963851\n",
      "Epoch: 12/20; Batch:130/468; Training loss:0.0959189\n",
      "Epoch: 12/20; Batch:131/468; Training loss:0.0948294\n",
      "Epoch: 12/20; Batch:132/468; Training loss:0.0948728\n",
      "Epoch: 12/20; Batch:133/468; Training loss:0.094564\n",
      "Epoch: 12/20; Batch:134/468; Training loss:0.0995865\n",
      "Epoch: 12/20; Batch:135/468; Training loss:0.0991557\n",
      "Epoch: 12/20; Batch:136/468; Training loss:0.099225\n",
      "Epoch: 12/20; Batch:137/468; Training loss:0.0968676\n",
      "Epoch: 12/20; Batch:138/468; Training loss:0.100667\n",
      "Epoch: 12/20; Batch:139/468; Training loss:0.0954528\n",
      "Epoch: 12/20; Batch:140/468; Training loss:0.0994791\n",
      "Epoch: 12/20; Batch:141/468; Training loss:0.102233\n",
      "Epoch: 12/20; Batch:142/468; Training loss:0.0921637\n",
      "Epoch: 12/20; Batch:143/468; Training loss:0.0955236\n",
      "Epoch: 12/20; Batch:144/468; Training loss:0.098396\n",
      "Epoch: 12/20; Batch:145/468; Training loss:0.099036\n",
      "Epoch: 12/20; Batch:146/468; Training loss:0.0954398\n",
      "Epoch: 12/20; Batch:147/468; Training loss:0.0969626\n",
      "Epoch: 12/20; Batch:148/468; Training loss:0.101461\n",
      "Epoch: 12/20; Batch:149/468; Training loss:0.102136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20; Batch:150/468; Training loss:0.0986896\n",
      "Epoch: 12/20; Batch:151/468; Training loss:0.0998921\n",
      "Epoch: 12/20; Batch:152/468; Training loss:0.102141\n",
      "Epoch: 12/20; Batch:153/468; Training loss:0.098478\n",
      "Epoch: 12/20; Batch:154/468; Training loss:0.0987175\n",
      "Epoch: 12/20; Batch:155/468; Training loss:0.0969203\n",
      "Epoch: 12/20; Batch:156/468; Training loss:0.0988583\n",
      "Epoch: 12/20; Batch:157/468; Training loss:0.0992199\n",
      "Epoch: 12/20; Batch:158/468; Training loss:0.098989\n",
      "Epoch: 12/20; Batch:159/468; Training loss:0.101251\n",
      "Epoch: 12/20; Batch:160/468; Training loss:0.100497\n",
      "Epoch: 12/20; Batch:161/468; Training loss:0.101214\n",
      "Epoch: 12/20; Batch:162/468; Training loss:0.0951013\n",
      "Epoch: 12/20; Batch:163/468; Training loss:0.101136\n",
      "Epoch: 12/20; Batch:164/468; Training loss:0.0934351\n",
      "Epoch: 12/20; Batch:165/468; Training loss:0.0996184\n",
      "Epoch: 12/20; Batch:166/468; Training loss:0.0978838\n",
      "Epoch: 12/20; Batch:167/468; Training loss:0.0975539\n",
      "Epoch: 12/20; Batch:168/468; Training loss:0.0973302\n",
      "Epoch: 12/20; Batch:169/468; Training loss:0.0975804\n",
      "Epoch: 12/20; Batch:170/468; Training loss:0.0967315\n",
      "Epoch: 12/20; Batch:171/468; Training loss:0.0965193\n",
      "Epoch: 12/20; Batch:172/468; Training loss:0.0984822\n",
      "Epoch: 12/20; Batch:173/468; Training loss:0.0998853\n",
      "Epoch: 12/20; Batch:174/468; Training loss:0.0979657\n",
      "Epoch: 12/20; Batch:175/468; Training loss:0.0956228\n",
      "Epoch: 12/20; Batch:176/468; Training loss:0.101423\n",
      "Epoch: 12/20; Batch:177/468; Training loss:0.0962117\n",
      "Epoch: 12/20; Batch:178/468; Training loss:0.101404\n",
      "Epoch: 12/20; Batch:179/468; Training loss:0.0966768\n",
      "Epoch: 12/20; Batch:180/468; Training loss:0.0990702\n",
      "Epoch: 12/20; Batch:181/468; Training loss:0.09897\n",
      "Epoch: 12/20; Batch:182/468; Training loss:0.0957465\n",
      "Epoch: 12/20; Batch:183/468; Training loss:0.102158\n",
      "Epoch: 12/20; Batch:184/468; Training loss:0.0980682\n",
      "Epoch: 12/20; Batch:185/468; Training loss:0.0991285\n",
      "Epoch: 12/20; Batch:186/468; Training loss:0.0983147\n",
      "Epoch: 12/20; Batch:187/468; Training loss:0.0983212\n",
      "Epoch: 12/20; Batch:188/468; Training loss:0.0983963\n",
      "Epoch: 12/20; Batch:189/468; Training loss:0.0960406\n",
      "Epoch: 12/20; Batch:190/468; Training loss:0.0974359\n",
      "Epoch: 12/20; Batch:191/468; Training loss:0.0999702\n",
      "Epoch: 12/20; Batch:192/468; Training loss:0.099759\n",
      "Epoch: 12/20; Batch:193/468; Training loss:0.097976\n",
      "Epoch: 12/20; Batch:194/468; Training loss:0.0992341\n",
      "Epoch: 12/20; Batch:195/468; Training loss:0.0992518\n",
      "Epoch: 12/20; Batch:196/468; Training loss:0.0913782\n",
      "Epoch: 12/20; Batch:197/468; Training loss:0.0972381\n",
      "Epoch: 12/20; Batch:198/468; Training loss:0.0990964\n",
      "Epoch: 12/20; Batch:199/468; Training loss:0.101243\n",
      "Epoch: 12/20; Batch:200/468; Training loss:0.0978114\n",
      "Epoch: 12/20; Batch:201/468; Training loss:0.0990479\n",
      "Epoch: 12/20; Batch:202/468; Training loss:0.0998142\n",
      "Epoch: 12/20; Batch:203/468; Training loss:0.101103\n",
      "Epoch: 12/20; Batch:204/468; Training loss:0.0972083\n",
      "Epoch: 12/20; Batch:205/468; Training loss:0.0966101\n",
      "Epoch: 12/20; Batch:206/468; Training loss:0.0975297\n",
      "Epoch: 12/20; Batch:207/468; Training loss:0.0980633\n",
      "Epoch: 12/20; Batch:208/468; Training loss:0.0958202\n",
      "Epoch: 12/20; Batch:209/468; Training loss:0.0969705\n",
      "Epoch: 12/20; Batch:210/468; Training loss:0.09924\n",
      "Epoch: 12/20; Batch:211/468; Training loss:0.0974387\n",
      "Epoch: 12/20; Batch:212/468; Training loss:0.0962208\n",
      "Epoch: 12/20; Batch:213/468; Training loss:0.0978265\n",
      "Epoch: 12/20; Batch:214/468; Training loss:0.0957174\n",
      "Epoch: 12/20; Batch:215/468; Training loss:0.0976813\n",
      "Epoch: 12/20; Batch:216/468; Training loss:0.0992524\n",
      "Epoch: 12/20; Batch:217/468; Training loss:0.097674\n",
      "Epoch: 12/20; Batch:218/468; Training loss:0.0944209\n",
      "Epoch: 12/20; Batch:219/468; Training loss:0.0967075\n",
      "Epoch: 12/20; Batch:220/468; Training loss:0.0999868\n",
      "Epoch: 12/20; Batch:221/468; Training loss:0.0970556\n",
      "Epoch: 12/20; Batch:222/468; Training loss:0.0958421\n",
      "Epoch: 12/20; Batch:223/468; Training loss:0.0944517\n",
      "Epoch: 12/20; Batch:224/468; Training loss:0.0984184\n",
      "Epoch: 12/20; Batch:225/468; Training loss:0.0961675\n",
      "Epoch: 12/20; Batch:226/468; Training loss:0.0971537\n",
      "Epoch: 12/20; Batch:227/468; Training loss:0.0987376\n",
      "Epoch: 12/20; Batch:228/468; Training loss:0.0989375\n",
      "Epoch: 12/20; Batch:229/468; Training loss:0.0983581\n",
      "Epoch: 12/20; Batch:230/468; Training loss:0.101929\n",
      "Epoch: 12/20; Batch:231/468; Training loss:0.0963652\n",
      "Epoch: 12/20; Batch:232/468; Training loss:0.100012\n",
      "Epoch: 12/20; Batch:233/468; Training loss:0.0950568\n",
      "Epoch: 12/20; Batch:234/468; Training loss:0.0988033\n",
      "Epoch: 12/20; Batch:235/468; Training loss:0.0986559\n",
      "Epoch: 12/20; Batch:236/468; Training loss:0.0957238\n",
      "Epoch: 12/20; Batch:237/468; Training loss:0.0947811\n",
      "Epoch: 12/20; Batch:238/468; Training loss:0.101068\n",
      "Epoch: 12/20; Batch:239/468; Training loss:0.100682\n",
      "Epoch: 12/20; Batch:240/468; Training loss:0.0980664\n",
      "Epoch: 12/20; Batch:241/468; Training loss:0.0996659\n",
      "Epoch: 12/20; Batch:242/468; Training loss:0.0987564\n",
      "Epoch: 12/20; Batch:243/468; Training loss:0.0954155\n",
      "Epoch: 12/20; Batch:244/468; Training loss:0.0987907\n",
      "Epoch: 12/20; Batch:245/468; Training loss:0.0966372\n",
      "Epoch: 12/20; Batch:246/468; Training loss:0.100763\n",
      "Epoch: 12/20; Batch:247/468; Training loss:0.0981919\n",
      "Epoch: 12/20; Batch:248/468; Training loss:0.0984813\n",
      "Epoch: 12/20; Batch:249/468; Training loss:0.0985759\n",
      "Epoch: 12/20; Batch:250/468; Training loss:0.0924289\n",
      "Epoch: 12/20; Batch:251/468; Training loss:0.0982207\n",
      "Epoch: 12/20; Batch:252/468; Training loss:0.103147\n",
      "Epoch: 12/20; Batch:253/468; Training loss:0.102107\n",
      "Epoch: 12/20; Batch:254/468; Training loss:0.101101\n",
      "Epoch: 12/20; Batch:255/468; Training loss:0.095977\n",
      "Epoch: 12/20; Batch:256/468; Training loss:0.101803\n",
      "Epoch: 12/20; Batch:257/468; Training loss:0.101081\n",
      "Epoch: 12/20; Batch:258/468; Training loss:0.0999714\n",
      "Epoch: 12/20; Batch:259/468; Training loss:0.0957879\n",
      "Epoch: 12/20; Batch:260/468; Training loss:0.0978469\n",
      "Epoch: 12/20; Batch:261/468; Training loss:0.0967159\n",
      "Epoch: 12/20; Batch:262/468; Training loss:0.0977931\n",
      "Epoch: 12/20; Batch:263/468; Training loss:0.0985367\n",
      "Epoch: 12/20; Batch:264/468; Training loss:0.096473\n",
      "Epoch: 12/20; Batch:265/468; Training loss:0.0983052\n",
      "Epoch: 12/20; Batch:266/468; Training loss:0.0991507\n",
      "Epoch: 12/20; Batch:267/468; Training loss:0.096765\n",
      "Epoch: 12/20; Batch:268/468; Training loss:0.0975324\n",
      "Epoch: 12/20; Batch:269/468; Training loss:0.0971575\n",
      "Epoch: 12/20; Batch:270/468; Training loss:0.10085\n",
      "Epoch: 12/20; Batch:271/468; Training loss:0.100466\n",
      "Epoch: 12/20; Batch:272/468; Training loss:0.0976679\n",
      "Epoch: 12/20; Batch:273/468; Training loss:0.0980193\n",
      "Epoch: 12/20; Batch:274/468; Training loss:0.0948236\n",
      "Epoch: 12/20; Batch:275/468; Training loss:0.100102\n",
      "Epoch: 12/20; Batch:276/468; Training loss:0.0942208\n",
      "Epoch: 12/20; Batch:277/468; Training loss:0.0989824\n",
      "Epoch: 12/20; Batch:278/468; Training loss:0.101361\n",
      "Epoch: 12/20; Batch:279/468; Training loss:0.0965182\n",
      "Epoch: 12/20; Batch:280/468; Training loss:0.096328\n",
      "Epoch: 12/20; Batch:281/468; Training loss:0.0980901\n",
      "Epoch: 12/20; Batch:282/468; Training loss:0.0983344\n",
      "Epoch: 12/20; Batch:283/468; Training loss:0.10221\n",
      "Epoch: 12/20; Batch:284/468; Training loss:0.100018\n",
      "Epoch: 12/20; Batch:285/468; Training loss:0.0946388\n",
      "Epoch: 12/20; Batch:286/468; Training loss:0.0997279\n",
      "Epoch: 12/20; Batch:287/468; Training loss:0.0983064\n",
      "Epoch: 12/20; Batch:288/468; Training loss:0.0998074\n",
      "Epoch: 12/20; Batch:289/468; Training loss:0.0996103\n",
      "Epoch: 12/20; Batch:290/468; Training loss:0.095307\n",
      "Epoch: 12/20; Batch:291/468; Training loss:0.0981035\n",
      "Epoch: 12/20; Batch:292/468; Training loss:0.0981274\n",
      "Epoch: 12/20; Batch:293/468; Training loss:0.0964179\n",
      "Epoch: 12/20; Batch:294/468; Training loss:0.104544\n",
      "Epoch: 12/20; Batch:295/468; Training loss:0.0982621\n",
      "Epoch: 12/20; Batch:296/468; Training loss:0.096834\n",
      "Epoch: 12/20; Batch:297/468; Training loss:0.099622\n",
      "Epoch: 12/20; Batch:298/468; Training loss:0.0997785\n",
      "Epoch: 12/20; Batch:299/468; Training loss:0.0960609\n",
      "Epoch: 12/20; Batch:300/468; Training loss:0.0968777\n",
      "Epoch: 12/20; Batch:301/468; Training loss:0.10006\n",
      "Epoch: 12/20; Batch:302/468; Training loss:0.0966345\n",
      "Epoch: 12/20; Batch:303/468; Training loss:0.0965556\n",
      "Epoch: 12/20; Batch:304/468; Training loss:0.0999503\n",
      "Epoch: 12/20; Batch:305/468; Training loss:0.0992325\n",
      "Epoch: 12/20; Batch:306/468; Training loss:0.100195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20; Batch:307/468; Training loss:0.100734\n",
      "Epoch: 12/20; Batch:308/468; Training loss:0.096229\n",
      "Epoch: 12/20; Batch:309/468; Training loss:0.0977024\n",
      "Epoch: 12/20; Batch:310/468; Training loss:0.0969606\n",
      "Epoch: 12/20; Batch:311/468; Training loss:0.0986372\n",
      "Epoch: 12/20; Batch:312/468; Training loss:0.0988815\n",
      "Epoch: 12/20; Batch:313/468; Training loss:0.0966397\n",
      "Epoch: 12/20; Batch:314/468; Training loss:0.0987787\n",
      "Epoch: 12/20; Batch:315/468; Training loss:0.100469\n",
      "Epoch: 12/20; Batch:316/468; Training loss:0.0938539\n",
      "Epoch: 12/20; Batch:317/468; Training loss:0.0958801\n",
      "Epoch: 12/20; Batch:318/468; Training loss:0.095798\n",
      "Epoch: 12/20; Batch:319/468; Training loss:0.0948481\n",
      "Epoch: 12/20; Batch:320/468; Training loss:0.0949482\n",
      "Epoch: 12/20; Batch:321/468; Training loss:0.0979557\n",
      "Epoch: 12/20; Batch:322/468; Training loss:0.0919152\n",
      "Epoch: 12/20; Batch:323/468; Training loss:0.0954469\n",
      "Epoch: 12/20; Batch:324/468; Training loss:0.1007\n",
      "Epoch: 12/20; Batch:325/468; Training loss:0.0978509\n",
      "Epoch: 12/20; Batch:326/468; Training loss:0.0966591\n",
      "Epoch: 12/20; Batch:327/468; Training loss:0.0984118\n",
      "Epoch: 12/20; Batch:328/468; Training loss:0.0960704\n",
      "Epoch: 12/20; Batch:329/468; Training loss:0.0985068\n",
      "Epoch: 12/20; Batch:330/468; Training loss:0.0937295\n",
      "Epoch: 12/20; Batch:331/468; Training loss:0.101622\n",
      "Epoch: 12/20; Batch:332/468; Training loss:0.0973068\n",
      "Epoch: 12/20; Batch:333/468; Training loss:0.0971302\n",
      "Epoch: 12/20; Batch:334/468; Training loss:0.095619\n",
      "Epoch: 12/20; Batch:335/468; Training loss:0.0975898\n",
      "Epoch: 12/20; Batch:336/468; Training loss:0.0958564\n",
      "Epoch: 12/20; Batch:337/468; Training loss:0.0989318\n",
      "Epoch: 12/20; Batch:338/468; Training loss:0.098215\n",
      "Epoch: 12/20; Batch:339/468; Training loss:0.10017\n",
      "Epoch: 12/20; Batch:340/468; Training loss:0.101095\n",
      "Epoch: 12/20; Batch:341/468; Training loss:0.100399\n",
      "Epoch: 12/20; Batch:342/468; Training loss:0.0945597\n",
      "Epoch: 12/20; Batch:343/468; Training loss:0.0983093\n",
      "Epoch: 12/20; Batch:344/468; Training loss:0.10077\n",
      "Epoch: 12/20; Batch:345/468; Training loss:0.0945388\n",
      "Epoch: 12/20; Batch:346/468; Training loss:0.0967018\n",
      "Epoch: 12/20; Batch:347/468; Training loss:0.100173\n",
      "Epoch: 12/20; Batch:348/468; Training loss:0.0969536\n",
      "Epoch: 12/20; Batch:349/468; Training loss:0.0976982\n",
      "Epoch: 12/20; Batch:350/468; Training loss:0.0986359\n",
      "Epoch: 12/20; Batch:351/468; Training loss:0.0978384\n",
      "Epoch: 12/20; Batch:352/468; Training loss:0.0988015\n",
      "Epoch: 12/20; Batch:353/468; Training loss:0.0967918\n",
      "Epoch: 12/20; Batch:354/468; Training loss:0.0957118\n",
      "Epoch: 12/20; Batch:355/468; Training loss:0.0972031\n",
      "Epoch: 12/20; Batch:356/468; Training loss:0.0976934\n",
      "Epoch: 12/20; Batch:357/468; Training loss:0.0963348\n",
      "Epoch: 12/20; Batch:358/468; Training loss:0.0944993\n",
      "Epoch: 12/20; Batch:359/468; Training loss:0.0989758\n",
      "Epoch: 12/20; Batch:360/468; Training loss:0.100861\n",
      "Epoch: 12/20; Batch:361/468; Training loss:0.098271\n",
      "Epoch: 12/20; Batch:362/468; Training loss:0.0971311\n",
      "Epoch: 12/20; Batch:363/468; Training loss:0.0957546\n",
      "Epoch: 12/20; Batch:364/468; Training loss:0.0970745\n",
      "Epoch: 12/20; Batch:365/468; Training loss:0.0948581\n",
      "Epoch: 12/20; Batch:366/468; Training loss:0.0945507\n",
      "Epoch: 12/20; Batch:367/468; Training loss:0.0981964\n",
      "Epoch: 12/20; Batch:368/468; Training loss:0.0952282\n",
      "Epoch: 12/20; Batch:369/468; Training loss:0.101975\n",
      "Epoch: 12/20; Batch:370/468; Training loss:0.0948068\n",
      "Epoch: 12/20; Batch:371/468; Training loss:0.0981743\n",
      "Epoch: 12/20; Batch:372/468; Training loss:0.104351\n",
      "Epoch: 12/20; Batch:373/468; Training loss:0.098955\n",
      "Epoch: 12/20; Batch:374/468; Training loss:0.0998083\n",
      "Epoch: 12/20; Batch:375/468; Training loss:0.100498\n",
      "Epoch: 12/20; Batch:376/468; Training loss:0.0972191\n",
      "Epoch: 12/20; Batch:377/468; Training loss:0.0952218\n",
      "Epoch: 12/20; Batch:378/468; Training loss:0.0975548\n",
      "Epoch: 12/20; Batch:379/468; Training loss:0.101353\n",
      "Epoch: 12/20; Batch:380/468; Training loss:0.0961802\n",
      "Epoch: 12/20; Batch:381/468; Training loss:0.0955212\n",
      "Epoch: 12/20; Batch:382/468; Training loss:0.095506\n",
      "Epoch: 12/20; Batch:383/468; Training loss:0.0977236\n",
      "Epoch: 12/20; Batch:384/468; Training loss:0.0976135\n",
      "Epoch: 12/20; Batch:385/468; Training loss:0.0976457\n",
      "Epoch: 12/20; Batch:386/468; Training loss:0.0940268\n",
      "Epoch: 12/20; Batch:387/468; Training loss:0.0968808\n",
      "Epoch: 12/20; Batch:388/468; Training loss:0.0987601\n",
      "Epoch: 12/20; Batch:389/468; Training loss:0.0977594\n",
      "Epoch: 12/20; Batch:390/468; Training loss:0.093339\n",
      "Epoch: 12/20; Batch:391/468; Training loss:0.0973469\n",
      "Epoch: 12/20; Batch:392/468; Training loss:0.0940525\n",
      "Epoch: 12/20; Batch:393/468; Training loss:0.100483\n",
      "Epoch: 12/20; Batch:394/468; Training loss:0.0998192\n",
      "Epoch: 12/20; Batch:395/468; Training loss:0.0990262\n",
      "Epoch: 12/20; Batch:396/468; Training loss:0.0982867\n",
      "Epoch: 12/20; Batch:397/468; Training loss:0.100584\n",
      "Epoch: 12/20; Batch:398/468; Training loss:0.0973747\n",
      "Epoch: 12/20; Batch:399/468; Training loss:0.0970911\n",
      "Epoch: 12/20; Batch:400/468; Training loss:0.0978097\n",
      "Epoch: 12/20; Batch:401/468; Training loss:0.0981386\n",
      "Epoch: 12/20; Batch:402/468; Training loss:0.102207\n",
      "Epoch: 12/20; Batch:403/468; Training loss:0.0962149\n",
      "Epoch: 12/20; Batch:404/468; Training loss:0.098905\n",
      "Epoch: 12/20; Batch:405/468; Training loss:0.0957322\n",
      "Epoch: 12/20; Batch:406/468; Training loss:0.0998437\n",
      "Epoch: 12/20; Batch:407/468; Training loss:0.100061\n",
      "Epoch: 12/20; Batch:408/468; Training loss:0.1007\n",
      "Epoch: 12/20; Batch:409/468; Training loss:0.101682\n",
      "Epoch: 12/20; Batch:410/468; Training loss:0.101952\n",
      "Epoch: 12/20; Batch:411/468; Training loss:0.096721\n",
      "Epoch: 12/20; Batch:412/468; Training loss:0.101719\n",
      "Epoch: 12/20; Batch:413/468; Training loss:0.0982379\n",
      "Epoch: 12/20; Batch:414/468; Training loss:0.102753\n",
      "Epoch: 12/20; Batch:415/468; Training loss:0.0994014\n",
      "Epoch: 12/20; Batch:416/468; Training loss:0.100043\n",
      "Epoch: 12/20; Batch:417/468; Training loss:0.0946986\n",
      "Epoch: 12/20; Batch:418/468; Training loss:0.100391\n",
      "Epoch: 12/20; Batch:419/468; Training loss:0.0951561\n",
      "Epoch: 12/20; Batch:420/468; Training loss:0.101696\n",
      "Epoch: 12/20; Batch:421/468; Training loss:0.0993847\n",
      "Epoch: 12/20; Batch:422/468; Training loss:0.0993666\n",
      "Epoch: 12/20; Batch:423/468; Training loss:0.102888\n",
      "Epoch: 12/20; Batch:424/468; Training loss:0.0960445\n",
      "Epoch: 12/20; Batch:425/468; Training loss:0.0980902\n",
      "Epoch: 12/20; Batch:426/468; Training loss:0.0958123\n",
      "Epoch: 12/20; Batch:427/468; Training loss:0.0960797\n",
      "Epoch: 12/20; Batch:428/468; Training loss:0.0969955\n",
      "Epoch: 12/20; Batch:429/468; Training loss:0.0978357\n",
      "Epoch: 12/20; Batch:430/468; Training loss:0.0996361\n",
      "Epoch: 12/20; Batch:431/468; Training loss:0.0955767\n",
      "Epoch: 12/20; Batch:432/468; Training loss:0.0969151\n",
      "Epoch: 12/20; Batch:433/468; Training loss:0.0998788\n",
      "Epoch: 12/20; Batch:434/468; Training loss:0.103597\n",
      "Epoch: 12/20; Batch:435/468; Training loss:0.100813\n",
      "Epoch: 12/20; Batch:436/468; Training loss:0.0940185\n",
      "Epoch: 12/20; Batch:437/468; Training loss:0.0990745\n",
      "Epoch: 12/20; Batch:438/468; Training loss:0.0945075\n",
      "Epoch: 12/20; Batch:439/468; Training loss:0.101408\n",
      "Epoch: 12/20; Batch:440/468; Training loss:0.0960132\n",
      "Epoch: 12/20; Batch:441/468; Training loss:0.0971993\n",
      "Epoch: 12/20; Batch:442/468; Training loss:0.0939199\n",
      "Epoch: 12/20; Batch:443/468; Training loss:0.0971954\n",
      "Epoch: 12/20; Batch:444/468; Training loss:0.0982742\n",
      "Epoch: 12/20; Batch:445/468; Training loss:0.0963607\n",
      "Epoch: 12/20; Batch:446/468; Training loss:0.0986996\n",
      "Epoch: 12/20; Batch:447/468; Training loss:0.100481\n",
      "Epoch: 12/20; Batch:448/468; Training loss:0.0973022\n",
      "Epoch: 12/20; Batch:449/468; Training loss:0.100408\n",
      "Epoch: 12/20; Batch:450/468; Training loss:0.0994274\n",
      "Epoch: 12/20; Batch:451/468; Training loss:0.098131\n",
      "Epoch: 12/20; Batch:452/468; Training loss:0.103586\n",
      "Epoch: 12/20; Batch:453/468; Training loss:0.0981527\n",
      "Epoch: 12/20; Batch:454/468; Training loss:0.100285\n",
      "Epoch: 12/20; Batch:455/468; Training loss:0.0980357\n",
      "Epoch: 12/20; Batch:456/468; Training loss:0.0961552\n",
      "Epoch: 12/20; Batch:457/468; Training loss:0.0994714\n",
      "Epoch: 12/20; Batch:458/468; Training loss:0.0959021\n",
      "Epoch: 12/20; Batch:459/468; Training loss:0.097086\n",
      "Epoch: 12/20; Batch:460/468; Training loss:0.0972709\n",
      "Epoch: 12/20; Batch:461/468; Training loss:0.0982779\n",
      "Epoch: 12/20; Batch:462/468; Training loss:0.0972992\n",
      "Epoch: 12/20; Batch:463/468; Training loss:0.0963254\n",
      "Epoch: 12/20; Batch:464/468; Training loss:0.0984927\n",
      "Epoch: 12/20; Batch:465/468; Training loss:0.0978564\n",
      "Epoch: 12/20; Batch:466/468; Training loss:0.101075\n",
      "Epoch: 12/20; Batch:467/468; Training loss:0.0982167\n",
      "Epoch: 12/20; Batch:468/468; Training loss:0.100846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20; Batch:1/468; Training loss:0.0989044\n",
      "Epoch: 13/20; Batch:2/468; Training loss:0.0970022\n",
      "Epoch: 13/20; Batch:3/468; Training loss:0.0962425\n",
      "Epoch: 13/20; Batch:4/468; Training loss:0.097367\n",
      "Epoch: 13/20; Batch:5/468; Training loss:0.0952898\n",
      "Epoch: 13/20; Batch:6/468; Training loss:0.0964265\n",
      "Epoch: 13/20; Batch:7/468; Training loss:0.0997745\n",
      "Epoch: 13/20; Batch:8/468; Training loss:0.0996182\n",
      "Epoch: 13/20; Batch:9/468; Training loss:0.100699\n",
      "Epoch: 13/20; Batch:10/468; Training loss:0.0989675\n",
      "Epoch: 13/20; Batch:11/468; Training loss:0.0997475\n",
      "Epoch: 13/20; Batch:12/468; Training loss:0.0966964\n",
      "Epoch: 13/20; Batch:13/468; Training loss:0.0966131\n",
      "Epoch: 13/20; Batch:14/468; Training loss:0.0967415\n",
      "Epoch: 13/20; Batch:15/468; Training loss:0.0974788\n",
      "Epoch: 13/20; Batch:16/468; Training loss:0.0977917\n",
      "Epoch: 13/20; Batch:17/468; Training loss:0.100337\n",
      "Epoch: 13/20; Batch:18/468; Training loss:0.0996677\n",
      "Epoch: 13/20; Batch:19/468; Training loss:0.0983402\n",
      "Epoch: 13/20; Batch:20/468; Training loss:0.0991479\n",
      "Epoch: 13/20; Batch:21/468; Training loss:0.0964719\n",
      "Epoch: 13/20; Batch:22/468; Training loss:0.0986662\n",
      "Epoch: 13/20; Batch:23/468; Training loss:0.0975694\n",
      "Epoch: 13/20; Batch:24/468; Training loss:0.0994689\n",
      "Epoch: 13/20; Batch:25/468; Training loss:0.0997311\n",
      "Epoch: 13/20; Batch:26/468; Training loss:0.0998818\n",
      "Epoch: 13/20; Batch:27/468; Training loss:0.100223\n",
      "Epoch: 13/20; Batch:28/468; Training loss:0.099415\n",
      "Epoch: 13/20; Batch:29/468; Training loss:0.0997889\n",
      "Epoch: 13/20; Batch:30/468; Training loss:0.101666\n",
      "Epoch: 13/20; Batch:31/468; Training loss:0.0988792\n",
      "Epoch: 13/20; Batch:32/468; Training loss:0.0973069\n",
      "Epoch: 13/20; Batch:33/468; Training loss:0.0976915\n",
      "Epoch: 13/20; Batch:34/468; Training loss:0.0987638\n",
      "Epoch: 13/20; Batch:35/468; Training loss:0.0980961\n",
      "Epoch: 13/20; Batch:36/468; Training loss:0.0992491\n",
      "Epoch: 13/20; Batch:37/468; Training loss:0.0985577\n",
      "Epoch: 13/20; Batch:38/468; Training loss:0.0970691\n",
      "Epoch: 13/20; Batch:39/468; Training loss:0.0998773\n",
      "Epoch: 13/20; Batch:40/468; Training loss:0.0960001\n",
      "Epoch: 13/20; Batch:41/468; Training loss:0.0992854\n",
      "Epoch: 13/20; Batch:42/468; Training loss:0.0998577\n",
      "Epoch: 13/20; Batch:43/468; Training loss:0.0969099\n",
      "Epoch: 13/20; Batch:44/468; Training loss:0.0961842\n",
      "Epoch: 13/20; Batch:45/468; Training loss:0.101786\n",
      "Epoch: 13/20; Batch:46/468; Training loss:0.0958417\n",
      "Epoch: 13/20; Batch:47/468; Training loss:0.0956016\n",
      "Epoch: 13/20; Batch:48/468; Training loss:0.0998864\n",
      "Epoch: 13/20; Batch:49/468; Training loss:0.0968421\n",
      "Epoch: 13/20; Batch:50/468; Training loss:0.0978835\n",
      "Epoch: 13/20; Batch:51/468; Training loss:0.100991\n",
      "Epoch: 13/20; Batch:52/468; Training loss:0.0978737\n",
      "Epoch: 13/20; Batch:53/468; Training loss:0.0961078\n",
      "Epoch: 13/20; Batch:54/468; Training loss:0.102502\n",
      "Epoch: 13/20; Batch:55/468; Training loss:0.0993768\n",
      "Epoch: 13/20; Batch:56/468; Training loss:0.0991953\n",
      "Epoch: 13/20; Batch:57/468; Training loss:0.101055\n",
      "Epoch: 13/20; Batch:58/468; Training loss:0.0988689\n",
      "Epoch: 13/20; Batch:59/468; Training loss:0.0991015\n",
      "Epoch: 13/20; Batch:60/468; Training loss:0.0941831\n",
      "Epoch: 13/20; Batch:61/468; Training loss:0.093322\n",
      "Epoch: 13/20; Batch:62/468; Training loss:0.0978527\n",
      "Epoch: 13/20; Batch:63/468; Training loss:0.0965146\n",
      "Epoch: 13/20; Batch:64/468; Training loss:0.0973073\n",
      "Epoch: 13/20; Batch:65/468; Training loss:0.0973876\n",
      "Epoch: 13/20; Batch:66/468; Training loss:0.0994064\n",
      "Epoch: 13/20; Batch:67/468; Training loss:0.0986254\n",
      "Epoch: 13/20; Batch:68/468; Training loss:0.0969857\n",
      "Epoch: 13/20; Batch:69/468; Training loss:0.0990648\n",
      "Epoch: 13/20; Batch:70/468; Training loss:0.0948417\n",
      "Epoch: 13/20; Batch:71/468; Training loss:0.0996987\n",
      "Epoch: 13/20; Batch:72/468; Training loss:0.0966457\n",
      "Epoch: 13/20; Batch:73/468; Training loss:0.0997213\n",
      "Epoch: 13/20; Batch:74/468; Training loss:0.0987387\n",
      "Epoch: 13/20; Batch:75/468; Training loss:0.093691\n",
      "Epoch: 13/20; Batch:76/468; Training loss:0.0960199\n",
      "Epoch: 13/20; Batch:77/468; Training loss:0.0970734\n",
      "Epoch: 13/20; Batch:78/468; Training loss:0.0980625\n",
      "Epoch: 13/20; Batch:79/468; Training loss:0.0974542\n",
      "Epoch: 13/20; Batch:80/468; Training loss:0.0968957\n",
      "Epoch: 13/20; Batch:81/468; Training loss:0.0933963\n",
      "Epoch: 13/20; Batch:82/468; Training loss:0.102727\n",
      "Epoch: 13/20; Batch:83/468; Training loss:0.0974009\n",
      "Epoch: 13/20; Batch:84/468; Training loss:0.0966468\n",
      "Epoch: 13/20; Batch:85/468; Training loss:0.0960891\n",
      "Epoch: 13/20; Batch:86/468; Training loss:0.0958719\n",
      "Epoch: 13/20; Batch:87/468; Training loss:0.101288\n",
      "Epoch: 13/20; Batch:88/468; Training loss:0.0997747\n",
      "Epoch: 13/20; Batch:89/468; Training loss:0.0975065\n",
      "Epoch: 13/20; Batch:90/468; Training loss:0.0985612\n",
      "Epoch: 13/20; Batch:91/468; Training loss:0.094466\n",
      "Epoch: 13/20; Batch:92/468; Training loss:0.0970267\n",
      "Epoch: 13/20; Batch:93/468; Training loss:0.0984895\n",
      "Epoch: 13/20; Batch:94/468; Training loss:0.0990009\n",
      "Epoch: 13/20; Batch:95/468; Training loss:0.099001\n",
      "Epoch: 13/20; Batch:96/468; Training loss:0.0999391\n",
      "Epoch: 13/20; Batch:97/468; Training loss:0.0988111\n",
      "Epoch: 13/20; Batch:98/468; Training loss:0.0997432\n",
      "Epoch: 13/20; Batch:99/468; Training loss:0.0978094\n",
      "Epoch: 13/20; Batch:100/468; Training loss:0.0966365\n",
      "Epoch: 13/20; Batch:101/468; Training loss:0.0958397\n",
      "Epoch: 13/20; Batch:102/468; Training loss:0.100467\n",
      "Epoch: 13/20; Batch:103/468; Training loss:0.0956044\n",
      "Epoch: 13/20; Batch:104/468; Training loss:0.101295\n",
      "Epoch: 13/20; Batch:105/468; Training loss:0.0994592\n",
      "Epoch: 13/20; Batch:106/468; Training loss:0.0991429\n",
      "Epoch: 13/20; Batch:107/468; Training loss:0.0953728\n",
      "Epoch: 13/20; Batch:108/468; Training loss:0.101231\n",
      "Epoch: 13/20; Batch:109/468; Training loss:0.100418\n",
      "Epoch: 13/20; Batch:110/468; Training loss:0.0990641\n",
      "Epoch: 13/20; Batch:111/468; Training loss:0.0986488\n",
      "Epoch: 13/20; Batch:112/468; Training loss:0.100958\n",
      "Epoch: 13/20; Batch:113/468; Training loss:0.10255\n",
      "Epoch: 13/20; Batch:114/468; Training loss:0.100401\n",
      "Epoch: 13/20; Batch:115/468; Training loss:0.0989832\n",
      "Epoch: 13/20; Batch:116/468; Training loss:0.0984442\n",
      "Epoch: 13/20; Batch:117/468; Training loss:0.0979473\n",
      "Epoch: 13/20; Batch:118/468; Training loss:0.0938031\n",
      "Epoch: 13/20; Batch:119/468; Training loss:0.0975637\n",
      "Epoch: 13/20; Batch:120/468; Training loss:0.0958908\n",
      "Epoch: 13/20; Batch:121/468; Training loss:0.101409\n",
      "Epoch: 13/20; Batch:122/468; Training loss:0.0951328\n",
      "Epoch: 13/20; Batch:123/468; Training loss:0.0969101\n",
      "Epoch: 13/20; Batch:124/468; Training loss:0.0979965\n",
      "Epoch: 13/20; Batch:125/468; Training loss:0.0969696\n",
      "Epoch: 13/20; Batch:126/468; Training loss:0.100826\n",
      "Epoch: 13/20; Batch:127/468; Training loss:0.0974112\n",
      "Epoch: 13/20; Batch:128/468; Training loss:0.0967975\n",
      "Epoch: 13/20; Batch:129/468; Training loss:0.0961582\n",
      "Epoch: 13/20; Batch:130/468; Training loss:0.0992577\n",
      "Epoch: 13/20; Batch:131/468; Training loss:0.0962595\n",
      "Epoch: 13/20; Batch:132/468; Training loss:0.100476\n",
      "Epoch: 13/20; Batch:133/468; Training loss:0.101178\n",
      "Epoch: 13/20; Batch:134/468; Training loss:0.098855\n",
      "Epoch: 13/20; Batch:135/468; Training loss:0.0985143\n",
      "Epoch: 13/20; Batch:136/468; Training loss:0.0933359\n",
      "Epoch: 13/20; Batch:137/468; Training loss:0.0949723\n",
      "Epoch: 13/20; Batch:138/468; Training loss:0.0968677\n",
      "Epoch: 13/20; Batch:139/468; Training loss:0.100545\n",
      "Epoch: 13/20; Batch:140/468; Training loss:0.0950705\n",
      "Epoch: 13/20; Batch:141/468; Training loss:0.0974335\n",
      "Epoch: 13/20; Batch:142/468; Training loss:0.0998708\n",
      "Epoch: 13/20; Batch:143/468; Training loss:0.0980744\n",
      "Epoch: 13/20; Batch:144/468; Training loss:0.0946916\n",
      "Epoch: 13/20; Batch:145/468; Training loss:0.10091\n",
      "Epoch: 13/20; Batch:146/468; Training loss:0.0912726\n",
      "Epoch: 13/20; Batch:147/468; Training loss:0.0988459\n",
      "Epoch: 13/20; Batch:148/468; Training loss:0.0988319\n",
      "Epoch: 13/20; Batch:149/468; Training loss:0.0987483\n",
      "Epoch: 13/20; Batch:150/468; Training loss:0.0970291\n",
      "Epoch: 13/20; Batch:151/468; Training loss:0.0960696\n",
      "Epoch: 13/20; Batch:152/468; Training loss:0.0911538\n",
      "Epoch: 13/20; Batch:153/468; Training loss:0.0972561\n",
      "Epoch: 13/20; Batch:154/468; Training loss:0.0943463\n",
      "Epoch: 13/20; Batch:155/468; Training loss:0.0984565\n",
      "Epoch: 13/20; Batch:156/468; Training loss:0.0966755\n",
      "Epoch: 13/20; Batch:157/468; Training loss:0.099946\n",
      "Epoch: 13/20; Batch:158/468; Training loss:0.0980285\n",
      "Epoch: 13/20; Batch:159/468; Training loss:0.0988237\n",
      "Epoch: 13/20; Batch:160/468; Training loss:0.100154\n",
      "Epoch: 13/20; Batch:161/468; Training loss:0.0926944\n",
      "Epoch: 13/20; Batch:162/468; Training loss:0.101829\n",
      "Epoch: 13/20; Batch:163/468; Training loss:0.0957703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20; Batch:164/468; Training loss:0.0975589\n",
      "Epoch: 13/20; Batch:165/468; Training loss:0.0962033\n",
      "Epoch: 13/20; Batch:166/468; Training loss:0.0956794\n",
      "Epoch: 13/20; Batch:167/468; Training loss:0.0985594\n",
      "Epoch: 13/20; Batch:168/468; Training loss:0.0955679\n",
      "Epoch: 13/20; Batch:169/468; Training loss:0.098513\n",
      "Epoch: 13/20; Batch:170/468; Training loss:0.098464\n",
      "Epoch: 13/20; Batch:171/468; Training loss:0.0954822\n",
      "Epoch: 13/20; Batch:172/468; Training loss:0.0972911\n",
      "Epoch: 13/20; Batch:173/468; Training loss:0.0998038\n",
      "Epoch: 13/20; Batch:174/468; Training loss:0.0977362\n",
      "Epoch: 13/20; Batch:175/468; Training loss:0.0996917\n",
      "Epoch: 13/20; Batch:176/468; Training loss:0.0958363\n",
      "Epoch: 13/20; Batch:177/468; Training loss:0.0942801\n",
      "Epoch: 13/20; Batch:178/468; Training loss:0.0997259\n",
      "Epoch: 13/20; Batch:179/468; Training loss:0.0986404\n",
      "Epoch: 13/20; Batch:180/468; Training loss:0.100515\n",
      "Epoch: 13/20; Batch:181/468; Training loss:0.0983687\n",
      "Epoch: 13/20; Batch:182/468; Training loss:0.0977819\n",
      "Epoch: 13/20; Batch:183/468; Training loss:0.0973284\n",
      "Epoch: 13/20; Batch:184/468; Training loss:0.101997\n",
      "Epoch: 13/20; Batch:185/468; Training loss:0.0921891\n",
      "Epoch: 13/20; Batch:186/468; Training loss:0.099725\n",
      "Epoch: 13/20; Batch:187/468; Training loss:0.0984927\n",
      "Epoch: 13/20; Batch:188/468; Training loss:0.0963039\n",
      "Epoch: 13/20; Batch:189/468; Training loss:0.0953677\n",
      "Epoch: 13/20; Batch:190/468; Training loss:0.0960868\n",
      "Epoch: 13/20; Batch:191/468; Training loss:0.098759\n",
      "Epoch: 13/20; Batch:192/468; Training loss:0.0946091\n",
      "Epoch: 13/20; Batch:193/468; Training loss:0.0969177\n",
      "Epoch: 13/20; Batch:194/468; Training loss:0.100828\n",
      "Epoch: 13/20; Batch:195/468; Training loss:0.0999968\n",
      "Epoch: 13/20; Batch:196/468; Training loss:0.0964092\n",
      "Epoch: 13/20; Batch:197/468; Training loss:0.100382\n",
      "Epoch: 13/20; Batch:198/468; Training loss:0.0978282\n",
      "Epoch: 13/20; Batch:199/468; Training loss:0.100184\n",
      "Epoch: 13/20; Batch:200/468; Training loss:0.0970139\n",
      "Epoch: 13/20; Batch:201/468; Training loss:0.0970097\n",
      "Epoch: 13/20; Batch:202/468; Training loss:0.0988009\n",
      "Epoch: 13/20; Batch:203/468; Training loss:0.0992302\n",
      "Epoch: 13/20; Batch:204/468; Training loss:0.099663\n",
      "Epoch: 13/20; Batch:205/468; Training loss:0.0976522\n",
      "Epoch: 13/20; Batch:206/468; Training loss:0.096492\n",
      "Epoch: 13/20; Batch:207/468; Training loss:0.0986531\n",
      "Epoch: 13/20; Batch:208/468; Training loss:0.0955084\n",
      "Epoch: 13/20; Batch:209/468; Training loss:0.0957141\n",
      "Epoch: 13/20; Batch:210/468; Training loss:0.0974925\n",
      "Epoch: 13/20; Batch:211/468; Training loss:0.093603\n",
      "Epoch: 13/20; Batch:212/468; Training loss:0.095644\n",
      "Epoch: 13/20; Batch:213/468; Training loss:0.0998966\n",
      "Epoch: 13/20; Batch:214/468; Training loss:0.0999017\n",
      "Epoch: 13/20; Batch:215/468; Training loss:0.0973941\n",
      "Epoch: 13/20; Batch:216/468; Training loss:0.0991825\n",
      "Epoch: 13/20; Batch:217/468; Training loss:0.0973045\n",
      "Epoch: 13/20; Batch:218/468; Training loss:0.0978442\n",
      "Epoch: 13/20; Batch:219/468; Training loss:0.094324\n",
      "Epoch: 13/20; Batch:220/468; Training loss:0.0975542\n",
      "Epoch: 13/20; Batch:221/468; Training loss:0.0977441\n",
      "Epoch: 13/20; Batch:222/468; Training loss:0.101744\n",
      "Epoch: 13/20; Batch:223/468; Training loss:0.0959232\n",
      "Epoch: 13/20; Batch:224/468; Training loss:0.0961634\n",
      "Epoch: 13/20; Batch:225/468; Training loss:0.0966453\n",
      "Epoch: 13/20; Batch:226/468; Training loss:0.101731\n",
      "Epoch: 13/20; Batch:227/468; Training loss:0.0970831\n",
      "Epoch: 13/20; Batch:228/468; Training loss:0.098896\n",
      "Epoch: 13/20; Batch:229/468; Training loss:0.102214\n",
      "Epoch: 13/20; Batch:230/468; Training loss:0.100612\n",
      "Epoch: 13/20; Batch:231/468; Training loss:0.0978497\n",
      "Epoch: 13/20; Batch:232/468; Training loss:0.0989758\n",
      "Epoch: 13/20; Batch:233/468; Training loss:0.101343\n",
      "Epoch: 13/20; Batch:234/468; Training loss:0.102328\n",
      "Epoch: 13/20; Batch:235/468; Training loss:0.097936\n",
      "Epoch: 13/20; Batch:236/468; Training loss:0.0968307\n",
      "Epoch: 13/20; Batch:237/468; Training loss:0.0975121\n",
      "Epoch: 13/20; Batch:238/468; Training loss:0.0976095\n",
      "Epoch: 13/20; Batch:239/468; Training loss:0.0976515\n",
      "Epoch: 13/20; Batch:240/468; Training loss:0.100727\n",
      "Epoch: 13/20; Batch:241/468; Training loss:0.0980109\n",
      "Epoch: 13/20; Batch:242/468; Training loss:0.09834\n",
      "Epoch: 13/20; Batch:243/468; Training loss:0.0966575\n",
      "Epoch: 13/20; Batch:244/468; Training loss:0.101415\n",
      "Epoch: 13/20; Batch:245/468; Training loss:0.0990164\n",
      "Epoch: 13/20; Batch:246/468; Training loss:0.0941641\n",
      "Epoch: 13/20; Batch:247/468; Training loss:0.0977298\n",
      "Epoch: 13/20; Batch:248/468; Training loss:0.0973417\n",
      "Epoch: 13/20; Batch:249/468; Training loss:0.096542\n",
      "Epoch: 13/20; Batch:250/468; Training loss:0.0938864\n",
      "Epoch: 13/20; Batch:251/468; Training loss:0.0986715\n",
      "Epoch: 13/20; Batch:252/468; Training loss:0.0927455\n",
      "Epoch: 13/20; Batch:253/468; Training loss:0.0978214\n",
      "Epoch: 13/20; Batch:254/468; Training loss:0.0977921\n",
      "Epoch: 13/20; Batch:255/468; Training loss:0.101923\n",
      "Epoch: 13/20; Batch:256/468; Training loss:0.0974999\n",
      "Epoch: 13/20; Batch:257/468; Training loss:0.0952145\n",
      "Epoch: 13/20; Batch:258/468; Training loss:0.0983411\n",
      "Epoch: 13/20; Batch:259/468; Training loss:0.0986962\n",
      "Epoch: 13/20; Batch:260/468; Training loss:0.0954937\n",
      "Epoch: 13/20; Batch:261/468; Training loss:0.0927473\n",
      "Epoch: 13/20; Batch:262/468; Training loss:0.0984817\n",
      "Epoch: 13/20; Batch:263/468; Training loss:0.0985574\n",
      "Epoch: 13/20; Batch:264/468; Training loss:0.0939766\n",
      "Epoch: 13/20; Batch:265/468; Training loss:0.0930812\n",
      "Epoch: 13/20; Batch:266/468; Training loss:0.0978202\n",
      "Epoch: 13/20; Batch:267/468; Training loss:0.0979974\n",
      "Epoch: 13/20; Batch:268/468; Training loss:0.0977113\n",
      "Epoch: 13/20; Batch:269/468; Training loss:0.0970691\n",
      "Epoch: 13/20; Batch:270/468; Training loss:0.0965179\n",
      "Epoch: 13/20; Batch:271/468; Training loss:0.0993356\n",
      "Epoch: 13/20; Batch:272/468; Training loss:0.0946813\n",
      "Epoch: 13/20; Batch:273/468; Training loss:0.0953603\n",
      "Epoch: 13/20; Batch:274/468; Training loss:0.0986953\n",
      "Epoch: 13/20; Batch:275/468; Training loss:0.097036\n",
      "Epoch: 13/20; Batch:276/468; Training loss:0.0986774\n",
      "Epoch: 13/20; Batch:277/468; Training loss:0.092959\n",
      "Epoch: 13/20; Batch:278/468; Training loss:0.100943\n",
      "Epoch: 13/20; Batch:279/468; Training loss:0.0978881\n",
      "Epoch: 13/20; Batch:280/468; Training loss:0.0978463\n",
      "Epoch: 13/20; Batch:281/468; Training loss:0.0987957\n",
      "Epoch: 13/20; Batch:282/468; Training loss:0.0970878\n",
      "Epoch: 13/20; Batch:283/468; Training loss:0.0966617\n",
      "Epoch: 13/20; Batch:284/468; Training loss:0.0996931\n",
      "Epoch: 13/20; Batch:285/468; Training loss:0.100342\n",
      "Epoch: 13/20; Batch:286/468; Training loss:0.0968126\n",
      "Epoch: 13/20; Batch:287/468; Training loss:0.093645\n",
      "Epoch: 13/20; Batch:288/468; Training loss:0.099002\n",
      "Epoch: 13/20; Batch:289/468; Training loss:0.0980226\n",
      "Epoch: 13/20; Batch:290/468; Training loss:0.100736\n",
      "Epoch: 13/20; Batch:291/468; Training loss:0.0982855\n",
      "Epoch: 13/20; Batch:292/468; Training loss:0.0964255\n",
      "Epoch: 13/20; Batch:293/468; Training loss:0.095087\n",
      "Epoch: 13/20; Batch:294/468; Training loss:0.0996266\n",
      "Epoch: 13/20; Batch:295/468; Training loss:0.0998881\n",
      "Epoch: 13/20; Batch:296/468; Training loss:0.0967718\n",
      "Epoch: 13/20; Batch:297/468; Training loss:0.0973369\n",
      "Epoch: 13/20; Batch:298/468; Training loss:0.0957069\n",
      "Epoch: 13/20; Batch:299/468; Training loss:0.0964016\n",
      "Epoch: 13/20; Batch:300/468; Training loss:0.099719\n",
      "Epoch: 13/20; Batch:301/468; Training loss:0.101091\n",
      "Epoch: 13/20; Batch:302/468; Training loss:0.0995523\n",
      "Epoch: 13/20; Batch:303/468; Training loss:0.100811\n",
      "Epoch: 13/20; Batch:304/468; Training loss:0.0944137\n",
      "Epoch: 13/20; Batch:305/468; Training loss:0.0948269\n",
      "Epoch: 13/20; Batch:306/468; Training loss:0.100894\n",
      "Epoch: 13/20; Batch:307/468; Training loss:0.0989095\n",
      "Epoch: 13/20; Batch:308/468; Training loss:0.0989465\n",
      "Epoch: 13/20; Batch:309/468; Training loss:0.0984236\n",
      "Epoch: 13/20; Batch:310/468; Training loss:0.0985541\n",
      "Epoch: 13/20; Batch:311/468; Training loss:0.0994261\n",
      "Epoch: 13/20; Batch:312/468; Training loss:0.0956758\n",
      "Epoch: 13/20; Batch:313/468; Training loss:0.0926148\n",
      "Epoch: 13/20; Batch:314/468; Training loss:0.0979477\n",
      "Epoch: 13/20; Batch:315/468; Training loss:0.0986957\n",
      "Epoch: 13/20; Batch:316/468; Training loss:0.0975053\n",
      "Epoch: 13/20; Batch:317/468; Training loss:0.095784\n",
      "Epoch: 13/20; Batch:318/468; Training loss:0.0971454\n",
      "Epoch: 13/20; Batch:319/468; Training loss:0.0994637\n",
      "Epoch: 13/20; Batch:320/468; Training loss:0.0976314\n",
      "Epoch: 13/20; Batch:321/468; Training loss:0.101596\n",
      "Epoch: 13/20; Batch:322/468; Training loss:0.0986499\n",
      "Epoch: 13/20; Batch:323/468; Training loss:0.0993477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20; Batch:324/468; Training loss:0.0951158\n",
      "Epoch: 13/20; Batch:325/468; Training loss:0.09901\n",
      "Epoch: 13/20; Batch:326/468; Training loss:0.0957785\n",
      "Epoch: 13/20; Batch:327/468; Training loss:0.0964491\n",
      "Epoch: 13/20; Batch:328/468; Training loss:0.0979476\n",
      "Epoch: 13/20; Batch:329/468; Training loss:0.0957228\n",
      "Epoch: 13/20; Batch:330/468; Training loss:0.0966626\n",
      "Epoch: 13/20; Batch:331/468; Training loss:0.0975097\n",
      "Epoch: 13/20; Batch:332/468; Training loss:0.0971004\n",
      "Epoch: 13/20; Batch:333/468; Training loss:0.0949578\n",
      "Epoch: 13/20; Batch:334/468; Training loss:0.0996131\n",
      "Epoch: 13/20; Batch:335/468; Training loss:0.099401\n",
      "Epoch: 13/20; Batch:336/468; Training loss:0.0914268\n",
      "Epoch: 13/20; Batch:337/468; Training loss:0.0981508\n",
      "Epoch: 13/20; Batch:338/468; Training loss:0.0968067\n",
      "Epoch: 13/20; Batch:339/468; Training loss:0.095243\n",
      "Epoch: 13/20; Batch:340/468; Training loss:0.0963267\n",
      "Epoch: 13/20; Batch:341/468; Training loss:0.0940815\n",
      "Epoch: 13/20; Batch:342/468; Training loss:0.0994834\n",
      "Epoch: 13/20; Batch:343/468; Training loss:0.0961588\n",
      "Epoch: 13/20; Batch:344/468; Training loss:0.0985921\n",
      "Epoch: 13/20; Batch:345/468; Training loss:0.0923993\n",
      "Epoch: 13/20; Batch:346/468; Training loss:0.09464\n",
      "Epoch: 13/20; Batch:347/468; Training loss:0.097959\n",
      "Epoch: 13/20; Batch:348/468; Training loss:0.0961345\n",
      "Epoch: 13/20; Batch:349/468; Training loss:0.0996241\n",
      "Epoch: 13/20; Batch:350/468; Training loss:0.0960047\n",
      "Epoch: 13/20; Batch:351/468; Training loss:0.0975377\n",
      "Epoch: 13/20; Batch:352/468; Training loss:0.0940554\n",
      "Epoch: 13/20; Batch:353/468; Training loss:0.0965728\n",
      "Epoch: 13/20; Batch:354/468; Training loss:0.0970681\n",
      "Epoch: 13/20; Batch:355/468; Training loss:0.10269\n",
      "Epoch: 13/20; Batch:356/468; Training loss:0.0994652\n",
      "Epoch: 13/20; Batch:357/468; Training loss:0.0978338\n",
      "Epoch: 13/20; Batch:358/468; Training loss:0.0985142\n",
      "Epoch: 13/20; Batch:359/468; Training loss:0.0993089\n",
      "Epoch: 13/20; Batch:360/468; Training loss:0.101449\n",
      "Epoch: 13/20; Batch:361/468; Training loss:0.0953274\n",
      "Epoch: 13/20; Batch:362/468; Training loss:0.0970114\n",
      "Epoch: 13/20; Batch:363/468; Training loss:0.0979778\n",
      "Epoch: 13/20; Batch:364/468; Training loss:0.094735\n",
      "Epoch: 13/20; Batch:365/468; Training loss:0.0955879\n",
      "Epoch: 13/20; Batch:366/468; Training loss:0.0985788\n",
      "Epoch: 13/20; Batch:367/468; Training loss:0.0960262\n",
      "Epoch: 13/20; Batch:368/468; Training loss:0.103011\n",
      "Epoch: 13/20; Batch:369/468; Training loss:0.0964019\n",
      "Epoch: 13/20; Batch:370/468; Training loss:0.0994423\n",
      "Epoch: 13/20; Batch:371/468; Training loss:0.0959842\n",
      "Epoch: 13/20; Batch:372/468; Training loss:0.0970697\n",
      "Epoch: 13/20; Batch:373/468; Training loss:0.0988578\n",
      "Epoch: 13/20; Batch:374/468; Training loss:0.0955401\n",
      "Epoch: 13/20; Batch:375/468; Training loss:0.0974379\n",
      "Epoch: 13/20; Batch:376/468; Training loss:0.0973843\n",
      "Epoch: 13/20; Batch:377/468; Training loss:0.0981573\n",
      "Epoch: 13/20; Batch:378/468; Training loss:0.0971036\n",
      "Epoch: 13/20; Batch:379/468; Training loss:0.0964663\n",
      "Epoch: 13/20; Batch:380/468; Training loss:0.0981615\n",
      "Epoch: 13/20; Batch:381/468; Training loss:0.0995503\n",
      "Epoch: 13/20; Batch:382/468; Training loss:0.100902\n",
      "Epoch: 13/20; Batch:383/468; Training loss:0.0966005\n",
      "Epoch: 13/20; Batch:384/468; Training loss:0.100841\n",
      "Epoch: 13/20; Batch:385/468; Training loss:0.0973802\n",
      "Epoch: 13/20; Batch:386/468; Training loss:0.100237\n",
      "Epoch: 13/20; Batch:387/468; Training loss:0.0951154\n",
      "Epoch: 13/20; Batch:388/468; Training loss:0.096442\n",
      "Epoch: 13/20; Batch:389/468; Training loss:0.0990186\n",
      "Epoch: 13/20; Batch:390/468; Training loss:0.104749\n",
      "Epoch: 13/20; Batch:391/468; Training loss:0.0976547\n",
      "Epoch: 13/20; Batch:392/468; Training loss:0.0963863\n",
      "Epoch: 13/20; Batch:393/468; Training loss:0.100247\n",
      "Epoch: 13/20; Batch:394/468; Training loss:0.0994901\n",
      "Epoch: 13/20; Batch:395/468; Training loss:0.0995624\n",
      "Epoch: 13/20; Batch:396/468; Training loss:0.0967128\n",
      "Epoch: 13/20; Batch:397/468; Training loss:0.0941433\n",
      "Epoch: 13/20; Batch:398/468; Training loss:0.0971572\n",
      "Epoch: 13/20; Batch:399/468; Training loss:0.096139\n",
      "Epoch: 13/20; Batch:400/468; Training loss:0.091527\n",
      "Epoch: 13/20; Batch:401/468; Training loss:0.102199\n",
      "Epoch: 13/20; Batch:402/468; Training loss:0.0958839\n",
      "Epoch: 13/20; Batch:403/468; Training loss:0.0986103\n",
      "Epoch: 13/20; Batch:404/468; Training loss:0.0965735\n",
      "Epoch: 13/20; Batch:405/468; Training loss:0.096235\n",
      "Epoch: 13/20; Batch:406/468; Training loss:0.100665\n",
      "Epoch: 13/20; Batch:407/468; Training loss:0.0982626\n",
      "Epoch: 13/20; Batch:408/468; Training loss:0.0956645\n",
      "Epoch: 13/20; Batch:409/468; Training loss:0.0998482\n",
      "Epoch: 13/20; Batch:410/468; Training loss:0.0985269\n",
      "Epoch: 13/20; Batch:411/468; Training loss:0.0993507\n",
      "Epoch: 13/20; Batch:412/468; Training loss:0.101061\n",
      "Epoch: 13/20; Batch:413/468; Training loss:0.10003\n",
      "Epoch: 13/20; Batch:414/468; Training loss:0.0987476\n",
      "Epoch: 13/20; Batch:415/468; Training loss:0.0985467\n",
      "Epoch: 13/20; Batch:416/468; Training loss:0.100278\n",
      "Epoch: 13/20; Batch:417/468; Training loss:0.102914\n",
      "Epoch: 13/20; Batch:418/468; Training loss:0.0962476\n",
      "Epoch: 13/20; Batch:419/468; Training loss:0.0964029\n",
      "Epoch: 13/20; Batch:420/468; Training loss:0.0986145\n",
      "Epoch: 13/20; Batch:421/468; Training loss:0.0972128\n",
      "Epoch: 13/20; Batch:422/468; Training loss:0.0950102\n",
      "Epoch: 13/20; Batch:423/468; Training loss:0.0973915\n",
      "Epoch: 13/20; Batch:424/468; Training loss:0.0983013\n",
      "Epoch: 13/20; Batch:425/468; Training loss:0.100436\n",
      "Epoch: 13/20; Batch:426/468; Training loss:0.0932993\n",
      "Epoch: 13/20; Batch:427/468; Training loss:0.0981318\n",
      "Epoch: 13/20; Batch:428/468; Training loss:0.0932628\n",
      "Epoch: 13/20; Batch:429/468; Training loss:0.0989219\n",
      "Epoch: 13/20; Batch:430/468; Training loss:0.0945717\n",
      "Epoch: 13/20; Batch:431/468; Training loss:0.0964897\n",
      "Epoch: 13/20; Batch:432/468; Training loss:0.0969517\n",
      "Epoch: 13/20; Batch:433/468; Training loss:0.0996278\n",
      "Epoch: 13/20; Batch:434/468; Training loss:0.0949045\n",
      "Epoch: 13/20; Batch:435/468; Training loss:0.096578\n",
      "Epoch: 13/20; Batch:436/468; Training loss:0.0940811\n",
      "Epoch: 13/20; Batch:437/468; Training loss:0.0962262\n",
      "Epoch: 13/20; Batch:438/468; Training loss:0.0994836\n",
      "Epoch: 13/20; Batch:439/468; Training loss:0.0976228\n",
      "Epoch: 13/20; Batch:440/468; Training loss:0.0968522\n",
      "Epoch: 13/20; Batch:441/468; Training loss:0.0980267\n",
      "Epoch: 13/20; Batch:442/468; Training loss:0.101465\n",
      "Epoch: 13/20; Batch:443/468; Training loss:0.0961023\n",
      "Epoch: 13/20; Batch:444/468; Training loss:0.0968395\n",
      "Epoch: 13/20; Batch:445/468; Training loss:0.099907\n",
      "Epoch: 13/20; Batch:446/468; Training loss:0.0931884\n",
      "Epoch: 13/20; Batch:447/468; Training loss:0.0966987\n",
      "Epoch: 13/20; Batch:448/468; Training loss:0.0947714\n",
      "Epoch: 13/20; Batch:449/468; Training loss:0.0972352\n",
      "Epoch: 13/20; Batch:450/468; Training loss:0.0983666\n",
      "Epoch: 13/20; Batch:451/468; Training loss:0.0965435\n",
      "Epoch: 13/20; Batch:452/468; Training loss:0.0954245\n",
      "Epoch: 13/20; Batch:453/468; Training loss:0.0973362\n",
      "Epoch: 13/20; Batch:454/468; Training loss:0.0997364\n",
      "Epoch: 13/20; Batch:455/468; Training loss:0.0961401\n",
      "Epoch: 13/20; Batch:456/468; Training loss:0.0945133\n",
      "Epoch: 13/20; Batch:457/468; Training loss:0.092498\n",
      "Epoch: 13/20; Batch:458/468; Training loss:0.0936552\n",
      "Epoch: 13/20; Batch:459/468; Training loss:0.0961737\n",
      "Epoch: 13/20; Batch:460/468; Training loss:0.0972777\n",
      "Epoch: 13/20; Batch:461/468; Training loss:0.0978274\n",
      "Epoch: 13/20; Batch:462/468; Training loss:0.0965571\n",
      "Epoch: 13/20; Batch:463/468; Training loss:0.103092\n",
      "Epoch: 13/20; Batch:464/468; Training loss:0.0944098\n",
      "Epoch: 13/20; Batch:465/468; Training loss:0.0980973\n",
      "Epoch: 13/20; Batch:466/468; Training loss:0.0943708\n",
      "Epoch: 13/20; Batch:467/468; Training loss:0.0971654\n",
      "Epoch: 13/20; Batch:468/468; Training loss:0.0965061\n",
      "Epoch: 14/20; Batch:1/468; Training loss:0.0994307\n",
      "Epoch: 14/20; Batch:2/468; Training loss:0.0976235\n",
      "Epoch: 14/20; Batch:3/468; Training loss:0.0956295\n",
      "Epoch: 14/20; Batch:4/468; Training loss:0.0973776\n",
      "Epoch: 14/20; Batch:5/468; Training loss:0.0985189\n",
      "Epoch: 14/20; Batch:6/468; Training loss:0.0953054\n",
      "Epoch: 14/20; Batch:7/468; Training loss:0.101309\n",
      "Epoch: 14/20; Batch:8/468; Training loss:0.0979091\n",
      "Epoch: 14/20; Batch:9/468; Training loss:0.0977953\n",
      "Epoch: 14/20; Batch:10/468; Training loss:0.0985458\n",
      "Epoch: 14/20; Batch:11/468; Training loss:0.0963762\n",
      "Epoch: 14/20; Batch:12/468; Training loss:0.0966497\n",
      "Epoch: 14/20; Batch:13/468; Training loss:0.0967006\n",
      "Epoch: 14/20; Batch:14/468; Training loss:0.0937245\n",
      "Epoch: 14/20; Batch:15/468; Training loss:0.0972164\n",
      "Epoch: 14/20; Batch:16/468; Training loss:0.0998224\n",
      "Epoch: 14/20; Batch:17/468; Training loss:0.100779\n",
      "Epoch: 14/20; Batch:18/468; Training loss:0.10024\n",
      "Epoch: 14/20; Batch:19/468; Training loss:0.0974063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20; Batch:20/468; Training loss:0.0964246\n",
      "Epoch: 14/20; Batch:21/468; Training loss:0.0961723\n",
      "Epoch: 14/20; Batch:22/468; Training loss:0.0998241\n",
      "Epoch: 14/20; Batch:23/468; Training loss:0.0961414\n",
      "Epoch: 14/20; Batch:24/468; Training loss:0.0992397\n",
      "Epoch: 14/20; Batch:25/468; Training loss:0.0951383\n",
      "Epoch: 14/20; Batch:26/468; Training loss:0.0989571\n",
      "Epoch: 14/20; Batch:27/468; Training loss:0.0968309\n",
      "Epoch: 14/20; Batch:28/468; Training loss:0.0937023\n",
      "Epoch: 14/20; Batch:29/468; Training loss:0.0972915\n",
      "Epoch: 14/20; Batch:30/468; Training loss:0.100282\n",
      "Epoch: 14/20; Batch:31/468; Training loss:0.0948764\n",
      "Epoch: 14/20; Batch:32/468; Training loss:0.0992343\n",
      "Epoch: 14/20; Batch:33/468; Training loss:0.0949899\n",
      "Epoch: 14/20; Batch:34/468; Training loss:0.0961546\n",
      "Epoch: 14/20; Batch:35/468; Training loss:0.0964752\n",
      "Epoch: 14/20; Batch:36/468; Training loss:0.0996086\n",
      "Epoch: 14/20; Batch:37/468; Training loss:0.100554\n",
      "Epoch: 14/20; Batch:38/468; Training loss:0.0971189\n",
      "Epoch: 14/20; Batch:39/468; Training loss:0.0963718\n",
      "Epoch: 14/20; Batch:40/468; Training loss:0.10101\n",
      "Epoch: 14/20; Batch:41/468; Training loss:0.0952715\n",
      "Epoch: 14/20; Batch:42/468; Training loss:0.0937215\n",
      "Epoch: 14/20; Batch:43/468; Training loss:0.0985992\n",
      "Epoch: 14/20; Batch:44/468; Training loss:0.101259\n",
      "Epoch: 14/20; Batch:45/468; Training loss:0.0954467\n",
      "Epoch: 14/20; Batch:46/468; Training loss:0.0981854\n",
      "Epoch: 14/20; Batch:47/468; Training loss:0.0964865\n",
      "Epoch: 14/20; Batch:48/468; Training loss:0.0956187\n",
      "Epoch: 14/20; Batch:49/468; Training loss:0.0965515\n",
      "Epoch: 14/20; Batch:50/468; Training loss:0.09716\n",
      "Epoch: 14/20; Batch:51/468; Training loss:0.0920918\n",
      "Epoch: 14/20; Batch:52/468; Training loss:0.0954316\n",
      "Epoch: 14/20; Batch:53/468; Training loss:0.095111\n",
      "Epoch: 14/20; Batch:54/468; Training loss:0.099963\n",
      "Epoch: 14/20; Batch:55/468; Training loss:0.0974026\n",
      "Epoch: 14/20; Batch:56/468; Training loss:0.0978347\n",
      "Epoch: 14/20; Batch:57/468; Training loss:0.100478\n",
      "Epoch: 14/20; Batch:58/468; Training loss:0.0974897\n",
      "Epoch: 14/20; Batch:59/468; Training loss:0.0980682\n",
      "Epoch: 14/20; Batch:60/468; Training loss:0.0964566\n",
      "Epoch: 14/20; Batch:61/468; Training loss:0.0983229\n",
      "Epoch: 14/20; Batch:62/468; Training loss:0.0943161\n",
      "Epoch: 14/20; Batch:63/468; Training loss:0.0951218\n",
      "Epoch: 14/20; Batch:64/468; Training loss:0.0963508\n",
      "Epoch: 14/20; Batch:65/468; Training loss:0.0966834\n",
      "Epoch: 14/20; Batch:66/468; Training loss:0.0950068\n",
      "Epoch: 14/20; Batch:67/468; Training loss:0.0955192\n",
      "Epoch: 14/20; Batch:68/468; Training loss:0.094447\n",
      "Epoch: 14/20; Batch:69/468; Training loss:0.100691\n",
      "Epoch: 14/20; Batch:70/468; Training loss:0.0966686\n",
      "Epoch: 14/20; Batch:71/468; Training loss:0.0963214\n",
      "Epoch: 14/20; Batch:72/468; Training loss:0.0954777\n",
      "Epoch: 14/20; Batch:73/468; Training loss:0.0969314\n",
      "Epoch: 14/20; Batch:74/468; Training loss:0.0954527\n",
      "Epoch: 14/20; Batch:75/468; Training loss:0.0966357\n",
      "Epoch: 14/20; Batch:76/468; Training loss:0.0935569\n",
      "Epoch: 14/20; Batch:77/468; Training loss:0.0976526\n",
      "Epoch: 14/20; Batch:78/468; Training loss:0.0964978\n",
      "Epoch: 14/20; Batch:79/468; Training loss:0.097189\n",
      "Epoch: 14/20; Batch:80/468; Training loss:0.0967389\n",
      "Epoch: 14/20; Batch:81/468; Training loss:0.099921\n",
      "Epoch: 14/20; Batch:82/468; Training loss:0.0971869\n",
      "Epoch: 14/20; Batch:83/468; Training loss:0.102243\n",
      "Epoch: 14/20; Batch:84/468; Training loss:0.0953357\n",
      "Epoch: 14/20; Batch:85/468; Training loss:0.0973777\n",
      "Epoch: 14/20; Batch:86/468; Training loss:0.0960907\n",
      "Epoch: 14/20; Batch:87/468; Training loss:0.0967734\n",
      "Epoch: 14/20; Batch:88/468; Training loss:0.0987932\n",
      "Epoch: 14/20; Batch:89/468; Training loss:0.0973747\n",
      "Epoch: 14/20; Batch:90/468; Training loss:0.0967547\n",
      "Epoch: 14/20; Batch:91/468; Training loss:0.100053\n",
      "Epoch: 14/20; Batch:92/468; Training loss:0.101289\n",
      "Epoch: 14/20; Batch:93/468; Training loss:0.0992993\n",
      "Epoch: 14/20; Batch:94/468; Training loss:0.0966185\n",
      "Epoch: 14/20; Batch:95/468; Training loss:0.102671\n",
      "Epoch: 14/20; Batch:96/468; Training loss:0.095745\n",
      "Epoch: 14/20; Batch:97/468; Training loss:0.0992725\n",
      "Epoch: 14/20; Batch:98/468; Training loss:0.0973802\n",
      "Epoch: 14/20; Batch:99/468; Training loss:0.0991454\n",
      "Epoch: 14/20; Batch:100/468; Training loss:0.0986116\n",
      "Epoch: 14/20; Batch:101/468; Training loss:0.102149\n",
      "Epoch: 14/20; Batch:102/468; Training loss:0.10003\n",
      "Epoch: 14/20; Batch:103/468; Training loss:0.101768\n",
      "Epoch: 14/20; Batch:104/468; Training loss:0.0960372\n",
      "Epoch: 14/20; Batch:105/468; Training loss:0.100339\n",
      "Epoch: 14/20; Batch:106/468; Training loss:0.100421\n",
      "Epoch: 14/20; Batch:107/468; Training loss:0.094412\n",
      "Epoch: 14/20; Batch:108/468; Training loss:0.0995983\n",
      "Epoch: 14/20; Batch:109/468; Training loss:0.0973561\n",
      "Epoch: 14/20; Batch:110/468; Training loss:0.0979395\n",
      "Epoch: 14/20; Batch:111/468; Training loss:0.0987119\n",
      "Epoch: 14/20; Batch:112/468; Training loss:0.0993186\n",
      "Epoch: 14/20; Batch:113/468; Training loss:0.0999077\n",
      "Epoch: 14/20; Batch:114/468; Training loss:0.0968206\n",
      "Epoch: 14/20; Batch:115/468; Training loss:0.0927791\n",
      "Epoch: 14/20; Batch:116/468; Training loss:0.0947921\n",
      "Epoch: 14/20; Batch:117/468; Training loss:0.0980693\n",
      "Epoch: 14/20; Batch:118/468; Training loss:0.0961661\n",
      "Epoch: 14/20; Batch:119/468; Training loss:0.10048\n",
      "Epoch: 14/20; Batch:120/468; Training loss:0.100405\n",
      "Epoch: 14/20; Batch:121/468; Training loss:0.0970556\n",
      "Epoch: 14/20; Batch:122/468; Training loss:0.0976746\n",
      "Epoch: 14/20; Batch:123/468; Training loss:0.0986566\n",
      "Epoch: 14/20; Batch:124/468; Training loss:0.0926047\n",
      "Epoch: 14/20; Batch:125/468; Training loss:0.0998393\n",
      "Epoch: 14/20; Batch:126/468; Training loss:0.100099\n",
      "Epoch: 14/20; Batch:127/468; Training loss:0.0968219\n",
      "Epoch: 14/20; Batch:128/468; Training loss:0.0951686\n",
      "Epoch: 14/20; Batch:129/468; Training loss:0.0935563\n",
      "Epoch: 14/20; Batch:130/468; Training loss:0.0971522\n",
      "Epoch: 14/20; Batch:131/468; Training loss:0.0963305\n",
      "Epoch: 14/20; Batch:132/468; Training loss:0.0995273\n",
      "Epoch: 14/20; Batch:133/468; Training loss:0.0953861\n",
      "Epoch: 14/20; Batch:134/468; Training loss:0.0992999\n",
      "Epoch: 14/20; Batch:135/468; Training loss:0.0980031\n",
      "Epoch: 14/20; Batch:136/468; Training loss:0.0976964\n",
      "Epoch: 14/20; Batch:137/468; Training loss:0.100427\n",
      "Epoch: 14/20; Batch:138/468; Training loss:0.0987995\n",
      "Epoch: 14/20; Batch:139/468; Training loss:0.0981567\n",
      "Epoch: 14/20; Batch:140/468; Training loss:0.100303\n",
      "Epoch: 14/20; Batch:141/468; Training loss:0.0964071\n",
      "Epoch: 14/20; Batch:142/468; Training loss:0.0972473\n",
      "Epoch: 14/20; Batch:143/468; Training loss:0.0977778\n",
      "Epoch: 14/20; Batch:144/468; Training loss:0.0956797\n",
      "Epoch: 14/20; Batch:145/468; Training loss:0.0976641\n",
      "Epoch: 14/20; Batch:146/468; Training loss:0.0962755\n",
      "Epoch: 14/20; Batch:147/468; Training loss:0.102633\n",
      "Epoch: 14/20; Batch:148/468; Training loss:0.0937327\n",
      "Epoch: 14/20; Batch:149/468; Training loss:0.101233\n",
      "Epoch: 14/20; Batch:150/468; Training loss:0.0926773\n",
      "Epoch: 14/20; Batch:151/468; Training loss:0.0999666\n",
      "Epoch: 14/20; Batch:152/468; Training loss:0.094557\n",
      "Epoch: 14/20; Batch:153/468; Training loss:0.0974136\n",
      "Epoch: 14/20; Batch:154/468; Training loss:0.097635\n",
      "Epoch: 14/20; Batch:155/468; Training loss:0.0988353\n",
      "Epoch: 14/20; Batch:156/468; Training loss:0.096951\n",
      "Epoch: 14/20; Batch:157/468; Training loss:0.0958312\n",
      "Epoch: 14/20; Batch:158/468; Training loss:0.0975606\n",
      "Epoch: 14/20; Batch:159/468; Training loss:0.098016\n",
      "Epoch: 14/20; Batch:160/468; Training loss:0.0954536\n",
      "Epoch: 14/20; Batch:161/468; Training loss:0.0987954\n",
      "Epoch: 14/20; Batch:162/468; Training loss:0.0948845\n",
      "Epoch: 14/20; Batch:163/468; Training loss:0.0993047\n",
      "Epoch: 14/20; Batch:164/468; Training loss:0.095788\n",
      "Epoch: 14/20; Batch:165/468; Training loss:0.0977309\n",
      "Epoch: 14/20; Batch:166/468; Training loss:0.0925292\n",
      "Epoch: 14/20; Batch:167/468; Training loss:0.100114\n",
      "Epoch: 14/20; Batch:168/468; Training loss:0.0946404\n",
      "Epoch: 14/20; Batch:169/468; Training loss:0.102664\n",
      "Epoch: 14/20; Batch:170/468; Training loss:0.0937367\n",
      "Epoch: 14/20; Batch:171/468; Training loss:0.0973731\n",
      "Epoch: 14/20; Batch:172/468; Training loss:0.0971112\n",
      "Epoch: 14/20; Batch:173/468; Training loss:0.100733\n",
      "Epoch: 14/20; Batch:174/468; Training loss:0.0987988\n",
      "Epoch: 14/20; Batch:175/468; Training loss:0.098622\n",
      "Epoch: 14/20; Batch:176/468; Training loss:0.0989011\n",
      "Epoch: 14/20; Batch:177/468; Training loss:0.0972639\n",
      "Epoch: 14/20; Batch:178/468; Training loss:0.10058\n",
      "Epoch: 14/20; Batch:179/468; Training loss:0.0966421\n",
      "Epoch: 14/20; Batch:180/468; Training loss:0.097\n",
      "Epoch: 14/20; Batch:181/468; Training loss:0.10087\n",
      "Epoch: 14/20; Batch:182/468; Training loss:0.103259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20; Batch:183/468; Training loss:0.0966865\n",
      "Epoch: 14/20; Batch:184/468; Training loss:0.0981881\n",
      "Epoch: 14/20; Batch:185/468; Training loss:0.0977582\n",
      "Epoch: 14/20; Batch:186/468; Training loss:0.096043\n",
      "Epoch: 14/20; Batch:187/468; Training loss:0.0951401\n",
      "Epoch: 14/20; Batch:188/468; Training loss:0.0963487\n",
      "Epoch: 14/20; Batch:189/468; Training loss:0.0977639\n",
      "Epoch: 14/20; Batch:190/468; Training loss:0.0988465\n",
      "Epoch: 14/20; Batch:191/468; Training loss:0.0985546\n",
      "Epoch: 14/20; Batch:192/468; Training loss:0.0948125\n",
      "Epoch: 14/20; Batch:193/468; Training loss:0.0981467\n",
      "Epoch: 14/20; Batch:194/468; Training loss:0.0990179\n",
      "Epoch: 14/20; Batch:195/468; Training loss:0.0963122\n",
      "Epoch: 14/20; Batch:196/468; Training loss:0.097084\n",
      "Epoch: 14/20; Batch:197/468; Training loss:0.0994996\n",
      "Epoch: 14/20; Batch:198/468; Training loss:0.0957613\n",
      "Epoch: 14/20; Batch:199/468; Training loss:0.101294\n",
      "Epoch: 14/20; Batch:200/468; Training loss:0.0945582\n",
      "Epoch: 14/20; Batch:201/468; Training loss:0.0999424\n",
      "Epoch: 14/20; Batch:202/468; Training loss:0.0994423\n",
      "Epoch: 14/20; Batch:203/468; Training loss:0.0949907\n",
      "Epoch: 14/20; Batch:204/468; Training loss:0.102355\n",
      "Epoch: 14/20; Batch:205/468; Training loss:0.0968346\n",
      "Epoch: 14/20; Batch:206/468; Training loss:0.100397\n",
      "Epoch: 14/20; Batch:207/468; Training loss:0.096841\n",
      "Epoch: 14/20; Batch:208/468; Training loss:0.0960235\n",
      "Epoch: 14/20; Batch:209/468; Training loss:0.0949759\n",
      "Epoch: 14/20; Batch:210/468; Training loss:0.0989243\n",
      "Epoch: 14/20; Batch:211/468; Training loss:0.0940622\n",
      "Epoch: 14/20; Batch:212/468; Training loss:0.0946357\n",
      "Epoch: 14/20; Batch:213/468; Training loss:0.0983473\n",
      "Epoch: 14/20; Batch:214/468; Training loss:0.101309\n",
      "Epoch: 14/20; Batch:215/468; Training loss:0.0964553\n",
      "Epoch: 14/20; Batch:216/468; Training loss:0.0995348\n",
      "Epoch: 14/20; Batch:217/468; Training loss:0.0956793\n",
      "Epoch: 14/20; Batch:218/468; Training loss:0.0968261\n",
      "Epoch: 14/20; Batch:219/468; Training loss:0.0948101\n",
      "Epoch: 14/20; Batch:220/468; Training loss:0.0979495\n",
      "Epoch: 14/20; Batch:221/468; Training loss:0.100874\n",
      "Epoch: 14/20; Batch:222/468; Training loss:0.0966824\n",
      "Epoch: 14/20; Batch:223/468; Training loss:0.097619\n",
      "Epoch: 14/20; Batch:224/468; Training loss:0.0952006\n",
      "Epoch: 14/20; Batch:225/468; Training loss:0.0956577\n",
      "Epoch: 14/20; Batch:226/468; Training loss:0.0948416\n",
      "Epoch: 14/20; Batch:227/468; Training loss:0.100421\n",
      "Epoch: 14/20; Batch:228/468; Training loss:0.0988806\n",
      "Epoch: 14/20; Batch:229/468; Training loss:0.0957831\n",
      "Epoch: 14/20; Batch:230/468; Training loss:0.101932\n",
      "Epoch: 14/20; Batch:231/468; Training loss:0.0961785\n",
      "Epoch: 14/20; Batch:232/468; Training loss:0.0986946\n",
      "Epoch: 14/20; Batch:233/468; Training loss:0.0986449\n",
      "Epoch: 14/20; Batch:234/468; Training loss:0.0974955\n",
      "Epoch: 14/20; Batch:235/468; Training loss:0.100809\n",
      "Epoch: 14/20; Batch:236/468; Training loss:0.101588\n",
      "Epoch: 14/20; Batch:237/468; Training loss:0.0963456\n",
      "Epoch: 14/20; Batch:238/468; Training loss:0.0985009\n",
      "Epoch: 14/20; Batch:239/468; Training loss:0.099017\n",
      "Epoch: 14/20; Batch:240/468; Training loss:0.0943075\n",
      "Epoch: 14/20; Batch:241/468; Training loss:0.0957636\n",
      "Epoch: 14/20; Batch:242/468; Training loss:0.0978213\n",
      "Epoch: 14/20; Batch:243/468; Training loss:0.0964686\n",
      "Epoch: 14/20; Batch:244/468; Training loss:0.0975829\n",
      "Epoch: 14/20; Batch:245/468; Training loss:0.095068\n",
      "Epoch: 14/20; Batch:246/468; Training loss:0.0953204\n",
      "Epoch: 14/20; Batch:247/468; Training loss:0.0943095\n",
      "Epoch: 14/20; Batch:248/468; Training loss:0.0947323\n",
      "Epoch: 14/20; Batch:249/468; Training loss:0.094561\n",
      "Epoch: 14/20; Batch:250/468; Training loss:0.0985585\n",
      "Epoch: 14/20; Batch:251/468; Training loss:0.0956974\n",
      "Epoch: 14/20; Batch:252/468; Training loss:0.0964034\n",
      "Epoch: 14/20; Batch:253/468; Training loss:0.0974774\n",
      "Epoch: 14/20; Batch:254/468; Training loss:0.0985864\n",
      "Epoch: 14/20; Batch:255/468; Training loss:0.0969795\n",
      "Epoch: 14/20; Batch:256/468; Training loss:0.0997085\n",
      "Epoch: 14/20; Batch:257/468; Training loss:0.0954503\n",
      "Epoch: 14/20; Batch:258/468; Training loss:0.0967398\n",
      "Epoch: 14/20; Batch:259/468; Training loss:0.0998254\n",
      "Epoch: 14/20; Batch:260/468; Training loss:0.0965667\n",
      "Epoch: 14/20; Batch:261/468; Training loss:0.0978508\n",
      "Epoch: 14/20; Batch:262/468; Training loss:0.0959402\n",
      "Epoch: 14/20; Batch:263/468; Training loss:0.0992743\n",
      "Epoch: 14/20; Batch:264/468; Training loss:0.0923831\n",
      "Epoch: 14/20; Batch:265/468; Training loss:0.0978859\n",
      "Epoch: 14/20; Batch:266/468; Training loss:0.0949764\n",
      "Epoch: 14/20; Batch:267/468; Training loss:0.0931314\n",
      "Epoch: 14/20; Batch:268/468; Training loss:0.0992414\n",
      "Epoch: 14/20; Batch:269/468; Training loss:0.0983414\n",
      "Epoch: 14/20; Batch:270/468; Training loss:0.102476\n",
      "Epoch: 14/20; Batch:271/468; Training loss:0.0988525\n",
      "Epoch: 14/20; Batch:272/468; Training loss:0.0972183\n",
      "Epoch: 14/20; Batch:273/468; Training loss:0.101248\n",
      "Epoch: 14/20; Batch:274/468; Training loss:0.0944074\n",
      "Epoch: 14/20; Batch:275/468; Training loss:0.0969975\n",
      "Epoch: 14/20; Batch:276/468; Training loss:0.0945222\n",
      "Epoch: 14/20; Batch:277/468; Training loss:0.0988236\n",
      "Epoch: 14/20; Batch:278/468; Training loss:0.0964753\n",
      "Epoch: 14/20; Batch:279/468; Training loss:0.095505\n",
      "Epoch: 14/20; Batch:280/468; Training loss:0.09707\n",
      "Epoch: 14/20; Batch:281/468; Training loss:0.0944874\n",
      "Epoch: 14/20; Batch:282/468; Training loss:0.0970786\n",
      "Epoch: 14/20; Batch:283/468; Training loss:0.0971902\n",
      "Epoch: 14/20; Batch:284/468; Training loss:0.0945418\n",
      "Epoch: 14/20; Batch:285/468; Training loss:0.0986919\n",
      "Epoch: 14/20; Batch:286/468; Training loss:0.0993069\n",
      "Epoch: 14/20; Batch:287/468; Training loss:0.0950586\n",
      "Epoch: 14/20; Batch:288/468; Training loss:0.0978386\n",
      "Epoch: 14/20; Batch:289/468; Training loss:0.0982548\n",
      "Epoch: 14/20; Batch:290/468; Training loss:0.0975289\n",
      "Epoch: 14/20; Batch:291/468; Training loss:0.0987107\n",
      "Epoch: 14/20; Batch:292/468; Training loss:0.0953943\n",
      "Epoch: 14/20; Batch:293/468; Training loss:0.0927457\n",
      "Epoch: 14/20; Batch:294/468; Training loss:0.097678\n",
      "Epoch: 14/20; Batch:295/468; Training loss:0.096128\n",
      "Epoch: 14/20; Batch:296/468; Training loss:0.0992855\n",
      "Epoch: 14/20; Batch:297/468; Training loss:0.100914\n",
      "Epoch: 14/20; Batch:298/468; Training loss:0.0970779\n",
      "Epoch: 14/20; Batch:299/468; Training loss:0.0965886\n",
      "Epoch: 14/20; Batch:300/468; Training loss:0.0978977\n",
      "Epoch: 14/20; Batch:301/468; Training loss:0.0941374\n",
      "Epoch: 14/20; Batch:302/468; Training loss:0.0994437\n",
      "Epoch: 14/20; Batch:303/468; Training loss:0.0990294\n",
      "Epoch: 14/20; Batch:304/468; Training loss:0.0937263\n",
      "Epoch: 14/20; Batch:305/468; Training loss:0.0993304\n",
      "Epoch: 14/20; Batch:306/468; Training loss:0.0978257\n",
      "Epoch: 14/20; Batch:307/468; Training loss:0.0971389\n",
      "Epoch: 14/20; Batch:308/468; Training loss:0.0981431\n",
      "Epoch: 14/20; Batch:309/468; Training loss:0.096858\n",
      "Epoch: 14/20; Batch:310/468; Training loss:0.0959558\n",
      "Epoch: 14/20; Batch:311/468; Training loss:0.0958974\n",
      "Epoch: 14/20; Batch:312/468; Training loss:0.0962521\n",
      "Epoch: 14/20; Batch:313/468; Training loss:0.0958255\n",
      "Epoch: 14/20; Batch:314/468; Training loss:0.095634\n",
      "Epoch: 14/20; Batch:315/468; Training loss:0.0957569\n",
      "Epoch: 14/20; Batch:316/468; Training loss:0.0966482\n",
      "Epoch: 14/20; Batch:317/468; Training loss:0.0979102\n",
      "Epoch: 14/20; Batch:318/468; Training loss:0.101687\n",
      "Epoch: 14/20; Batch:319/468; Training loss:0.0986134\n",
      "Epoch: 14/20; Batch:320/468; Training loss:0.0958283\n",
      "Epoch: 14/20; Batch:321/468; Training loss:0.0971396\n",
      "Epoch: 14/20; Batch:322/468; Training loss:0.0986486\n",
      "Epoch: 14/20; Batch:323/468; Training loss:0.0937951\n",
      "Epoch: 14/20; Batch:324/468; Training loss:0.0981287\n",
      "Epoch: 14/20; Batch:325/468; Training loss:0.0995083\n",
      "Epoch: 14/20; Batch:326/468; Training loss:0.0993727\n",
      "Epoch: 14/20; Batch:327/468; Training loss:0.097355\n",
      "Epoch: 14/20; Batch:328/468; Training loss:0.0945217\n",
      "Epoch: 14/20; Batch:329/468; Training loss:0.0988764\n",
      "Epoch: 14/20; Batch:330/468; Training loss:0.0952329\n",
      "Epoch: 14/20; Batch:331/468; Training loss:0.093127\n",
      "Epoch: 14/20; Batch:332/468; Training loss:0.0957179\n",
      "Epoch: 14/20; Batch:333/468; Training loss:0.0957482\n",
      "Epoch: 14/20; Batch:334/468; Training loss:0.0960281\n",
      "Epoch: 14/20; Batch:335/468; Training loss:0.0965116\n",
      "Epoch: 14/20; Batch:336/468; Training loss:0.0971363\n",
      "Epoch: 14/20; Batch:337/468; Training loss:0.0988273\n",
      "Epoch: 14/20; Batch:338/468; Training loss:0.0999104\n",
      "Epoch: 14/20; Batch:339/468; Training loss:0.0925215\n",
      "Epoch: 14/20; Batch:340/468; Training loss:0.0991044\n",
      "Epoch: 14/20; Batch:341/468; Training loss:0.0962831\n",
      "Epoch: 14/20; Batch:342/468; Training loss:0.0948311\n",
      "Epoch: 14/20; Batch:343/468; Training loss:0.0997766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20; Batch:344/468; Training loss:0.0973832\n",
      "Epoch: 14/20; Batch:345/468; Training loss:0.0991059\n",
      "Epoch: 14/20; Batch:346/468; Training loss:0.0972343\n",
      "Epoch: 14/20; Batch:347/468; Training loss:0.0928821\n",
      "Epoch: 14/20; Batch:348/468; Training loss:0.101391\n",
      "Epoch: 14/20; Batch:349/468; Training loss:0.0987546\n",
      "Epoch: 14/20; Batch:350/468; Training loss:0.0971087\n",
      "Epoch: 14/20; Batch:351/468; Training loss:0.100778\n",
      "Epoch: 14/20; Batch:352/468; Training loss:0.0964813\n",
      "Epoch: 14/20; Batch:353/468; Training loss:0.0973081\n",
      "Epoch: 14/20; Batch:354/468; Training loss:0.0916968\n",
      "Epoch: 14/20; Batch:355/468; Training loss:0.0946832\n",
      "Epoch: 14/20; Batch:356/468; Training loss:0.101582\n",
      "Epoch: 14/20; Batch:357/468; Training loss:0.0971868\n",
      "Epoch: 14/20; Batch:358/468; Training loss:0.0971094\n",
      "Epoch: 14/20; Batch:359/468; Training loss:0.100012\n",
      "Epoch: 14/20; Batch:360/468; Training loss:0.0969006\n",
      "Epoch: 14/20; Batch:361/468; Training loss:0.0999441\n",
      "Epoch: 14/20; Batch:362/468; Training loss:0.0985874\n",
      "Epoch: 14/20; Batch:363/468; Training loss:0.0959024\n",
      "Epoch: 14/20; Batch:364/468; Training loss:0.10051\n",
      "Epoch: 14/20; Batch:365/468; Training loss:0.0950368\n",
      "Epoch: 14/20; Batch:366/468; Training loss:0.0953071\n",
      "Epoch: 14/20; Batch:367/468; Training loss:0.0961811\n",
      "Epoch: 14/20; Batch:368/468; Training loss:0.0983892\n",
      "Epoch: 14/20; Batch:369/468; Training loss:0.0983499\n",
      "Epoch: 14/20; Batch:370/468; Training loss:0.0997601\n",
      "Epoch: 14/20; Batch:371/468; Training loss:0.0999653\n",
      "Epoch: 14/20; Batch:372/468; Training loss:0.0963328\n",
      "Epoch: 14/20; Batch:373/468; Training loss:0.100286\n",
      "Epoch: 14/20; Batch:374/468; Training loss:0.0963901\n",
      "Epoch: 14/20; Batch:375/468; Training loss:0.0974231\n",
      "Epoch: 14/20; Batch:376/468; Training loss:0.0967061\n",
      "Epoch: 14/20; Batch:377/468; Training loss:0.0978313\n",
      "Epoch: 14/20; Batch:378/468; Training loss:0.0969619\n",
      "Epoch: 14/20; Batch:379/468; Training loss:0.097225\n",
      "Epoch: 14/20; Batch:380/468; Training loss:0.0953415\n",
      "Epoch: 14/20; Batch:381/468; Training loss:0.100154\n",
      "Epoch: 14/20; Batch:382/468; Training loss:0.0957633\n",
      "Epoch: 14/20; Batch:383/468; Training loss:0.0982476\n",
      "Epoch: 14/20; Batch:384/468; Training loss:0.0977993\n",
      "Epoch: 14/20; Batch:385/468; Training loss:0.0982365\n",
      "Epoch: 14/20; Batch:386/468; Training loss:0.0956231\n",
      "Epoch: 14/20; Batch:387/468; Training loss:0.0985993\n",
      "Epoch: 14/20; Batch:388/468; Training loss:0.097968\n",
      "Epoch: 14/20; Batch:389/468; Training loss:0.0983952\n",
      "Epoch: 14/20; Batch:390/468; Training loss:0.0988764\n",
      "Epoch: 14/20; Batch:391/468; Training loss:0.0933774\n",
      "Epoch: 14/20; Batch:392/468; Training loss:0.0918775\n",
      "Epoch: 14/20; Batch:393/468; Training loss:0.0990115\n",
      "Epoch: 14/20; Batch:394/468; Training loss:0.0984046\n",
      "Epoch: 14/20; Batch:395/468; Training loss:0.0976851\n",
      "Epoch: 14/20; Batch:396/468; Training loss:0.0981844\n",
      "Epoch: 14/20; Batch:397/468; Training loss:0.102652\n",
      "Epoch: 14/20; Batch:398/468; Training loss:0.0991685\n",
      "Epoch: 14/20; Batch:399/468; Training loss:0.0939363\n",
      "Epoch: 14/20; Batch:400/468; Training loss:0.095132\n",
      "Epoch: 14/20; Batch:401/468; Training loss:0.0956608\n",
      "Epoch: 14/20; Batch:402/468; Training loss:0.0935575\n",
      "Epoch: 14/20; Batch:403/468; Training loss:0.0991861\n",
      "Epoch: 14/20; Batch:404/468; Training loss:0.0961063\n",
      "Epoch: 14/20; Batch:405/468; Training loss:0.100113\n",
      "Epoch: 14/20; Batch:406/468; Training loss:0.0961267\n",
      "Epoch: 14/20; Batch:407/468; Training loss:0.100435\n",
      "Epoch: 14/20; Batch:408/468; Training loss:0.0977774\n",
      "Epoch: 14/20; Batch:409/468; Training loss:0.0989612\n",
      "Epoch: 14/20; Batch:410/468; Training loss:0.0978413\n",
      "Epoch: 14/20; Batch:411/468; Training loss:0.098466\n",
      "Epoch: 14/20; Batch:412/468; Training loss:0.0933695\n",
      "Epoch: 14/20; Batch:413/468; Training loss:0.0998806\n",
      "Epoch: 14/20; Batch:414/468; Training loss:0.0941412\n",
      "Epoch: 14/20; Batch:415/468; Training loss:0.0971721\n",
      "Epoch: 14/20; Batch:416/468; Training loss:0.0971142\n",
      "Epoch: 14/20; Batch:417/468; Training loss:0.0986088\n",
      "Epoch: 14/20; Batch:418/468; Training loss:0.0966983\n",
      "Epoch: 14/20; Batch:419/468; Training loss:0.0934351\n",
      "Epoch: 14/20; Batch:420/468; Training loss:0.0910699\n",
      "Epoch: 14/20; Batch:421/468; Training loss:0.0949403\n",
      "Epoch: 14/20; Batch:422/468; Training loss:0.0963541\n",
      "Epoch: 14/20; Batch:423/468; Training loss:0.0966192\n",
      "Epoch: 14/20; Batch:424/468; Training loss:0.0976154\n",
      "Epoch: 14/20; Batch:425/468; Training loss:0.0956258\n",
      "Epoch: 14/20; Batch:426/468; Training loss:0.0993254\n",
      "Epoch: 14/20; Batch:427/468; Training loss:0.0980199\n",
      "Epoch: 14/20; Batch:428/468; Training loss:0.0973929\n",
      "Epoch: 14/20; Batch:429/468; Training loss:0.0952796\n",
      "Epoch: 14/20; Batch:430/468; Training loss:0.0995455\n",
      "Epoch: 14/20; Batch:431/468; Training loss:0.0958996\n",
      "Epoch: 14/20; Batch:432/468; Training loss:0.0982797\n",
      "Epoch: 14/20; Batch:433/468; Training loss:0.0946153\n",
      "Epoch: 14/20; Batch:434/468; Training loss:0.0968547\n",
      "Epoch: 14/20; Batch:435/468; Training loss:0.100873\n",
      "Epoch: 14/20; Batch:436/468; Training loss:0.0987087\n",
      "Epoch: 14/20; Batch:437/468; Training loss:0.0969305\n",
      "Epoch: 14/20; Batch:438/468; Training loss:0.0976649\n",
      "Epoch: 14/20; Batch:439/468; Training loss:0.0998288\n",
      "Epoch: 14/20; Batch:440/468; Training loss:0.09351\n",
      "Epoch: 14/20; Batch:441/468; Training loss:0.0964403\n",
      "Epoch: 14/20; Batch:442/468; Training loss:0.0974067\n",
      "Epoch: 14/20; Batch:443/468; Training loss:0.0969111\n",
      "Epoch: 14/20; Batch:444/468; Training loss:0.09761\n",
      "Epoch: 14/20; Batch:445/468; Training loss:0.0967844\n",
      "Epoch: 14/20; Batch:446/468; Training loss:0.0973444\n",
      "Epoch: 14/20; Batch:447/468; Training loss:0.0939981\n",
      "Epoch: 14/20; Batch:448/468; Training loss:0.0964791\n",
      "Epoch: 14/20; Batch:449/468; Training loss:0.0948217\n",
      "Epoch: 14/20; Batch:450/468; Training loss:0.0982502\n",
      "Epoch: 14/20; Batch:451/468; Training loss:0.0947929\n",
      "Epoch: 14/20; Batch:452/468; Training loss:0.0999712\n",
      "Epoch: 14/20; Batch:453/468; Training loss:0.0973096\n",
      "Epoch: 14/20; Batch:454/468; Training loss:0.0995331\n",
      "Epoch: 14/20; Batch:455/468; Training loss:0.0925442\n",
      "Epoch: 14/20; Batch:456/468; Training loss:0.0981888\n",
      "Epoch: 14/20; Batch:457/468; Training loss:0.0951742\n",
      "Epoch: 14/20; Batch:458/468; Training loss:0.101175\n",
      "Epoch: 14/20; Batch:459/468; Training loss:0.098906\n",
      "Epoch: 14/20; Batch:460/468; Training loss:0.0984385\n",
      "Epoch: 14/20; Batch:461/468; Training loss:0.0955165\n",
      "Epoch: 14/20; Batch:462/468; Training loss:0.096721\n",
      "Epoch: 14/20; Batch:463/468; Training loss:0.101102\n",
      "Epoch: 14/20; Batch:464/468; Training loss:0.0981054\n",
      "Epoch: 14/20; Batch:465/468; Training loss:0.0972977\n",
      "Epoch: 14/20; Batch:466/468; Training loss:0.0958054\n",
      "Epoch: 14/20; Batch:467/468; Training loss:0.0975904\n",
      "Epoch: 14/20; Batch:468/468; Training loss:0.0982018\n",
      "Epoch: 15/20; Batch:1/468; Training loss:0.0967894\n",
      "Epoch: 15/20; Batch:2/468; Training loss:0.0968678\n",
      "Epoch: 15/20; Batch:3/468; Training loss:0.0967748\n",
      "Epoch: 15/20; Batch:4/468; Training loss:0.0972659\n",
      "Epoch: 15/20; Batch:5/468; Training loss:0.0915519\n",
      "Epoch: 15/20; Batch:6/468; Training loss:0.100431\n",
      "Epoch: 15/20; Batch:7/468; Training loss:0.0983607\n",
      "Epoch: 15/20; Batch:8/468; Training loss:0.0984988\n",
      "Epoch: 15/20; Batch:9/468; Training loss:0.0977753\n",
      "Epoch: 15/20; Batch:10/468; Training loss:0.0952378\n",
      "Epoch: 15/20; Batch:11/468; Training loss:0.0957962\n",
      "Epoch: 15/20; Batch:12/468; Training loss:0.0978264\n",
      "Epoch: 15/20; Batch:13/468; Training loss:0.101455\n",
      "Epoch: 15/20; Batch:14/468; Training loss:0.0958435\n",
      "Epoch: 15/20; Batch:15/468; Training loss:0.094279\n",
      "Epoch: 15/20; Batch:16/468; Training loss:0.095183\n",
      "Epoch: 15/20; Batch:17/468; Training loss:0.0931904\n",
      "Epoch: 15/20; Batch:18/468; Training loss:0.0951312\n",
      "Epoch: 15/20; Batch:19/468; Training loss:0.0972128\n",
      "Epoch: 15/20; Batch:20/468; Training loss:0.100311\n",
      "Epoch: 15/20; Batch:21/468; Training loss:0.0968543\n",
      "Epoch: 15/20; Batch:22/468; Training loss:0.0965861\n",
      "Epoch: 15/20; Batch:23/468; Training loss:0.0959147\n",
      "Epoch: 15/20; Batch:24/468; Training loss:0.0968649\n",
      "Epoch: 15/20; Batch:25/468; Training loss:0.0972837\n",
      "Epoch: 15/20; Batch:26/468; Training loss:0.0978802\n",
      "Epoch: 15/20; Batch:27/468; Training loss:0.0989522\n",
      "Epoch: 15/20; Batch:28/468; Training loss:0.0957915\n",
      "Epoch: 15/20; Batch:29/468; Training loss:0.0950078\n",
      "Epoch: 15/20; Batch:30/468; Training loss:0.0985018\n",
      "Epoch: 15/20; Batch:31/468; Training loss:0.0985035\n",
      "Epoch: 15/20; Batch:32/468; Training loss:0.0937984\n",
      "Epoch: 15/20; Batch:33/468; Training loss:0.0994583\n",
      "Epoch: 15/20; Batch:34/468; Training loss:0.0997902\n",
      "Epoch: 15/20; Batch:35/468; Training loss:0.0966473\n",
      "Epoch: 15/20; Batch:36/468; Training loss:0.0941206\n",
      "Epoch: 15/20; Batch:37/468; Training loss:0.0995918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20; Batch:38/468; Training loss:0.0968434\n",
      "Epoch: 15/20; Batch:39/468; Training loss:0.0977323\n",
      "Epoch: 15/20; Batch:40/468; Training loss:0.100789\n",
      "Epoch: 15/20; Batch:41/468; Training loss:0.100205\n",
      "Epoch: 15/20; Batch:42/468; Training loss:0.0986852\n",
      "Epoch: 15/20; Batch:43/468; Training loss:0.0945463\n",
      "Epoch: 15/20; Batch:44/468; Training loss:0.0996452\n",
      "Epoch: 15/20; Batch:45/468; Training loss:0.0897591\n",
      "Epoch: 15/20; Batch:46/468; Training loss:0.0946516\n",
      "Epoch: 15/20; Batch:47/468; Training loss:0.0929038\n",
      "Epoch: 15/20; Batch:48/468; Training loss:0.0968413\n",
      "Epoch: 15/20; Batch:49/468; Training loss:0.0985805\n",
      "Epoch: 15/20; Batch:50/468; Training loss:0.0935918\n",
      "Epoch: 15/20; Batch:51/468; Training loss:0.0944732\n",
      "Epoch: 15/20; Batch:52/468; Training loss:0.0961061\n",
      "Epoch: 15/20; Batch:53/468; Training loss:0.0912595\n",
      "Epoch: 15/20; Batch:54/468; Training loss:0.0981076\n",
      "Epoch: 15/20; Batch:55/468; Training loss:0.0959748\n",
      "Epoch: 15/20; Batch:56/468; Training loss:0.0948875\n",
      "Epoch: 15/20; Batch:57/468; Training loss:0.0947167\n",
      "Epoch: 15/20; Batch:58/468; Training loss:0.100258\n",
      "Epoch: 15/20; Batch:59/468; Training loss:0.0982884\n",
      "Epoch: 15/20; Batch:60/468; Training loss:0.0963539\n",
      "Epoch: 15/20; Batch:61/468; Training loss:0.0960924\n",
      "Epoch: 15/20; Batch:62/468; Training loss:0.0979912\n",
      "Epoch: 15/20; Batch:63/468; Training loss:0.0986688\n",
      "Epoch: 15/20; Batch:64/468; Training loss:0.0940422\n",
      "Epoch: 15/20; Batch:65/468; Training loss:0.0986936\n",
      "Epoch: 15/20; Batch:66/468; Training loss:0.0990904\n",
      "Epoch: 15/20; Batch:67/468; Training loss:0.0991913\n",
      "Epoch: 15/20; Batch:68/468; Training loss:0.0976536\n",
      "Epoch: 15/20; Batch:69/468; Training loss:0.0962317\n",
      "Epoch: 15/20; Batch:70/468; Training loss:0.101909\n",
      "Epoch: 15/20; Batch:71/468; Training loss:0.0985563\n",
      "Epoch: 15/20; Batch:72/468; Training loss:0.0986181\n",
      "Epoch: 15/20; Batch:73/468; Training loss:0.0993127\n",
      "Epoch: 15/20; Batch:74/468; Training loss:0.0982303\n",
      "Epoch: 15/20; Batch:75/468; Training loss:0.0971545\n",
      "Epoch: 15/20; Batch:76/468; Training loss:0.0993374\n",
      "Epoch: 15/20; Batch:77/468; Training loss:0.096452\n",
      "Epoch: 15/20; Batch:78/468; Training loss:0.096092\n",
      "Epoch: 15/20; Batch:79/468; Training loss:0.0976258\n",
      "Epoch: 15/20; Batch:80/468; Training loss:0.0978693\n",
      "Epoch: 15/20; Batch:81/468; Training loss:0.0950724\n",
      "Epoch: 15/20; Batch:82/468; Training loss:0.0991926\n",
      "Epoch: 15/20; Batch:83/468; Training loss:0.101157\n",
      "Epoch: 15/20; Batch:84/468; Training loss:0.0986939\n",
      "Epoch: 15/20; Batch:85/468; Training loss:0.0983534\n",
      "Epoch: 15/20; Batch:86/468; Training loss:0.0982908\n",
      "Epoch: 15/20; Batch:87/468; Training loss:0.0934767\n",
      "Epoch: 15/20; Batch:88/468; Training loss:0.0941583\n",
      "Epoch: 15/20; Batch:89/468; Training loss:0.100196\n",
      "Epoch: 15/20; Batch:90/468; Training loss:0.0961144\n",
      "Epoch: 15/20; Batch:91/468; Training loss:0.0952735\n",
      "Epoch: 15/20; Batch:92/468; Training loss:0.0940458\n",
      "Epoch: 15/20; Batch:93/468; Training loss:0.102059\n",
      "Epoch: 15/20; Batch:94/468; Training loss:0.0987799\n",
      "Epoch: 15/20; Batch:95/468; Training loss:0.096211\n",
      "Epoch: 15/20; Batch:96/468; Training loss:0.0974279\n",
      "Epoch: 15/20; Batch:97/468; Training loss:0.0930574\n",
      "Epoch: 15/20; Batch:98/468; Training loss:0.0978077\n",
      "Epoch: 15/20; Batch:99/468; Training loss:0.0954565\n",
      "Epoch: 15/20; Batch:100/468; Training loss:0.0973978\n",
      "Epoch: 15/20; Batch:101/468; Training loss:0.101229\n",
      "Epoch: 15/20; Batch:102/468; Training loss:0.097636\n",
      "Epoch: 15/20; Batch:103/468; Training loss:0.094248\n",
      "Epoch: 15/20; Batch:104/468; Training loss:0.0974365\n",
      "Epoch: 15/20; Batch:105/468; Training loss:0.0943006\n",
      "Epoch: 15/20; Batch:106/468; Training loss:0.0980935\n",
      "Epoch: 15/20; Batch:107/468; Training loss:0.0946799\n",
      "Epoch: 15/20; Batch:108/468; Training loss:0.0947632\n",
      "Epoch: 15/20; Batch:109/468; Training loss:0.0964487\n",
      "Epoch: 15/20; Batch:110/468; Training loss:0.0982933\n",
      "Epoch: 15/20; Batch:111/468; Training loss:0.0947534\n",
      "Epoch: 15/20; Batch:112/468; Training loss:0.0936769\n",
      "Epoch: 15/20; Batch:113/468; Training loss:0.0988331\n",
      "Epoch: 15/20; Batch:114/468; Training loss:0.0987243\n",
      "Epoch: 15/20; Batch:115/468; Training loss:0.093753\n",
      "Epoch: 15/20; Batch:116/468; Training loss:0.0949941\n",
      "Epoch: 15/20; Batch:117/468; Training loss:0.0956517\n",
      "Epoch: 15/20; Batch:118/468; Training loss:0.0964253\n",
      "Epoch: 15/20; Batch:119/468; Training loss:0.0998168\n",
      "Epoch: 15/20; Batch:120/468; Training loss:0.0981167\n",
      "Epoch: 15/20; Batch:121/468; Training loss:0.0981715\n",
      "Epoch: 15/20; Batch:122/468; Training loss:0.0988732\n",
      "Epoch: 15/20; Batch:123/468; Training loss:0.0982672\n",
      "Epoch: 15/20; Batch:124/468; Training loss:0.095436\n",
      "Epoch: 15/20; Batch:125/468; Training loss:0.0905755\n",
      "Epoch: 15/20; Batch:126/468; Training loss:0.0962573\n",
      "Epoch: 15/20; Batch:127/468; Training loss:0.0962807\n",
      "Epoch: 15/20; Batch:128/468; Training loss:0.0986149\n",
      "Epoch: 15/20; Batch:129/468; Training loss:0.0963487\n",
      "Epoch: 15/20; Batch:130/468; Training loss:0.101474\n",
      "Epoch: 15/20; Batch:131/468; Training loss:0.0994502\n",
      "Epoch: 15/20; Batch:132/468; Training loss:0.0999867\n",
      "Epoch: 15/20; Batch:133/468; Training loss:0.100272\n",
      "Epoch: 15/20; Batch:134/468; Training loss:0.0981398\n",
      "Epoch: 15/20; Batch:135/468; Training loss:0.098443\n",
      "Epoch: 15/20; Batch:136/468; Training loss:0.0944336\n",
      "Epoch: 15/20; Batch:137/468; Training loss:0.0988511\n",
      "Epoch: 15/20; Batch:138/468; Training loss:0.101727\n",
      "Epoch: 15/20; Batch:139/468; Training loss:0.0988954\n",
      "Epoch: 15/20; Batch:140/468; Training loss:0.0940332\n",
      "Epoch: 15/20; Batch:141/468; Training loss:0.0963659\n",
      "Epoch: 15/20; Batch:142/468; Training loss:0.100751\n",
      "Epoch: 15/20; Batch:143/468; Training loss:0.0990831\n",
      "Epoch: 15/20; Batch:144/468; Training loss:0.09409\n",
      "Epoch: 15/20; Batch:145/468; Training loss:0.0970115\n",
      "Epoch: 15/20; Batch:146/468; Training loss:0.0999657\n",
      "Epoch: 15/20; Batch:147/468; Training loss:0.0957306\n",
      "Epoch: 15/20; Batch:148/468; Training loss:0.0984389\n",
      "Epoch: 15/20; Batch:149/468; Training loss:0.094955\n",
      "Epoch: 15/20; Batch:150/468; Training loss:0.0956622\n",
      "Epoch: 15/20; Batch:151/468; Training loss:0.09325\n",
      "Epoch: 15/20; Batch:152/468; Training loss:0.0967191\n",
      "Epoch: 15/20; Batch:153/468; Training loss:0.0966667\n",
      "Epoch: 15/20; Batch:154/468; Training loss:0.0939683\n",
      "Epoch: 15/20; Batch:155/468; Training loss:0.0956748\n",
      "Epoch: 15/20; Batch:156/468; Training loss:0.0979275\n",
      "Epoch: 15/20; Batch:157/468; Training loss:0.092622\n",
      "Epoch: 15/20; Batch:158/468; Training loss:0.093586\n",
      "Epoch: 15/20; Batch:159/468; Training loss:0.0970041\n",
      "Epoch: 15/20; Batch:160/468; Training loss:0.101161\n",
      "Epoch: 15/20; Batch:161/468; Training loss:0.0947598\n",
      "Epoch: 15/20; Batch:162/468; Training loss:0.0987255\n",
      "Epoch: 15/20; Batch:163/468; Training loss:0.0935337\n",
      "Epoch: 15/20; Batch:164/468; Training loss:0.0951858\n",
      "Epoch: 15/20; Batch:165/468; Training loss:0.0958831\n",
      "Epoch: 15/20; Batch:166/468; Training loss:0.0928357\n",
      "Epoch: 15/20; Batch:167/468; Training loss:0.0947392\n",
      "Epoch: 15/20; Batch:168/468; Training loss:0.0996584\n",
      "Epoch: 15/20; Batch:169/468; Training loss:0.0942944\n",
      "Epoch: 15/20; Batch:170/468; Training loss:0.0970831\n",
      "Epoch: 15/20; Batch:171/468; Training loss:0.0949454\n",
      "Epoch: 15/20; Batch:172/468; Training loss:0.0946298\n",
      "Epoch: 15/20; Batch:173/468; Training loss:0.0947588\n",
      "Epoch: 15/20; Batch:174/468; Training loss:0.0966857\n",
      "Epoch: 15/20; Batch:175/468; Training loss:0.0947159\n",
      "Epoch: 15/20; Batch:176/468; Training loss:0.102676\n",
      "Epoch: 15/20; Batch:177/468; Training loss:0.0983333\n",
      "Epoch: 15/20; Batch:178/468; Training loss:0.0985604\n",
      "Epoch: 15/20; Batch:179/468; Training loss:0.0955955\n",
      "Epoch: 15/20; Batch:180/468; Training loss:0.0963848\n",
      "Epoch: 15/20; Batch:181/468; Training loss:0.0976599\n",
      "Epoch: 15/20; Batch:182/468; Training loss:0.098415\n",
      "Epoch: 15/20; Batch:183/468; Training loss:0.0964782\n",
      "Epoch: 15/20; Batch:184/468; Training loss:0.0980155\n",
      "Epoch: 15/20; Batch:185/468; Training loss:0.0953507\n",
      "Epoch: 15/20; Batch:186/468; Training loss:0.0979517\n",
      "Epoch: 15/20; Batch:187/468; Training loss:0.0968377\n",
      "Epoch: 15/20; Batch:188/468; Training loss:0.0948573\n",
      "Epoch: 15/20; Batch:189/468; Training loss:0.0989708\n",
      "Epoch: 15/20; Batch:190/468; Training loss:0.100223\n",
      "Epoch: 15/20; Batch:191/468; Training loss:0.0957647\n",
      "Epoch: 15/20; Batch:192/468; Training loss:0.100135\n",
      "Epoch: 15/20; Batch:193/468; Training loss:0.0938572\n",
      "Epoch: 15/20; Batch:194/468; Training loss:0.0948922\n",
      "Epoch: 15/20; Batch:195/468; Training loss:0.094118\n",
      "Epoch: 15/20; Batch:196/468; Training loss:0.0947258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20; Batch:197/468; Training loss:0.0933014\n",
      "Epoch: 15/20; Batch:198/468; Training loss:0.0973795\n",
      "Epoch: 15/20; Batch:199/468; Training loss:0.0985344\n",
      "Epoch: 15/20; Batch:200/468; Training loss:0.0977874\n",
      "Epoch: 15/20; Batch:201/468; Training loss:0.0960429\n",
      "Epoch: 15/20; Batch:202/468; Training loss:0.097475\n",
      "Epoch: 15/20; Batch:203/468; Training loss:0.0966541\n",
      "Epoch: 15/20; Batch:204/468; Training loss:0.0969522\n",
      "Epoch: 15/20; Batch:205/468; Training loss:0.0993038\n",
      "Epoch: 15/20; Batch:206/468; Training loss:0.0943538\n",
      "Epoch: 15/20; Batch:207/468; Training loss:0.0963912\n",
      "Epoch: 15/20; Batch:208/468; Training loss:0.0977288\n",
      "Epoch: 15/20; Batch:209/468; Training loss:0.0987061\n",
      "Epoch: 15/20; Batch:210/468; Training loss:0.0917933\n",
      "Epoch: 15/20; Batch:211/468; Training loss:0.0964254\n",
      "Epoch: 15/20; Batch:212/468; Training loss:0.0981533\n",
      "Epoch: 15/20; Batch:213/468; Training loss:0.0985443\n",
      "Epoch: 15/20; Batch:214/468; Training loss:0.0966258\n",
      "Epoch: 15/20; Batch:215/468; Training loss:0.0974027\n",
      "Epoch: 15/20; Batch:216/468; Training loss:0.0913892\n",
      "Epoch: 15/20; Batch:217/468; Training loss:0.0970195\n",
      "Epoch: 15/20; Batch:218/468; Training loss:0.0946211\n",
      "Epoch: 15/20; Batch:219/468; Training loss:0.0984561\n",
      "Epoch: 15/20; Batch:220/468; Training loss:0.0975312\n",
      "Epoch: 15/20; Batch:221/468; Training loss:0.0962273\n",
      "Epoch: 15/20; Batch:222/468; Training loss:0.0965492\n",
      "Epoch: 15/20; Batch:223/468; Training loss:0.0983424\n",
      "Epoch: 15/20; Batch:224/468; Training loss:0.0977348\n",
      "Epoch: 15/20; Batch:225/468; Training loss:0.0946004\n",
      "Epoch: 15/20; Batch:226/468; Training loss:0.0986035\n",
      "Epoch: 15/20; Batch:227/468; Training loss:0.0984959\n",
      "Epoch: 15/20; Batch:228/468; Training loss:0.0974293\n",
      "Epoch: 15/20; Batch:229/468; Training loss:0.0965383\n",
      "Epoch: 15/20; Batch:230/468; Training loss:0.097475\n",
      "Epoch: 15/20; Batch:231/468; Training loss:0.0979157\n",
      "Epoch: 15/20; Batch:232/468; Training loss:0.094144\n",
      "Epoch: 15/20; Batch:233/468; Training loss:0.0980997\n",
      "Epoch: 15/20; Batch:234/468; Training loss:0.0954692\n",
      "Epoch: 15/20; Batch:235/468; Training loss:0.0957536\n",
      "Epoch: 15/20; Batch:236/468; Training loss:0.096763\n",
      "Epoch: 15/20; Batch:237/468; Training loss:0.0965197\n",
      "Epoch: 15/20; Batch:238/468; Training loss:0.0961054\n",
      "Epoch: 15/20; Batch:239/468; Training loss:0.0951126\n",
      "Epoch: 15/20; Batch:240/468; Training loss:0.0943847\n",
      "Epoch: 15/20; Batch:241/468; Training loss:0.0960795\n",
      "Epoch: 15/20; Batch:242/468; Training loss:0.0984506\n",
      "Epoch: 15/20; Batch:243/468; Training loss:0.0992495\n",
      "Epoch: 15/20; Batch:244/468; Training loss:0.0987932\n",
      "Epoch: 15/20; Batch:245/468; Training loss:0.0971627\n",
      "Epoch: 15/20; Batch:246/468; Training loss:0.0987454\n",
      "Epoch: 15/20; Batch:247/468; Training loss:0.0970344\n",
      "Epoch: 15/20; Batch:248/468; Training loss:0.0986858\n",
      "Epoch: 15/20; Batch:249/468; Training loss:0.0985767\n",
      "Epoch: 15/20; Batch:250/468; Training loss:0.0995763\n",
      "Epoch: 15/20; Batch:251/468; Training loss:0.0984135\n",
      "Epoch: 15/20; Batch:252/468; Training loss:0.10042\n",
      "Epoch: 15/20; Batch:253/468; Training loss:0.0987612\n",
      "Epoch: 15/20; Batch:254/468; Training loss:0.095999\n",
      "Epoch: 15/20; Batch:255/468; Training loss:0.0969461\n",
      "Epoch: 15/20; Batch:256/468; Training loss:0.0935926\n",
      "Epoch: 15/20; Batch:257/468; Training loss:0.0990106\n",
      "Epoch: 15/20; Batch:258/468; Training loss:0.0950381\n",
      "Epoch: 15/20; Batch:259/468; Training loss:0.100066\n",
      "Epoch: 15/20; Batch:260/468; Training loss:0.0926979\n",
      "Epoch: 15/20; Batch:261/468; Training loss:0.0975479\n",
      "Epoch: 15/20; Batch:262/468; Training loss:0.096436\n",
      "Epoch: 15/20; Batch:263/468; Training loss:0.0965936\n",
      "Epoch: 15/20; Batch:264/468; Training loss:0.0956893\n",
      "Epoch: 15/20; Batch:265/468; Training loss:0.0962296\n",
      "Epoch: 15/20; Batch:266/468; Training loss:0.0962164\n",
      "Epoch: 15/20; Batch:267/468; Training loss:0.101895\n",
      "Epoch: 15/20; Batch:268/468; Training loss:0.0973871\n",
      "Epoch: 15/20; Batch:269/468; Training loss:0.098523\n",
      "Epoch: 15/20; Batch:270/468; Training loss:0.0945615\n",
      "Epoch: 15/20; Batch:271/468; Training loss:0.0966633\n",
      "Epoch: 15/20; Batch:272/468; Training loss:0.0961778\n",
      "Epoch: 15/20; Batch:273/468; Training loss:0.104347\n",
      "Epoch: 15/20; Batch:274/468; Training loss:0.0963631\n",
      "Epoch: 15/20; Batch:275/468; Training loss:0.0966017\n",
      "Epoch: 15/20; Batch:276/468; Training loss:0.0980546\n",
      "Epoch: 15/20; Batch:277/468; Training loss:0.0968006\n",
      "Epoch: 15/20; Batch:278/468; Training loss:0.0941391\n",
      "Epoch: 15/20; Batch:279/468; Training loss:0.094025\n",
      "Epoch: 15/20; Batch:280/468; Training loss:0.0942348\n",
      "Epoch: 15/20; Batch:281/468; Training loss:0.0926565\n",
      "Epoch: 15/20; Batch:282/468; Training loss:0.0945286\n",
      "Epoch: 15/20; Batch:283/468; Training loss:0.0979614\n",
      "Epoch: 15/20; Batch:284/468; Training loss:0.0973101\n",
      "Epoch: 15/20; Batch:285/468; Training loss:0.100429\n",
      "Epoch: 15/20; Batch:286/468; Training loss:0.0982425\n",
      "Epoch: 15/20; Batch:287/468; Training loss:0.0970804\n",
      "Epoch: 15/20; Batch:288/468; Training loss:0.0993077\n",
      "Epoch: 15/20; Batch:289/468; Training loss:0.0978535\n",
      "Epoch: 15/20; Batch:290/468; Training loss:0.0993502\n",
      "Epoch: 15/20; Batch:291/468; Training loss:0.098883\n",
      "Epoch: 15/20; Batch:292/468; Training loss:0.0956487\n",
      "Epoch: 15/20; Batch:293/468; Training loss:0.096473\n",
      "Epoch: 15/20; Batch:294/468; Training loss:0.0981558\n",
      "Epoch: 15/20; Batch:295/468; Training loss:0.0993685\n",
      "Epoch: 15/20; Batch:296/468; Training loss:0.0977779\n",
      "Epoch: 15/20; Batch:297/468; Training loss:0.0997111\n",
      "Epoch: 15/20; Batch:298/468; Training loss:0.0965223\n",
      "Epoch: 15/20; Batch:299/468; Training loss:0.0973967\n",
      "Epoch: 15/20; Batch:300/468; Training loss:0.101252\n",
      "Epoch: 15/20; Batch:301/468; Training loss:0.100991\n",
      "Epoch: 15/20; Batch:302/468; Training loss:0.0978413\n",
      "Epoch: 15/20; Batch:303/468; Training loss:0.0974053\n",
      "Epoch: 15/20; Batch:304/468; Training loss:0.100006\n",
      "Epoch: 15/20; Batch:305/468; Training loss:0.098685\n",
      "Epoch: 15/20; Batch:306/468; Training loss:0.100034\n",
      "Epoch: 15/20; Batch:307/468; Training loss:0.0986745\n",
      "Epoch: 15/20; Batch:308/468; Training loss:0.0962833\n",
      "Epoch: 15/20; Batch:309/468; Training loss:0.101451\n",
      "Epoch: 15/20; Batch:310/468; Training loss:0.0995289\n",
      "Epoch: 15/20; Batch:311/468; Training loss:0.0953624\n",
      "Epoch: 15/20; Batch:312/468; Training loss:0.100783\n",
      "Epoch: 15/20; Batch:313/468; Training loss:0.0959205\n",
      "Epoch: 15/20; Batch:314/468; Training loss:0.100962\n",
      "Epoch: 15/20; Batch:315/468; Training loss:0.100372\n",
      "Epoch: 15/20; Batch:316/468; Training loss:0.098102\n",
      "Epoch: 15/20; Batch:317/468; Training loss:0.100489\n",
      "Epoch: 15/20; Batch:318/468; Training loss:0.0984621\n",
      "Epoch: 15/20; Batch:319/468; Training loss:0.099698\n",
      "Epoch: 15/20; Batch:320/468; Training loss:0.0961969\n",
      "Epoch: 15/20; Batch:321/468; Training loss:0.0961038\n",
      "Epoch: 15/20; Batch:322/468; Training loss:0.0965243\n",
      "Epoch: 15/20; Batch:323/468; Training loss:0.100502\n",
      "Epoch: 15/20; Batch:324/468; Training loss:0.0996953\n",
      "Epoch: 15/20; Batch:325/468; Training loss:0.0969323\n",
      "Epoch: 15/20; Batch:326/468; Training loss:0.0962799\n",
      "Epoch: 15/20; Batch:327/468; Training loss:0.0994411\n",
      "Epoch: 15/20; Batch:328/468; Training loss:0.0963733\n",
      "Epoch: 15/20; Batch:329/468; Training loss:0.100826\n",
      "Epoch: 15/20; Batch:330/468; Training loss:0.0945953\n",
      "Epoch: 15/20; Batch:331/468; Training loss:0.093265\n",
      "Epoch: 15/20; Batch:332/468; Training loss:0.0995854\n",
      "Epoch: 15/20; Batch:333/468; Training loss:0.0967918\n",
      "Epoch: 15/20; Batch:334/468; Training loss:0.100589\n",
      "Epoch: 15/20; Batch:335/468; Training loss:0.09297\n",
      "Epoch: 15/20; Batch:336/468; Training loss:0.0956773\n",
      "Epoch: 15/20; Batch:337/468; Training loss:0.0977703\n",
      "Epoch: 15/20; Batch:338/468; Training loss:0.100072\n",
      "Epoch: 15/20; Batch:339/468; Training loss:0.0946372\n",
      "Epoch: 15/20; Batch:340/468; Training loss:0.096494\n",
      "Epoch: 15/20; Batch:341/468; Training loss:0.101207\n",
      "Epoch: 15/20; Batch:342/468; Training loss:0.0994477\n",
      "Epoch: 15/20; Batch:343/468; Training loss:0.100921\n",
      "Epoch: 15/20; Batch:344/468; Training loss:0.0994143\n",
      "Epoch: 15/20; Batch:345/468; Training loss:0.0960787\n",
      "Epoch: 15/20; Batch:346/468; Training loss:0.09875\n",
      "Epoch: 15/20; Batch:347/468; Training loss:0.0965227\n",
      "Epoch: 15/20; Batch:348/468; Training loss:0.0968764\n",
      "Epoch: 15/20; Batch:349/468; Training loss:0.0956026\n",
      "Epoch: 15/20; Batch:350/468; Training loss:0.0996765\n",
      "Epoch: 15/20; Batch:351/468; Training loss:0.0999007\n",
      "Epoch: 15/20; Batch:352/468; Training loss:0.0992085\n",
      "Epoch: 15/20; Batch:353/468; Training loss:0.0963125\n",
      "Epoch: 15/20; Batch:354/468; Training loss:0.0986495\n",
      "Epoch: 15/20; Batch:355/468; Training loss:0.0926701\n",
      "Epoch: 15/20; Batch:356/468; Training loss:0.0966444\n",
      "Epoch: 15/20; Batch:357/468; Training loss:0.0979793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20; Batch:358/468; Training loss:0.0992214\n",
      "Epoch: 15/20; Batch:359/468; Training loss:0.0987992\n",
      "Epoch: 15/20; Batch:360/468; Training loss:0.0905761\n",
      "Epoch: 15/20; Batch:361/468; Training loss:0.0952057\n",
      "Epoch: 15/20; Batch:362/468; Training loss:0.092972\n",
      "Epoch: 15/20; Batch:363/468; Training loss:0.0959643\n",
      "Epoch: 15/20; Batch:364/468; Training loss:0.095697\n",
      "Epoch: 15/20; Batch:365/468; Training loss:0.0983009\n",
      "Epoch: 15/20; Batch:366/468; Training loss:0.0987494\n",
      "Epoch: 15/20; Batch:367/468; Training loss:0.0947561\n",
      "Epoch: 15/20; Batch:368/468; Training loss:0.0967989\n",
      "Epoch: 15/20; Batch:369/468; Training loss:0.0969151\n",
      "Epoch: 15/20; Batch:370/468; Training loss:0.0966799\n",
      "Epoch: 15/20; Batch:371/468; Training loss:0.0936921\n",
      "Epoch: 15/20; Batch:372/468; Training loss:0.101134\n",
      "Epoch: 15/20; Batch:373/468; Training loss:0.0961246\n",
      "Epoch: 15/20; Batch:374/468; Training loss:0.0957069\n",
      "Epoch: 15/20; Batch:375/468; Training loss:0.0949646\n",
      "Epoch: 15/20; Batch:376/468; Training loss:0.0966362\n",
      "Epoch: 15/20; Batch:377/468; Training loss:0.101016\n",
      "Epoch: 15/20; Batch:378/468; Training loss:0.0971892\n",
      "Epoch: 15/20; Batch:379/468; Training loss:0.0931837\n",
      "Epoch: 15/20; Batch:380/468; Training loss:0.0975635\n",
      "Epoch: 15/20; Batch:381/468; Training loss:0.0970188\n",
      "Epoch: 15/20; Batch:382/468; Training loss:0.096097\n",
      "Epoch: 15/20; Batch:383/468; Training loss:0.0956094\n",
      "Epoch: 15/20; Batch:384/468; Training loss:0.0971637\n",
      "Epoch: 15/20; Batch:385/468; Training loss:0.0937846\n",
      "Epoch: 15/20; Batch:386/468; Training loss:0.0944742\n",
      "Epoch: 15/20; Batch:387/468; Training loss:0.0966753\n",
      "Epoch: 15/20; Batch:388/468; Training loss:0.0930042\n",
      "Epoch: 15/20; Batch:389/468; Training loss:0.0964827\n",
      "Epoch: 15/20; Batch:390/468; Training loss:0.0960231\n",
      "Epoch: 15/20; Batch:391/468; Training loss:0.0970271\n",
      "Epoch: 15/20; Batch:392/468; Training loss:0.0962527\n",
      "Epoch: 15/20; Batch:393/468; Training loss:0.0973225\n",
      "Epoch: 15/20; Batch:394/468; Training loss:0.0939969\n",
      "Epoch: 15/20; Batch:395/468; Training loss:0.0977702\n",
      "Epoch: 15/20; Batch:396/468; Training loss:0.0943412\n",
      "Epoch: 15/20; Batch:397/468; Training loss:0.0965246\n",
      "Epoch: 15/20; Batch:398/468; Training loss:0.0977333\n",
      "Epoch: 15/20; Batch:399/468; Training loss:0.0988602\n",
      "Epoch: 15/20; Batch:400/468; Training loss:0.0973322\n",
      "Epoch: 15/20; Batch:401/468; Training loss:0.0989852\n",
      "Epoch: 15/20; Batch:402/468; Training loss:0.0971631\n",
      "Epoch: 15/20; Batch:403/468; Training loss:0.0968576\n",
      "Epoch: 15/20; Batch:404/468; Training loss:0.0961619\n",
      "Epoch: 15/20; Batch:405/468; Training loss:0.100149\n",
      "Epoch: 15/20; Batch:406/468; Training loss:0.0966736\n",
      "Epoch: 15/20; Batch:407/468; Training loss:0.095149\n",
      "Epoch: 15/20; Batch:408/468; Training loss:0.0959333\n",
      "Epoch: 15/20; Batch:409/468; Training loss:0.0988519\n",
      "Epoch: 15/20; Batch:410/468; Training loss:0.0963979\n",
      "Epoch: 15/20; Batch:411/468; Training loss:0.0954748\n",
      "Epoch: 15/20; Batch:412/468; Training loss:0.0976269\n",
      "Epoch: 15/20; Batch:413/468; Training loss:0.0970915\n",
      "Epoch: 15/20; Batch:414/468; Training loss:0.0951248\n",
      "Epoch: 15/20; Batch:415/468; Training loss:0.0984359\n",
      "Epoch: 15/20; Batch:416/468; Training loss:0.093601\n",
      "Epoch: 15/20; Batch:417/468; Training loss:0.0962638\n",
      "Epoch: 15/20; Batch:418/468; Training loss:0.098035\n",
      "Epoch: 15/20; Batch:419/468; Training loss:0.0943184\n",
      "Epoch: 15/20; Batch:420/468; Training loss:0.0964611\n",
      "Epoch: 15/20; Batch:421/468; Training loss:0.0973017\n",
      "Epoch: 15/20; Batch:422/468; Training loss:0.0962597\n",
      "Epoch: 15/20; Batch:423/468; Training loss:0.0962849\n",
      "Epoch: 15/20; Batch:424/468; Training loss:0.0970831\n",
      "Epoch: 15/20; Batch:425/468; Training loss:0.0987097\n",
      "Epoch: 15/20; Batch:426/468; Training loss:0.0946746\n",
      "Epoch: 15/20; Batch:427/468; Training loss:0.0902088\n",
      "Epoch: 15/20; Batch:428/468; Training loss:0.0982483\n",
      "Epoch: 15/20; Batch:429/468; Training loss:0.0969624\n",
      "Epoch: 15/20; Batch:430/468; Training loss:0.0963883\n",
      "Epoch: 15/20; Batch:431/468; Training loss:0.0915752\n",
      "Epoch: 15/20; Batch:432/468; Training loss:0.0964413\n",
      "Epoch: 15/20; Batch:433/468; Training loss:0.0943045\n",
      "Epoch: 15/20; Batch:434/468; Training loss:0.0972576\n",
      "Epoch: 15/20; Batch:435/468; Training loss:0.0945194\n",
      "Epoch: 15/20; Batch:436/468; Training loss:0.0952826\n",
      "Epoch: 15/20; Batch:437/468; Training loss:0.0941479\n",
      "Epoch: 15/20; Batch:438/468; Training loss:0.096744\n",
      "Epoch: 15/20; Batch:439/468; Training loss:0.0990235\n",
      "Epoch: 15/20; Batch:440/468; Training loss:0.0999543\n",
      "Epoch: 15/20; Batch:441/468; Training loss:0.0937726\n",
      "Epoch: 15/20; Batch:442/468; Training loss:0.102634\n",
      "Epoch: 15/20; Batch:443/468; Training loss:0.0925284\n",
      "Epoch: 15/20; Batch:444/468; Training loss:0.0935441\n",
      "Epoch: 15/20; Batch:445/468; Training loss:0.0961868\n",
      "Epoch: 15/20; Batch:446/468; Training loss:0.0978185\n",
      "Epoch: 15/20; Batch:447/468; Training loss:0.101078\n",
      "Epoch: 15/20; Batch:448/468; Training loss:0.0912139\n",
      "Epoch: 15/20; Batch:449/468; Training loss:0.100389\n",
      "Epoch: 15/20; Batch:450/468; Training loss:0.0957667\n",
      "Epoch: 15/20; Batch:451/468; Training loss:0.0910157\n",
      "Epoch: 15/20; Batch:452/468; Training loss:0.0988888\n",
      "Epoch: 15/20; Batch:453/468; Training loss:0.0959825\n",
      "Epoch: 15/20; Batch:454/468; Training loss:0.0985537\n",
      "Epoch: 15/20; Batch:455/468; Training loss:0.0962439\n",
      "Epoch: 15/20; Batch:456/468; Training loss:0.0932334\n",
      "Epoch: 15/20; Batch:457/468; Training loss:0.0946563\n",
      "Epoch: 15/20; Batch:458/468; Training loss:0.0956396\n",
      "Epoch: 15/20; Batch:459/468; Training loss:0.0932332\n",
      "Epoch: 15/20; Batch:460/468; Training loss:0.101828\n",
      "Epoch: 15/20; Batch:461/468; Training loss:0.0974573\n",
      "Epoch: 15/20; Batch:462/468; Training loss:0.100187\n",
      "Epoch: 15/20; Batch:463/468; Training loss:0.0989096\n",
      "Epoch: 15/20; Batch:464/468; Training loss:0.0964125\n",
      "Epoch: 15/20; Batch:465/468; Training loss:0.100487\n",
      "Epoch: 15/20; Batch:466/468; Training loss:0.102416\n",
      "Epoch: 15/20; Batch:467/468; Training loss:0.100092\n",
      "Epoch: 15/20; Batch:468/468; Training loss:0.0962847\n",
      "Epoch: 16/20; Batch:1/468; Training loss:0.0980485\n",
      "Epoch: 16/20; Batch:2/468; Training loss:0.0924442\n",
      "Epoch: 16/20; Batch:3/468; Training loss:0.0975814\n",
      "Epoch: 16/20; Batch:4/468; Training loss:0.0984484\n",
      "Epoch: 16/20; Batch:5/468; Training loss:0.0976233\n",
      "Epoch: 16/20; Batch:6/468; Training loss:0.0989883\n",
      "Epoch: 16/20; Batch:7/468; Training loss:0.0964476\n",
      "Epoch: 16/20; Batch:8/468; Training loss:0.0951377\n",
      "Epoch: 16/20; Batch:9/468; Training loss:0.0937415\n",
      "Epoch: 16/20; Batch:10/468; Training loss:0.100047\n",
      "Epoch: 16/20; Batch:11/468; Training loss:0.098923\n",
      "Epoch: 16/20; Batch:12/468; Training loss:0.0932815\n",
      "Epoch: 16/20; Batch:13/468; Training loss:0.0944849\n",
      "Epoch: 16/20; Batch:14/468; Training loss:0.096694\n",
      "Epoch: 16/20; Batch:15/468; Training loss:0.0971155\n",
      "Epoch: 16/20; Batch:16/468; Training loss:0.0969488\n",
      "Epoch: 16/20; Batch:17/468; Training loss:0.0944609\n",
      "Epoch: 16/20; Batch:18/468; Training loss:0.0961769\n",
      "Epoch: 16/20; Batch:19/468; Training loss:0.0958386\n",
      "Epoch: 16/20; Batch:20/468; Training loss:0.0981235\n",
      "Epoch: 16/20; Batch:21/468; Training loss:0.0967429\n",
      "Epoch: 16/20; Batch:22/468; Training loss:0.0943171\n",
      "Epoch: 16/20; Batch:23/468; Training loss:0.0951338\n",
      "Epoch: 16/20; Batch:24/468; Training loss:0.0979405\n",
      "Epoch: 16/20; Batch:25/468; Training loss:0.0983574\n",
      "Epoch: 16/20; Batch:26/468; Training loss:0.098927\n",
      "Epoch: 16/20; Batch:27/468; Training loss:0.0931022\n",
      "Epoch: 16/20; Batch:28/468; Training loss:0.0992444\n",
      "Epoch: 16/20; Batch:29/468; Training loss:0.101111\n",
      "Epoch: 16/20; Batch:30/468; Training loss:0.100392\n",
      "Epoch: 16/20; Batch:31/468; Training loss:0.0910243\n",
      "Epoch: 16/20; Batch:32/468; Training loss:0.0990297\n",
      "Epoch: 16/20; Batch:33/468; Training loss:0.0969141\n",
      "Epoch: 16/20; Batch:34/468; Training loss:0.0990463\n",
      "Epoch: 16/20; Batch:35/468; Training loss:0.0954309\n",
      "Epoch: 16/20; Batch:36/468; Training loss:0.0966441\n",
      "Epoch: 16/20; Batch:37/468; Training loss:0.0959693\n",
      "Epoch: 16/20; Batch:38/468; Training loss:0.0986813\n",
      "Epoch: 16/20; Batch:39/468; Training loss:0.0945826\n",
      "Epoch: 16/20; Batch:40/468; Training loss:0.0998771\n",
      "Epoch: 16/20; Batch:41/468; Training loss:0.0993007\n",
      "Epoch: 16/20; Batch:42/468; Training loss:0.0948212\n",
      "Epoch: 16/20; Batch:43/468; Training loss:0.0998972\n",
      "Epoch: 16/20; Batch:44/468; Training loss:0.0943692\n",
      "Epoch: 16/20; Batch:45/468; Training loss:0.0966779\n",
      "Epoch: 16/20; Batch:46/468; Training loss:0.094476\n",
      "Epoch: 16/20; Batch:47/468; Training loss:0.0965314\n",
      "Epoch: 16/20; Batch:48/468; Training loss:0.10241\n",
      "Epoch: 16/20; Batch:49/468; Training loss:0.0950603\n",
      "Epoch: 16/20; Batch:50/468; Training loss:0.098818\n",
      "Epoch: 16/20; Batch:51/468; Training loss:0.0997921\n",
      "Epoch: 16/20; Batch:52/468; Training loss:0.0961156\n",
      "Epoch: 16/20; Batch:53/468; Training loss:0.0933724\n",
      "Epoch: 16/20; Batch:54/468; Training loss:0.0944183\n",
      "Epoch: 16/20; Batch:55/468; Training loss:0.0958065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20; Batch:56/468; Training loss:0.093577\n",
      "Epoch: 16/20; Batch:57/468; Training loss:0.0953183\n",
      "Epoch: 16/20; Batch:58/468; Training loss:0.0975293\n",
      "Epoch: 16/20; Batch:59/468; Training loss:0.0956378\n",
      "Epoch: 16/20; Batch:60/468; Training loss:0.0969522\n",
      "Epoch: 16/20; Batch:61/468; Training loss:0.103518\n",
      "Epoch: 16/20; Batch:62/468; Training loss:0.0987073\n",
      "Epoch: 16/20; Batch:63/468; Training loss:0.0970684\n",
      "Epoch: 16/20; Batch:64/468; Training loss:0.0953488\n",
      "Epoch: 16/20; Batch:65/468; Training loss:0.0966333\n",
      "Epoch: 16/20; Batch:66/468; Training loss:0.093649\n",
      "Epoch: 16/20; Batch:67/468; Training loss:0.0954467\n",
      "Epoch: 16/20; Batch:68/468; Training loss:0.0980822\n",
      "Epoch: 16/20; Batch:69/468; Training loss:0.0996886\n",
      "Epoch: 16/20; Batch:70/468; Training loss:0.0969285\n",
      "Epoch: 16/20; Batch:71/468; Training loss:0.0980542\n",
      "Epoch: 16/20; Batch:72/468; Training loss:0.0967589\n",
      "Epoch: 16/20; Batch:73/468; Training loss:0.0922307\n",
      "Epoch: 16/20; Batch:74/468; Training loss:0.0986947\n",
      "Epoch: 16/20; Batch:75/468; Training loss:0.0961223\n",
      "Epoch: 16/20; Batch:76/468; Training loss:0.0980596\n",
      "Epoch: 16/20; Batch:77/468; Training loss:0.0966401\n",
      "Epoch: 16/20; Batch:78/468; Training loss:0.0982253\n",
      "Epoch: 16/20; Batch:79/468; Training loss:0.095301\n",
      "Epoch: 16/20; Batch:80/468; Training loss:0.0937937\n",
      "Epoch: 16/20; Batch:81/468; Training loss:0.0985444\n",
      "Epoch: 16/20; Batch:82/468; Training loss:0.0978843\n",
      "Epoch: 16/20; Batch:83/468; Training loss:0.0947796\n",
      "Epoch: 16/20; Batch:84/468; Training loss:0.0969685\n",
      "Epoch: 16/20; Batch:85/468; Training loss:0.0968155\n",
      "Epoch: 16/20; Batch:86/468; Training loss:0.0941231\n",
      "Epoch: 16/20; Batch:87/468; Training loss:0.0975532\n",
      "Epoch: 16/20; Batch:88/468; Training loss:0.0972729\n",
      "Epoch: 16/20; Batch:89/468; Training loss:0.093346\n",
      "Epoch: 16/20; Batch:90/468; Training loss:0.0963052\n",
      "Epoch: 16/20; Batch:91/468; Training loss:0.0975321\n",
      "Epoch: 16/20; Batch:92/468; Training loss:0.0969077\n",
      "Epoch: 16/20; Batch:93/468; Training loss:0.0928383\n",
      "Epoch: 16/20; Batch:94/468; Training loss:0.0952878\n",
      "Epoch: 16/20; Batch:95/468; Training loss:0.102311\n",
      "Epoch: 16/20; Batch:96/468; Training loss:0.0955534\n",
      "Epoch: 16/20; Batch:97/468; Training loss:0.0958152\n",
      "Epoch: 16/20; Batch:98/468; Training loss:0.0971626\n",
      "Epoch: 16/20; Batch:99/468; Training loss:0.0981975\n",
      "Epoch: 16/20; Batch:100/468; Training loss:0.0916762\n",
      "Epoch: 16/20; Batch:101/468; Training loss:0.0971311\n",
      "Epoch: 16/20; Batch:102/468; Training loss:0.100011\n",
      "Epoch: 16/20; Batch:103/468; Training loss:0.0975831\n",
      "Epoch: 16/20; Batch:104/468; Training loss:0.101404\n",
      "Epoch: 16/20; Batch:105/468; Training loss:0.0964749\n",
      "Epoch: 16/20; Batch:106/468; Training loss:0.0921145\n",
      "Epoch: 16/20; Batch:107/468; Training loss:0.0977015\n",
      "Epoch: 16/20; Batch:108/468; Training loss:0.0996174\n",
      "Epoch: 16/20; Batch:109/468; Training loss:0.098762\n",
      "Epoch: 16/20; Batch:110/468; Training loss:0.0990734\n",
      "Epoch: 16/20; Batch:111/468; Training loss:0.0987951\n",
      "Epoch: 16/20; Batch:112/468; Training loss:0.0964372\n",
      "Epoch: 16/20; Batch:113/468; Training loss:0.0968369\n",
      "Epoch: 16/20; Batch:114/468; Training loss:0.0976761\n",
      "Epoch: 16/20; Batch:115/468; Training loss:0.0993511\n",
      "Epoch: 16/20; Batch:116/468; Training loss:0.0943619\n",
      "Epoch: 16/20; Batch:117/468; Training loss:0.0972486\n",
      "Epoch: 16/20; Batch:118/468; Training loss:0.0972143\n",
      "Epoch: 16/20; Batch:119/468; Training loss:0.0982057\n",
      "Epoch: 16/20; Batch:120/468; Training loss:0.0963336\n",
      "Epoch: 16/20; Batch:121/468; Training loss:0.102614\n",
      "Epoch: 16/20; Batch:122/468; Training loss:0.0947368\n",
      "Epoch: 16/20; Batch:123/468; Training loss:0.0961947\n",
      "Epoch: 16/20; Batch:124/468; Training loss:0.0979813\n",
      "Epoch: 16/20; Batch:125/468; Training loss:0.0993107\n",
      "Epoch: 16/20; Batch:126/468; Training loss:0.0982437\n",
      "Epoch: 16/20; Batch:127/468; Training loss:0.0994719\n",
      "Epoch: 16/20; Batch:128/468; Training loss:0.0943426\n",
      "Epoch: 16/20; Batch:129/468; Training loss:0.0979334\n",
      "Epoch: 16/20; Batch:130/468; Training loss:0.09386\n",
      "Epoch: 16/20; Batch:131/468; Training loss:0.09839\n",
      "Epoch: 16/20; Batch:132/468; Training loss:0.0931812\n",
      "Epoch: 16/20; Batch:133/468; Training loss:0.0989616\n",
      "Epoch: 16/20; Batch:134/468; Training loss:0.0946833\n",
      "Epoch: 16/20; Batch:135/468; Training loss:0.0938422\n",
      "Epoch: 16/20; Batch:136/468; Training loss:0.0984566\n",
      "Epoch: 16/20; Batch:137/468; Training loss:0.0975699\n",
      "Epoch: 16/20; Batch:138/468; Training loss:0.0982869\n",
      "Epoch: 16/20; Batch:139/468; Training loss:0.0977305\n",
      "Epoch: 16/20; Batch:140/468; Training loss:0.0921962\n",
      "Epoch: 16/20; Batch:141/468; Training loss:0.0987597\n",
      "Epoch: 16/20; Batch:142/468; Training loss:0.0911667\n",
      "Epoch: 16/20; Batch:143/468; Training loss:0.0951991\n",
      "Epoch: 16/20; Batch:144/468; Training loss:0.0960659\n",
      "Epoch: 16/20; Batch:145/468; Training loss:0.0972435\n",
      "Epoch: 16/20; Batch:146/468; Training loss:0.0976868\n",
      "Epoch: 16/20; Batch:147/468; Training loss:0.0948279\n",
      "Epoch: 16/20; Batch:148/468; Training loss:0.102816\n",
      "Epoch: 16/20; Batch:149/468; Training loss:0.0955422\n",
      "Epoch: 16/20; Batch:150/468; Training loss:0.0951112\n",
      "Epoch: 16/20; Batch:151/468; Training loss:0.0988661\n",
      "Epoch: 16/20; Batch:152/468; Training loss:0.0945991\n",
      "Epoch: 16/20; Batch:153/468; Training loss:0.0971617\n",
      "Epoch: 16/20; Batch:154/468; Training loss:0.094671\n",
      "Epoch: 16/20; Batch:155/468; Training loss:0.0940438\n",
      "Epoch: 16/20; Batch:156/468; Training loss:0.0965658\n",
      "Epoch: 16/20; Batch:157/468; Training loss:0.100251\n",
      "Epoch: 16/20; Batch:158/468; Training loss:0.0973476\n",
      "Epoch: 16/20; Batch:159/468; Training loss:0.0961341\n",
      "Epoch: 16/20; Batch:160/468; Training loss:0.0975917\n",
      "Epoch: 16/20; Batch:161/468; Training loss:0.09733\n",
      "Epoch: 16/20; Batch:162/468; Training loss:0.0953978\n",
      "Epoch: 16/20; Batch:163/468; Training loss:0.101459\n",
      "Epoch: 16/20; Batch:164/468; Training loss:0.0977074\n",
      "Epoch: 16/20; Batch:165/468; Training loss:0.0974372\n",
      "Epoch: 16/20; Batch:166/468; Training loss:0.091439\n",
      "Epoch: 16/20; Batch:167/468; Training loss:0.0957129\n",
      "Epoch: 16/20; Batch:168/468; Training loss:0.0937323\n",
      "Epoch: 16/20; Batch:169/468; Training loss:0.0984864\n",
      "Epoch: 16/20; Batch:170/468; Training loss:0.0918408\n",
      "Epoch: 16/20; Batch:171/468; Training loss:0.100727\n",
      "Epoch: 16/20; Batch:172/468; Training loss:0.0971548\n",
      "Epoch: 16/20; Batch:173/468; Training loss:0.0978875\n",
      "Epoch: 16/20; Batch:174/468; Training loss:0.0978534\n",
      "Epoch: 16/20; Batch:175/468; Training loss:0.0936335\n",
      "Epoch: 16/20; Batch:176/468; Training loss:0.0936032\n",
      "Epoch: 16/20; Batch:177/468; Training loss:0.0987344\n",
      "Epoch: 16/20; Batch:178/468; Training loss:0.0971523\n",
      "Epoch: 16/20; Batch:179/468; Training loss:0.0982632\n",
      "Epoch: 16/20; Batch:180/468; Training loss:0.0982174\n",
      "Epoch: 16/20; Batch:181/468; Training loss:0.0936496\n",
      "Epoch: 16/20; Batch:182/468; Training loss:0.0981325\n",
      "Epoch: 16/20; Batch:183/468; Training loss:0.0973463\n",
      "Epoch: 16/20; Batch:184/468; Training loss:0.0944756\n",
      "Epoch: 16/20; Batch:185/468; Training loss:0.0970639\n",
      "Epoch: 16/20; Batch:186/468; Training loss:0.098231\n",
      "Epoch: 16/20; Batch:187/468; Training loss:0.0965179\n",
      "Epoch: 16/20; Batch:188/468; Training loss:0.0950314\n",
      "Epoch: 16/20; Batch:189/468; Training loss:0.0922741\n",
      "Epoch: 16/20; Batch:190/468; Training loss:0.0958174\n",
      "Epoch: 16/20; Batch:191/468; Training loss:0.0986014\n",
      "Epoch: 16/20; Batch:192/468; Training loss:0.100561\n",
      "Epoch: 16/20; Batch:193/468; Training loss:0.0990725\n",
      "Epoch: 16/20; Batch:194/468; Training loss:0.096246\n",
      "Epoch: 16/20; Batch:195/468; Training loss:0.0965\n",
      "Epoch: 16/20; Batch:196/468; Training loss:0.0971621\n",
      "Epoch: 16/20; Batch:197/468; Training loss:0.0982593\n",
      "Epoch: 16/20; Batch:198/468; Training loss:0.0969097\n",
      "Epoch: 16/20; Batch:199/468; Training loss:0.099004\n",
      "Epoch: 16/20; Batch:200/468; Training loss:0.0911405\n",
      "Epoch: 16/20; Batch:201/468; Training loss:0.0986034\n",
      "Epoch: 16/20; Batch:202/468; Training loss:0.0965026\n",
      "Epoch: 16/20; Batch:203/468; Training loss:0.0929571\n",
      "Epoch: 16/20; Batch:204/468; Training loss:0.0995155\n",
      "Epoch: 16/20; Batch:205/468; Training loss:0.0966977\n",
      "Epoch: 16/20; Batch:206/468; Training loss:0.0969989\n",
      "Epoch: 16/20; Batch:207/468; Training loss:0.0963868\n",
      "Epoch: 16/20; Batch:208/468; Training loss:0.0978754\n",
      "Epoch: 16/20; Batch:209/468; Training loss:0.0972137\n",
      "Epoch: 16/20; Batch:210/468; Training loss:0.0935844\n",
      "Epoch: 16/20; Batch:211/468; Training loss:0.0949676\n",
      "Epoch: 16/20; Batch:212/468; Training loss:0.0981256\n",
      "Epoch: 16/20; Batch:213/468; Training loss:0.0988336\n",
      "Epoch: 16/20; Batch:214/468; Training loss:0.0964906\n",
      "Epoch: 16/20; Batch:215/468; Training loss:0.0937697\n",
      "Epoch: 16/20; Batch:216/468; Training loss:0.0972057\n",
      "Epoch: 16/20; Batch:217/468; Training loss:0.0938296\n",
      "Epoch: 16/20; Batch:218/468; Training loss:0.0958163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20; Batch:219/468; Training loss:0.0937683\n",
      "Epoch: 16/20; Batch:220/468; Training loss:0.0961761\n",
      "Epoch: 16/20; Batch:221/468; Training loss:0.0955491\n",
      "Epoch: 16/20; Batch:222/468; Training loss:0.0963434\n",
      "Epoch: 16/20; Batch:223/468; Training loss:0.0960218\n",
      "Epoch: 16/20; Batch:224/468; Training loss:0.0987716\n",
      "Epoch: 16/20; Batch:225/468; Training loss:0.0980143\n",
      "Epoch: 16/20; Batch:226/468; Training loss:0.0943023\n",
      "Epoch: 16/20; Batch:227/468; Training loss:0.099274\n",
      "Epoch: 16/20; Batch:228/468; Training loss:0.0978189\n",
      "Epoch: 16/20; Batch:229/468; Training loss:0.0949772\n",
      "Epoch: 16/20; Batch:230/468; Training loss:0.103548\n",
      "Epoch: 16/20; Batch:231/468; Training loss:0.099285\n",
      "Epoch: 16/20; Batch:232/468; Training loss:0.0957031\n",
      "Epoch: 16/20; Batch:233/468; Training loss:0.0993599\n",
      "Epoch: 16/20; Batch:234/468; Training loss:0.09623\n",
      "Epoch: 16/20; Batch:235/468; Training loss:0.0977786\n",
      "Epoch: 16/20; Batch:236/468; Training loss:0.092192\n",
      "Epoch: 16/20; Batch:237/468; Training loss:0.0996035\n",
      "Epoch: 16/20; Batch:238/468; Training loss:0.094443\n",
      "Epoch: 16/20; Batch:239/468; Training loss:0.0955113\n",
      "Epoch: 16/20; Batch:240/468; Training loss:0.0955377\n",
      "Epoch: 16/20; Batch:241/468; Training loss:0.100399\n",
      "Epoch: 16/20; Batch:242/468; Training loss:0.0989957\n",
      "Epoch: 16/20; Batch:243/468; Training loss:0.0958934\n",
      "Epoch: 16/20; Batch:244/468; Training loss:0.0979723\n",
      "Epoch: 16/20; Batch:245/468; Training loss:0.0963118\n",
      "Epoch: 16/20; Batch:246/468; Training loss:0.093068\n",
      "Epoch: 16/20; Batch:247/468; Training loss:0.0961689\n",
      "Epoch: 16/20; Batch:248/468; Training loss:0.096572\n",
      "Epoch: 16/20; Batch:249/468; Training loss:0.0975704\n",
      "Epoch: 16/20; Batch:250/468; Training loss:0.0971224\n",
      "Epoch: 16/20; Batch:251/468; Training loss:0.0988681\n",
      "Epoch: 16/20; Batch:252/468; Training loss:0.0954491\n",
      "Epoch: 16/20; Batch:253/468; Training loss:0.0973915\n",
      "Epoch: 16/20; Batch:254/468; Training loss:0.0956284\n",
      "Epoch: 16/20; Batch:255/468; Training loss:0.0956164\n",
      "Epoch: 16/20; Batch:256/468; Training loss:0.0968537\n",
      "Epoch: 16/20; Batch:257/468; Training loss:0.0970468\n",
      "Epoch: 16/20; Batch:258/468; Training loss:0.0966397\n",
      "Epoch: 16/20; Batch:259/468; Training loss:0.0990136\n",
      "Epoch: 16/20; Batch:260/468; Training loss:0.0993168\n",
      "Epoch: 16/20; Batch:261/468; Training loss:0.0928767\n",
      "Epoch: 16/20; Batch:262/468; Training loss:0.0986136\n",
      "Epoch: 16/20; Batch:263/468; Training loss:0.0956153\n",
      "Epoch: 16/20; Batch:264/468; Training loss:0.0945829\n",
      "Epoch: 16/20; Batch:265/468; Training loss:0.0969707\n",
      "Epoch: 16/20; Batch:266/468; Training loss:0.0960958\n",
      "Epoch: 16/20; Batch:267/468; Training loss:0.0986511\n",
      "Epoch: 16/20; Batch:268/468; Training loss:0.0961615\n",
      "Epoch: 16/20; Batch:269/468; Training loss:0.0972257\n",
      "Epoch: 16/20; Batch:270/468; Training loss:0.100412\n",
      "Epoch: 16/20; Batch:271/468; Training loss:0.0987728\n",
      "Epoch: 16/20; Batch:272/468; Training loss:0.0938096\n",
      "Epoch: 16/20; Batch:273/468; Training loss:0.0953157\n",
      "Epoch: 16/20; Batch:274/468; Training loss:0.0996362\n",
      "Epoch: 16/20; Batch:275/468; Training loss:0.0955163\n",
      "Epoch: 16/20; Batch:276/468; Training loss:0.0978159\n",
      "Epoch: 16/20; Batch:277/468; Training loss:0.0972616\n",
      "Epoch: 16/20; Batch:278/468; Training loss:0.095542\n",
      "Epoch: 16/20; Batch:279/468; Training loss:0.0946967\n",
      "Epoch: 16/20; Batch:280/468; Training loss:0.0979094\n",
      "Epoch: 16/20; Batch:281/468; Training loss:0.0921609\n",
      "Epoch: 16/20; Batch:282/468; Training loss:0.0993168\n",
      "Epoch: 16/20; Batch:283/468; Training loss:0.0964667\n",
      "Epoch: 16/20; Batch:284/468; Training loss:0.0973253\n",
      "Epoch: 16/20; Batch:285/468; Training loss:0.0933072\n",
      "Epoch: 16/20; Batch:286/468; Training loss:0.0979678\n",
      "Epoch: 16/20; Batch:287/468; Training loss:0.0946923\n",
      "Epoch: 16/20; Batch:288/468; Training loss:0.0969697\n",
      "Epoch: 16/20; Batch:289/468; Training loss:0.100332\n",
      "Epoch: 16/20; Batch:290/468; Training loss:0.0984419\n",
      "Epoch: 16/20; Batch:291/468; Training loss:0.10033\n",
      "Epoch: 16/20; Batch:292/468; Training loss:0.0993017\n",
      "Epoch: 16/20; Batch:293/468; Training loss:0.0991046\n",
      "Epoch: 16/20; Batch:294/468; Training loss:0.0971407\n",
      "Epoch: 16/20; Batch:295/468; Training loss:0.0953893\n",
      "Epoch: 16/20; Batch:296/468; Training loss:0.0980855\n",
      "Epoch: 16/20; Batch:297/468; Training loss:0.0987737\n",
      "Epoch: 16/20; Batch:298/468; Training loss:0.0975454\n",
      "Epoch: 16/20; Batch:299/468; Training loss:0.0990124\n",
      "Epoch: 16/20; Batch:300/468; Training loss:0.0956682\n",
      "Epoch: 16/20; Batch:301/468; Training loss:0.0961941\n",
      "Epoch: 16/20; Batch:302/468; Training loss:0.100841\n",
      "Epoch: 16/20; Batch:303/468; Training loss:0.0950964\n",
      "Epoch: 16/20; Batch:304/468; Training loss:0.0971867\n",
      "Epoch: 16/20; Batch:305/468; Training loss:0.0964127\n",
      "Epoch: 16/20; Batch:306/468; Training loss:0.0947266\n",
      "Epoch: 16/20; Batch:307/468; Training loss:0.0966329\n",
      "Epoch: 16/20; Batch:308/468; Training loss:0.100122\n",
      "Epoch: 16/20; Batch:309/468; Training loss:0.0957957\n",
      "Epoch: 16/20; Batch:310/468; Training loss:0.102594\n",
      "Epoch: 16/20; Batch:311/468; Training loss:0.0947101\n",
      "Epoch: 16/20; Batch:312/468; Training loss:0.0955615\n",
      "Epoch: 16/20; Batch:313/468; Training loss:0.0945089\n",
      "Epoch: 16/20; Batch:314/468; Training loss:0.0938976\n",
      "Epoch: 16/20; Batch:315/468; Training loss:0.0972573\n",
      "Epoch: 16/20; Batch:316/468; Training loss:0.0929216\n",
      "Epoch: 16/20; Batch:317/468; Training loss:0.096125\n",
      "Epoch: 16/20; Batch:318/468; Training loss:0.0981434\n",
      "Epoch: 16/20; Batch:319/468; Training loss:0.0973934\n",
      "Epoch: 16/20; Batch:320/468; Training loss:0.0957496\n",
      "Epoch: 16/20; Batch:321/468; Training loss:0.0973302\n",
      "Epoch: 16/20; Batch:322/468; Training loss:0.099259\n",
      "Epoch: 16/20; Batch:323/468; Training loss:0.0979636\n",
      "Epoch: 16/20; Batch:324/468; Training loss:0.0910532\n",
      "Epoch: 16/20; Batch:325/468; Training loss:0.0984054\n",
      "Epoch: 16/20; Batch:326/468; Training loss:0.0960147\n",
      "Epoch: 16/20; Batch:327/468; Training loss:0.0995625\n",
      "Epoch: 16/20; Batch:328/468; Training loss:0.0953661\n",
      "Epoch: 16/20; Batch:329/468; Training loss:0.0987867\n",
      "Epoch: 16/20; Batch:330/468; Training loss:0.097422\n",
      "Epoch: 16/20; Batch:331/468; Training loss:0.0931904\n",
      "Epoch: 16/20; Batch:332/468; Training loss:0.0959924\n",
      "Epoch: 16/20; Batch:333/468; Training loss:0.101289\n",
      "Epoch: 16/20; Batch:334/468; Training loss:0.096609\n",
      "Epoch: 16/20; Batch:335/468; Training loss:0.0970931\n",
      "Epoch: 16/20; Batch:336/468; Training loss:0.0980595\n",
      "Epoch: 16/20; Batch:337/468; Training loss:0.097034\n",
      "Epoch: 16/20; Batch:338/468; Training loss:0.093193\n",
      "Epoch: 16/20; Batch:339/468; Training loss:0.0961415\n",
      "Epoch: 16/20; Batch:340/468; Training loss:0.0965075\n",
      "Epoch: 16/20; Batch:341/468; Training loss:0.0956001\n",
      "Epoch: 16/20; Batch:342/468; Training loss:0.0991112\n",
      "Epoch: 16/20; Batch:343/468; Training loss:0.0971971\n",
      "Epoch: 16/20; Batch:344/468; Training loss:0.0948075\n",
      "Epoch: 16/20; Batch:345/468; Training loss:0.0966537\n",
      "Epoch: 16/20; Batch:346/468; Training loss:0.0964146\n",
      "Epoch: 16/20; Batch:347/468; Training loss:0.0953278\n",
      "Epoch: 16/20; Batch:348/468; Training loss:0.0971329\n",
      "Epoch: 16/20; Batch:349/468; Training loss:0.0998097\n",
      "Epoch: 16/20; Batch:350/468; Training loss:0.09573\n",
      "Epoch: 16/20; Batch:351/468; Training loss:0.093411\n",
      "Epoch: 16/20; Batch:352/468; Training loss:0.101341\n",
      "Epoch: 16/20; Batch:353/468; Training loss:0.0949491\n",
      "Epoch: 16/20; Batch:354/468; Training loss:0.0970886\n",
      "Epoch: 16/20; Batch:355/468; Training loss:0.0985085\n",
      "Epoch: 16/20; Batch:356/468; Training loss:0.0976636\n",
      "Epoch: 16/20; Batch:357/468; Training loss:0.0981961\n",
      "Epoch: 16/20; Batch:358/468; Training loss:0.0952707\n",
      "Epoch: 16/20; Batch:359/468; Training loss:0.0953047\n",
      "Epoch: 16/20; Batch:360/468; Training loss:0.0949323\n",
      "Epoch: 16/20; Batch:361/468; Training loss:0.0974844\n",
      "Epoch: 16/20; Batch:362/468; Training loss:0.0981061\n",
      "Epoch: 16/20; Batch:363/468; Training loss:0.100494\n",
      "Epoch: 16/20; Batch:364/468; Training loss:0.0936482\n",
      "Epoch: 16/20; Batch:365/468; Training loss:0.0947247\n",
      "Epoch: 16/20; Batch:366/468; Training loss:0.0963529\n",
      "Epoch: 16/20; Batch:367/468; Training loss:0.0965982\n",
      "Epoch: 16/20; Batch:368/468; Training loss:0.101242\n",
      "Epoch: 16/20; Batch:369/468; Training loss:0.0932641\n",
      "Epoch: 16/20; Batch:370/468; Training loss:0.092362\n",
      "Epoch: 16/20; Batch:371/468; Training loss:0.0930284\n",
      "Epoch: 16/20; Batch:372/468; Training loss:0.0989878\n",
      "Epoch: 16/20; Batch:373/468; Training loss:0.0947114\n",
      "Epoch: 16/20; Batch:374/468; Training loss:0.0929272\n",
      "Epoch: 16/20; Batch:375/468; Training loss:0.104144\n",
      "Epoch: 16/20; Batch:376/468; Training loss:0.0960221\n",
      "Epoch: 16/20; Batch:377/468; Training loss:0.0989499\n",
      "Epoch: 16/20; Batch:378/468; Training loss:0.0965285\n",
      "Epoch: 16/20; Batch:379/468; Training loss:0.0945909\n",
      "Epoch: 16/20; Batch:380/468; Training loss:0.0940225\n",
      "Epoch: 16/20; Batch:381/468; Training loss:0.099243\n",
      "Epoch: 16/20; Batch:382/468; Training loss:0.0943594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20; Batch:383/468; Training loss:0.0968109\n",
      "Epoch: 16/20; Batch:384/468; Training loss:0.0953311\n",
      "Epoch: 16/20; Batch:385/468; Training loss:0.0945547\n",
      "Epoch: 16/20; Batch:386/468; Training loss:0.0968873\n",
      "Epoch: 16/20; Batch:387/468; Training loss:0.0961819\n",
      "Epoch: 16/20; Batch:388/468; Training loss:0.0938497\n",
      "Epoch: 16/20; Batch:389/468; Training loss:0.0968583\n",
      "Epoch: 16/20; Batch:390/468; Training loss:0.0928032\n",
      "Epoch: 16/20; Batch:391/468; Training loss:0.0968793\n",
      "Epoch: 16/20; Batch:392/468; Training loss:0.0976138\n",
      "Epoch: 16/20; Batch:393/468; Training loss:0.0964786\n",
      "Epoch: 16/20; Batch:394/468; Training loss:0.0975013\n",
      "Epoch: 16/20; Batch:395/468; Training loss:0.0966022\n",
      "Epoch: 16/20; Batch:396/468; Training loss:0.0962879\n",
      "Epoch: 16/20; Batch:397/468; Training loss:0.0954769\n",
      "Epoch: 16/20; Batch:398/468; Training loss:0.101054\n",
      "Epoch: 16/20; Batch:399/468; Training loss:0.095783\n",
      "Epoch: 16/20; Batch:400/468; Training loss:0.0941556\n",
      "Epoch: 16/20; Batch:401/468; Training loss:0.0970236\n",
      "Epoch: 16/20; Batch:402/468; Training loss:0.0972552\n",
      "Epoch: 16/20; Batch:403/468; Training loss:0.0955499\n",
      "Epoch: 16/20; Batch:404/468; Training loss:0.0975826\n",
      "Epoch: 16/20; Batch:405/468; Training loss:0.0940384\n",
      "Epoch: 16/20; Batch:406/468; Training loss:0.100697\n",
      "Epoch: 16/20; Batch:407/468; Training loss:0.0932615\n",
      "Epoch: 16/20; Batch:408/468; Training loss:0.095294\n",
      "Epoch: 16/20; Batch:409/468; Training loss:0.094554\n",
      "Epoch: 16/20; Batch:410/468; Training loss:0.0929827\n",
      "Epoch: 16/20; Batch:411/468; Training loss:0.0956483\n",
      "Epoch: 16/20; Batch:412/468; Training loss:0.0980889\n",
      "Epoch: 16/20; Batch:413/468; Training loss:0.0939668\n",
      "Epoch: 16/20; Batch:414/468; Training loss:0.0973106\n",
      "Epoch: 16/20; Batch:415/468; Training loss:0.0985802\n",
      "Epoch: 16/20; Batch:416/468; Training loss:0.0963862\n",
      "Epoch: 16/20; Batch:417/468; Training loss:0.0985926\n",
      "Epoch: 16/20; Batch:418/468; Training loss:0.0993463\n",
      "Epoch: 16/20; Batch:419/468; Training loss:0.0976668\n",
      "Epoch: 16/20; Batch:420/468; Training loss:0.0958395\n",
      "Epoch: 16/20; Batch:421/468; Training loss:0.0992583\n",
      "Epoch: 16/20; Batch:422/468; Training loss:0.0979596\n",
      "Epoch: 16/20; Batch:423/468; Training loss:0.0950996\n",
      "Epoch: 16/20; Batch:424/468; Training loss:0.0961796\n",
      "Epoch: 16/20; Batch:425/468; Training loss:0.0942364\n",
      "Epoch: 16/20; Batch:426/468; Training loss:0.0989857\n",
      "Epoch: 16/20; Batch:427/468; Training loss:0.0954603\n",
      "Epoch: 16/20; Batch:428/468; Training loss:0.0968415\n",
      "Epoch: 16/20; Batch:429/468; Training loss:0.0966064\n",
      "Epoch: 16/20; Batch:430/468; Training loss:0.0979054\n",
      "Epoch: 16/20; Batch:431/468; Training loss:0.0950315\n",
      "Epoch: 16/20; Batch:432/468; Training loss:0.0977176\n",
      "Epoch: 16/20; Batch:433/468; Training loss:0.0984487\n",
      "Epoch: 16/20; Batch:434/468; Training loss:0.0974552\n",
      "Epoch: 16/20; Batch:435/468; Training loss:0.0988957\n",
      "Epoch: 16/20; Batch:436/468; Training loss:0.0908738\n",
      "Epoch: 16/20; Batch:437/468; Training loss:0.0970502\n",
      "Epoch: 16/20; Batch:438/468; Training loss:0.0964595\n",
      "Epoch: 16/20; Batch:439/468; Training loss:0.0954208\n",
      "Epoch: 16/20; Batch:440/468; Training loss:0.0925395\n",
      "Epoch: 16/20; Batch:441/468; Training loss:0.0957425\n",
      "Epoch: 16/20; Batch:442/468; Training loss:0.0908192\n",
      "Epoch: 16/20; Batch:443/468; Training loss:0.096576\n",
      "Epoch: 16/20; Batch:444/468; Training loss:0.0959843\n",
      "Epoch: 16/20; Batch:445/468; Training loss:0.093277\n",
      "Epoch: 16/20; Batch:446/468; Training loss:0.0939378\n",
      "Epoch: 16/20; Batch:447/468; Training loss:0.0972399\n",
      "Epoch: 16/20; Batch:448/468; Training loss:0.0976014\n",
      "Epoch: 16/20; Batch:449/468; Training loss:0.100377\n",
      "Epoch: 16/20; Batch:450/468; Training loss:0.0990367\n",
      "Epoch: 16/20; Batch:451/468; Training loss:0.100079\n",
      "Epoch: 16/20; Batch:452/468; Training loss:0.0954656\n",
      "Epoch: 16/20; Batch:453/468; Training loss:0.0954407\n",
      "Epoch: 16/20; Batch:454/468; Training loss:0.0981679\n",
      "Epoch: 16/20; Batch:455/468; Training loss:0.0963766\n",
      "Epoch: 16/20; Batch:456/468; Training loss:0.0979331\n",
      "Epoch: 16/20; Batch:457/468; Training loss:0.0989607\n",
      "Epoch: 16/20; Batch:458/468; Training loss:0.09569\n",
      "Epoch: 16/20; Batch:459/468; Training loss:0.0948483\n",
      "Epoch: 16/20; Batch:460/468; Training loss:0.0962003\n",
      "Epoch: 16/20; Batch:461/468; Training loss:0.0983788\n",
      "Epoch: 16/20; Batch:462/468; Training loss:0.0973393\n",
      "Epoch: 16/20; Batch:463/468; Training loss:0.0976727\n",
      "Epoch: 16/20; Batch:464/468; Training loss:0.0986713\n",
      "Epoch: 16/20; Batch:465/468; Training loss:0.0946683\n",
      "Epoch: 16/20; Batch:466/468; Training loss:0.0944049\n",
      "Epoch: 16/20; Batch:467/468; Training loss:0.0984845\n",
      "Epoch: 16/20; Batch:468/468; Training loss:0.0968831\n",
      "Epoch: 17/20; Batch:1/468; Training loss:0.0946021\n",
      "Epoch: 17/20; Batch:2/468; Training loss:0.0967742\n",
      "Epoch: 17/20; Batch:3/468; Training loss:0.0959246\n",
      "Epoch: 17/20; Batch:4/468; Training loss:0.0948402\n",
      "Epoch: 17/20; Batch:5/468; Training loss:0.0971618\n",
      "Epoch: 17/20; Batch:6/468; Training loss:0.0966771\n",
      "Epoch: 17/20; Batch:7/468; Training loss:0.0939789\n",
      "Epoch: 17/20; Batch:8/468; Training loss:0.0944322\n",
      "Epoch: 17/20; Batch:9/468; Training loss:0.0980213\n",
      "Epoch: 17/20; Batch:10/468; Training loss:0.0942726\n",
      "Epoch: 17/20; Batch:11/468; Training loss:0.0953879\n",
      "Epoch: 17/20; Batch:12/468; Training loss:0.0937697\n",
      "Epoch: 17/20; Batch:13/468; Training loss:0.0976107\n",
      "Epoch: 17/20; Batch:14/468; Training loss:0.0956502\n",
      "Epoch: 17/20; Batch:15/468; Training loss:0.0910539\n",
      "Epoch: 17/20; Batch:16/468; Training loss:0.0970482\n",
      "Epoch: 17/20; Batch:17/468; Training loss:0.0960102\n",
      "Epoch: 17/20; Batch:18/468; Training loss:0.0988651\n",
      "Epoch: 17/20; Batch:19/468; Training loss:0.0934924\n",
      "Epoch: 17/20; Batch:20/468; Training loss:0.0982675\n",
      "Epoch: 17/20; Batch:21/468; Training loss:0.0980047\n",
      "Epoch: 17/20; Batch:22/468; Training loss:0.0955376\n",
      "Epoch: 17/20; Batch:23/468; Training loss:0.0973639\n",
      "Epoch: 17/20; Batch:24/468; Training loss:0.0981217\n",
      "Epoch: 17/20; Batch:25/468; Training loss:0.0993062\n",
      "Epoch: 17/20; Batch:26/468; Training loss:0.0962128\n",
      "Epoch: 17/20; Batch:27/468; Training loss:0.0936614\n",
      "Epoch: 17/20; Batch:28/468; Training loss:0.0962687\n",
      "Epoch: 17/20; Batch:29/468; Training loss:0.0978015\n",
      "Epoch: 17/20; Batch:30/468; Training loss:0.0957316\n",
      "Epoch: 17/20; Batch:31/468; Training loss:0.0965274\n",
      "Epoch: 17/20; Batch:32/468; Training loss:0.0984047\n",
      "Epoch: 17/20; Batch:33/468; Training loss:0.0994554\n",
      "Epoch: 17/20; Batch:34/468; Training loss:0.0996702\n",
      "Epoch: 17/20; Batch:35/468; Training loss:0.0993515\n",
      "Epoch: 17/20; Batch:36/468; Training loss:0.0948479\n",
      "Epoch: 17/20; Batch:37/468; Training loss:0.0937263\n",
      "Epoch: 17/20; Batch:38/468; Training loss:0.0921881\n",
      "Epoch: 17/20; Batch:39/468; Training loss:0.0954423\n",
      "Epoch: 17/20; Batch:40/468; Training loss:0.0959354\n",
      "Epoch: 17/20; Batch:41/468; Training loss:0.0964973\n",
      "Epoch: 17/20; Batch:42/468; Training loss:0.0946173\n",
      "Epoch: 17/20; Batch:43/468; Training loss:0.0969869\n",
      "Epoch: 17/20; Batch:44/468; Training loss:0.096576\n",
      "Epoch: 17/20; Batch:45/468; Training loss:0.097543\n",
      "Epoch: 17/20; Batch:46/468; Training loss:0.096039\n",
      "Epoch: 17/20; Batch:47/468; Training loss:0.0948964\n",
      "Epoch: 17/20; Batch:48/468; Training loss:0.0972626\n",
      "Epoch: 17/20; Batch:49/468; Training loss:0.0956429\n",
      "Epoch: 17/20; Batch:50/468; Training loss:0.0969452\n",
      "Epoch: 17/20; Batch:51/468; Training loss:0.0970785\n",
      "Epoch: 17/20; Batch:52/468; Training loss:0.0904121\n",
      "Epoch: 17/20; Batch:53/468; Training loss:0.0963902\n",
      "Epoch: 17/20; Batch:54/468; Training loss:0.100451\n",
      "Epoch: 17/20; Batch:55/468; Training loss:0.0966791\n",
      "Epoch: 17/20; Batch:56/468; Training loss:0.0974066\n",
      "Epoch: 17/20; Batch:57/468; Training loss:0.0934156\n",
      "Epoch: 17/20; Batch:58/468; Training loss:0.0999622\n",
      "Epoch: 17/20; Batch:59/468; Training loss:0.0988944\n",
      "Epoch: 17/20; Batch:60/468; Training loss:0.0937605\n",
      "Epoch: 17/20; Batch:61/468; Training loss:0.0997761\n",
      "Epoch: 17/20; Batch:62/468; Training loss:0.094892\n",
      "Epoch: 17/20; Batch:63/468; Training loss:0.0941326\n",
      "Epoch: 17/20; Batch:64/468; Training loss:0.0979324\n",
      "Epoch: 17/20; Batch:65/468; Training loss:0.0994849\n",
      "Epoch: 17/20; Batch:66/468; Training loss:0.0975012\n",
      "Epoch: 17/20; Batch:67/468; Training loss:0.0988705\n",
      "Epoch: 17/20; Batch:68/468; Training loss:0.0967526\n",
      "Epoch: 17/20; Batch:69/468; Training loss:0.0972257\n",
      "Epoch: 17/20; Batch:70/468; Training loss:0.0993501\n",
      "Epoch: 17/20; Batch:71/468; Training loss:0.0956621\n",
      "Epoch: 17/20; Batch:72/468; Training loss:0.0948637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20; Batch:73/468; Training loss:0.0962532\n",
      "Epoch: 17/20; Batch:74/468; Training loss:0.0953773\n",
      "Epoch: 17/20; Batch:75/468; Training loss:0.101643\n",
      "Epoch: 17/20; Batch:76/468; Training loss:0.102707\n",
      "Epoch: 17/20; Batch:77/468; Training loss:0.0972176\n",
      "Epoch: 17/20; Batch:78/468; Training loss:0.0933551\n",
      "Epoch: 17/20; Batch:79/468; Training loss:0.10001\n",
      "Epoch: 17/20; Batch:80/468; Training loss:0.0946501\n",
      "Epoch: 17/20; Batch:81/468; Training loss:0.0960808\n",
      "Epoch: 17/20; Batch:82/468; Training loss:0.0938328\n",
      "Epoch: 17/20; Batch:83/468; Training loss:0.090958\n",
      "Epoch: 17/20; Batch:84/468; Training loss:0.0973401\n",
      "Epoch: 17/20; Batch:85/468; Training loss:0.0973095\n",
      "Epoch: 17/20; Batch:86/468; Training loss:0.0977199\n",
      "Epoch: 17/20; Batch:87/468; Training loss:0.0951423\n",
      "Epoch: 17/20; Batch:88/468; Training loss:0.0935677\n",
      "Epoch: 17/20; Batch:89/468; Training loss:0.0939042\n",
      "Epoch: 17/20; Batch:90/468; Training loss:0.0971121\n",
      "Epoch: 17/20; Batch:91/468; Training loss:0.0921966\n",
      "Epoch: 17/20; Batch:92/468; Training loss:0.0996446\n",
      "Epoch: 17/20; Batch:93/468; Training loss:0.0940752\n",
      "Epoch: 17/20; Batch:94/468; Training loss:0.0968555\n",
      "Epoch: 17/20; Batch:95/468; Training loss:0.100439\n",
      "Epoch: 17/20; Batch:96/468; Training loss:0.100846\n",
      "Epoch: 17/20; Batch:97/468; Training loss:0.0951061\n",
      "Epoch: 17/20; Batch:98/468; Training loss:0.0964227\n",
      "Epoch: 17/20; Batch:99/468; Training loss:0.0917823\n",
      "Epoch: 17/20; Batch:100/468; Training loss:0.0970871\n",
      "Epoch: 17/20; Batch:101/468; Training loss:0.094051\n",
      "Epoch: 17/20; Batch:102/468; Training loss:0.0967314\n",
      "Epoch: 17/20; Batch:103/468; Training loss:0.0963032\n",
      "Epoch: 17/20; Batch:104/468; Training loss:0.0938881\n",
      "Epoch: 17/20; Batch:105/468; Training loss:0.0948142\n",
      "Epoch: 17/20; Batch:106/468; Training loss:0.0967264\n",
      "Epoch: 17/20; Batch:107/468; Training loss:0.0983376\n",
      "Epoch: 17/20; Batch:108/468; Training loss:0.0960365\n",
      "Epoch: 17/20; Batch:109/468; Training loss:0.0975307\n",
      "Epoch: 17/20; Batch:110/468; Training loss:0.0994791\n",
      "Epoch: 17/20; Batch:111/468; Training loss:0.0966096\n",
      "Epoch: 17/20; Batch:112/468; Training loss:0.0969246\n",
      "Epoch: 17/20; Batch:113/468; Training loss:0.0949699\n",
      "Epoch: 17/20; Batch:114/468; Training loss:0.0952919\n",
      "Epoch: 17/20; Batch:115/468; Training loss:0.0939482\n",
      "Epoch: 17/20; Batch:116/468; Training loss:0.0911553\n",
      "Epoch: 17/20; Batch:117/468; Training loss:0.10168\n",
      "Epoch: 17/20; Batch:118/468; Training loss:0.0937641\n",
      "Epoch: 17/20; Batch:119/468; Training loss:0.0966786\n",
      "Epoch: 17/20; Batch:120/468; Training loss:0.0961847\n",
      "Epoch: 17/20; Batch:121/468; Training loss:0.0970814\n",
      "Epoch: 17/20; Batch:122/468; Training loss:0.0969662\n",
      "Epoch: 17/20; Batch:123/468; Training loss:0.0958758\n",
      "Epoch: 17/20; Batch:124/468; Training loss:0.0987864\n",
      "Epoch: 17/20; Batch:125/468; Training loss:0.0915864\n",
      "Epoch: 17/20; Batch:126/468; Training loss:0.0985646\n",
      "Epoch: 17/20; Batch:127/468; Training loss:0.0982773\n",
      "Epoch: 17/20; Batch:128/468; Training loss:0.0946246\n",
      "Epoch: 17/20; Batch:129/468; Training loss:0.099719\n",
      "Epoch: 17/20; Batch:130/468; Training loss:0.0976957\n",
      "Epoch: 17/20; Batch:131/468; Training loss:0.0958403\n",
      "Epoch: 17/20; Batch:132/468; Training loss:0.0975534\n",
      "Epoch: 17/20; Batch:133/468; Training loss:0.0935891\n",
      "Epoch: 17/20; Batch:134/468; Training loss:0.0925727\n",
      "Epoch: 17/20; Batch:135/468; Training loss:0.0980325\n",
      "Epoch: 17/20; Batch:136/468; Training loss:0.0995367\n",
      "Epoch: 17/20; Batch:137/468; Training loss:0.0951076\n",
      "Epoch: 17/20; Batch:138/468; Training loss:0.0961284\n",
      "Epoch: 17/20; Batch:139/468; Training loss:0.0956168\n",
      "Epoch: 17/20; Batch:140/468; Training loss:0.0975469\n",
      "Epoch: 17/20; Batch:141/468; Training loss:0.100699\n",
      "Epoch: 17/20; Batch:142/468; Training loss:0.0958403\n",
      "Epoch: 17/20; Batch:143/468; Training loss:0.0949142\n",
      "Epoch: 17/20; Batch:144/468; Training loss:0.0959902\n",
      "Epoch: 17/20; Batch:145/468; Training loss:0.0970989\n",
      "Epoch: 17/20; Batch:146/468; Training loss:0.0963603\n",
      "Epoch: 17/20; Batch:147/468; Training loss:0.094954\n",
      "Epoch: 17/20; Batch:148/468; Training loss:0.093607\n",
      "Epoch: 17/20; Batch:149/468; Training loss:0.0974559\n",
      "Epoch: 17/20; Batch:150/468; Training loss:0.0957476\n",
      "Epoch: 17/20; Batch:151/468; Training loss:0.0984513\n",
      "Epoch: 17/20; Batch:152/468; Training loss:0.0961405\n",
      "Epoch: 17/20; Batch:153/468; Training loss:0.0949249\n",
      "Epoch: 17/20; Batch:154/468; Training loss:0.0943728\n",
      "Epoch: 17/20; Batch:155/468; Training loss:0.0946723\n",
      "Epoch: 17/20; Batch:156/468; Training loss:0.0970754\n",
      "Epoch: 17/20; Batch:157/468; Training loss:0.0975767\n",
      "Epoch: 17/20; Batch:158/468; Training loss:0.0950686\n",
      "Epoch: 17/20; Batch:159/468; Training loss:0.0991822\n",
      "Epoch: 17/20; Batch:160/468; Training loss:0.0981568\n",
      "Epoch: 17/20; Batch:161/468; Training loss:0.0960224\n",
      "Epoch: 17/20; Batch:162/468; Training loss:0.0965893\n",
      "Epoch: 17/20; Batch:163/468; Training loss:0.0984443\n",
      "Epoch: 17/20; Batch:164/468; Training loss:0.0982413\n",
      "Epoch: 17/20; Batch:165/468; Training loss:0.0958819\n",
      "Epoch: 17/20; Batch:166/468; Training loss:0.0944545\n",
      "Epoch: 17/20; Batch:167/468; Training loss:0.0963428\n",
      "Epoch: 17/20; Batch:168/468; Training loss:0.0950026\n",
      "Epoch: 17/20; Batch:169/468; Training loss:0.098738\n",
      "Epoch: 17/20; Batch:170/468; Training loss:0.095725\n",
      "Epoch: 17/20; Batch:171/468; Training loss:0.0979266\n",
      "Epoch: 17/20; Batch:172/468; Training loss:0.0926929\n",
      "Epoch: 17/20; Batch:173/468; Training loss:0.0987369\n",
      "Epoch: 17/20; Batch:174/468; Training loss:0.0953735\n",
      "Epoch: 17/20; Batch:175/468; Training loss:0.0984244\n",
      "Epoch: 17/20; Batch:176/468; Training loss:0.0913635\n",
      "Epoch: 17/20; Batch:177/468; Training loss:0.100096\n",
      "Epoch: 17/20; Batch:178/468; Training loss:0.0973699\n",
      "Epoch: 17/20; Batch:179/468; Training loss:0.0989837\n",
      "Epoch: 17/20; Batch:180/468; Training loss:0.0999833\n",
      "Epoch: 17/20; Batch:181/468; Training loss:0.0931563\n",
      "Epoch: 17/20; Batch:182/468; Training loss:0.0960344\n",
      "Epoch: 17/20; Batch:183/468; Training loss:0.0956188\n",
      "Epoch: 17/20; Batch:184/468; Training loss:0.099465\n",
      "Epoch: 17/20; Batch:185/468; Training loss:0.0979044\n",
      "Epoch: 17/20; Batch:186/468; Training loss:0.0921646\n",
      "Epoch: 17/20; Batch:187/468; Training loss:0.0987261\n",
      "Epoch: 17/20; Batch:188/468; Training loss:0.0951726\n",
      "Epoch: 17/20; Batch:189/468; Training loss:0.100858\n",
      "Epoch: 17/20; Batch:190/468; Training loss:0.095378\n",
      "Epoch: 17/20; Batch:191/468; Training loss:0.0977514\n",
      "Epoch: 17/20; Batch:192/468; Training loss:0.100038\n",
      "Epoch: 17/20; Batch:193/468; Training loss:0.0989525\n",
      "Epoch: 17/20; Batch:194/468; Training loss:0.0952395\n",
      "Epoch: 17/20; Batch:195/468; Training loss:0.0942633\n",
      "Epoch: 17/20; Batch:196/468; Training loss:0.093463\n",
      "Epoch: 17/20; Batch:197/468; Training loss:0.0932737\n",
      "Epoch: 17/20; Batch:198/468; Training loss:0.0980775\n",
      "Epoch: 17/20; Batch:199/468; Training loss:0.097403\n",
      "Epoch: 17/20; Batch:200/468; Training loss:0.0955972\n",
      "Epoch: 17/20; Batch:201/468; Training loss:0.0999807\n",
      "Epoch: 17/20; Batch:202/468; Training loss:0.0963953\n",
      "Epoch: 17/20; Batch:203/468; Training loss:0.0938278\n",
      "Epoch: 17/20; Batch:204/468; Training loss:0.0943795\n",
      "Epoch: 17/20; Batch:205/468; Training loss:0.0956028\n",
      "Epoch: 17/20; Batch:206/468; Training loss:0.0954844\n",
      "Epoch: 17/20; Batch:207/468; Training loss:0.0930995\n",
      "Epoch: 17/20; Batch:208/468; Training loss:0.10124\n",
      "Epoch: 17/20; Batch:209/468; Training loss:0.100686\n",
      "Epoch: 17/20; Batch:210/468; Training loss:0.0958122\n",
      "Epoch: 17/20; Batch:211/468; Training loss:0.0933963\n",
      "Epoch: 17/20; Batch:212/468; Training loss:0.098158\n",
      "Epoch: 17/20; Batch:213/468; Training loss:0.0955626\n",
      "Epoch: 17/20; Batch:214/468; Training loss:0.0987705\n",
      "Epoch: 17/20; Batch:215/468; Training loss:0.0991529\n",
      "Epoch: 17/20; Batch:216/468; Training loss:0.0959944\n",
      "Epoch: 17/20; Batch:217/468; Training loss:0.0972618\n",
      "Epoch: 17/20; Batch:218/468; Training loss:0.0924573\n",
      "Epoch: 17/20; Batch:219/468; Training loss:0.0967096\n",
      "Epoch: 17/20; Batch:220/468; Training loss:0.0978039\n",
      "Epoch: 17/20; Batch:221/468; Training loss:0.097156\n",
      "Epoch: 17/20; Batch:222/468; Training loss:0.0982177\n",
      "Epoch: 17/20; Batch:223/468; Training loss:0.0945513\n",
      "Epoch: 17/20; Batch:224/468; Training loss:0.0967403\n",
      "Epoch: 17/20; Batch:225/468; Training loss:0.0963881\n",
      "Epoch: 17/20; Batch:226/468; Training loss:0.0974194\n",
      "Epoch: 17/20; Batch:227/468; Training loss:0.0948451\n",
      "Epoch: 17/20; Batch:228/468; Training loss:0.0950857\n",
      "Epoch: 17/20; Batch:229/468; Training loss:0.0982841\n",
      "Epoch: 17/20; Batch:230/468; Training loss:0.0996799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20; Batch:231/468; Training loss:0.0983102\n",
      "Epoch: 17/20; Batch:232/468; Training loss:0.0968778\n",
      "Epoch: 17/20; Batch:233/468; Training loss:0.0949614\n",
      "Epoch: 17/20; Batch:234/468; Training loss:0.0948414\n",
      "Epoch: 17/20; Batch:235/468; Training loss:0.0952768\n",
      "Epoch: 17/20; Batch:236/468; Training loss:0.0931411\n",
      "Epoch: 17/20; Batch:237/468; Training loss:0.0946537\n",
      "Epoch: 17/20; Batch:238/468; Training loss:0.0961186\n",
      "Epoch: 17/20; Batch:239/468; Training loss:0.0942179\n",
      "Epoch: 17/20; Batch:240/468; Training loss:0.0955372\n",
      "Epoch: 17/20; Batch:241/468; Training loss:0.100242\n",
      "Epoch: 17/20; Batch:242/468; Training loss:0.0966395\n",
      "Epoch: 17/20; Batch:243/468; Training loss:0.095018\n",
      "Epoch: 17/20; Batch:244/468; Training loss:0.0976522\n",
      "Epoch: 17/20; Batch:245/468; Training loss:0.0952752\n",
      "Epoch: 17/20; Batch:246/468; Training loss:0.101145\n",
      "Epoch: 17/20; Batch:247/468; Training loss:0.0973281\n",
      "Epoch: 17/20; Batch:248/468; Training loss:0.0959341\n",
      "Epoch: 17/20; Batch:249/468; Training loss:0.0975077\n",
      "Epoch: 17/20; Batch:250/468; Training loss:0.101511\n",
      "Epoch: 17/20; Batch:251/468; Training loss:0.0979803\n",
      "Epoch: 17/20; Batch:252/468; Training loss:0.0968492\n",
      "Epoch: 17/20; Batch:253/468; Training loss:0.0975794\n",
      "Epoch: 17/20; Batch:254/468; Training loss:0.095709\n",
      "Epoch: 17/20; Batch:255/468; Training loss:0.0979016\n",
      "Epoch: 17/20; Batch:256/468; Training loss:0.0978597\n",
      "Epoch: 17/20; Batch:257/468; Training loss:0.0931257\n",
      "Epoch: 17/20; Batch:258/468; Training loss:0.100172\n",
      "Epoch: 17/20; Batch:259/468; Training loss:0.0945334\n",
      "Epoch: 17/20; Batch:260/468; Training loss:0.0943738\n",
      "Epoch: 17/20; Batch:261/468; Training loss:0.0947588\n",
      "Epoch: 17/20; Batch:262/468; Training loss:0.0961193\n",
      "Epoch: 17/20; Batch:263/468; Training loss:0.0971817\n",
      "Epoch: 17/20; Batch:264/468; Training loss:0.0938847\n",
      "Epoch: 17/20; Batch:265/468; Training loss:0.0954978\n",
      "Epoch: 17/20; Batch:266/468; Training loss:0.0959363\n",
      "Epoch: 17/20; Batch:267/468; Training loss:0.095774\n",
      "Epoch: 17/20; Batch:268/468; Training loss:0.0955532\n",
      "Epoch: 17/20; Batch:269/468; Training loss:0.0957512\n",
      "Epoch: 17/20; Batch:270/468; Training loss:0.0975912\n",
      "Epoch: 17/20; Batch:271/468; Training loss:0.098417\n",
      "Epoch: 17/20; Batch:272/468; Training loss:0.0932106\n",
      "Epoch: 17/20; Batch:273/468; Training loss:0.0967298\n",
      "Epoch: 17/20; Batch:274/468; Training loss:0.094752\n",
      "Epoch: 17/20; Batch:275/468; Training loss:0.094782\n",
      "Epoch: 17/20; Batch:276/468; Training loss:0.0975389\n",
      "Epoch: 17/20; Batch:277/468; Training loss:0.097777\n",
      "Epoch: 17/20; Batch:278/468; Training loss:0.0996639\n",
      "Epoch: 17/20; Batch:279/468; Training loss:0.0952143\n",
      "Epoch: 17/20; Batch:280/468; Training loss:0.098518\n",
      "Epoch: 17/20; Batch:281/468; Training loss:0.0957931\n",
      "Epoch: 17/20; Batch:282/468; Training loss:0.0994277\n",
      "Epoch: 17/20; Batch:283/468; Training loss:0.0960865\n",
      "Epoch: 17/20; Batch:284/468; Training loss:0.0977694\n",
      "Epoch: 17/20; Batch:285/468; Training loss:0.101871\n",
      "Epoch: 17/20; Batch:286/468; Training loss:0.0996016\n",
      "Epoch: 17/20; Batch:287/468; Training loss:0.0958538\n",
      "Epoch: 17/20; Batch:288/468; Training loss:0.0966024\n",
      "Epoch: 17/20; Batch:289/468; Training loss:0.0939209\n",
      "Epoch: 17/20; Batch:290/468; Training loss:0.0944741\n",
      "Epoch: 17/20; Batch:291/468; Training loss:0.0986058\n",
      "Epoch: 17/20; Batch:292/468; Training loss:0.096729\n",
      "Epoch: 17/20; Batch:293/468; Training loss:0.0969771\n",
      "Epoch: 17/20; Batch:294/468; Training loss:0.091913\n",
      "Epoch: 17/20; Batch:295/468; Training loss:0.0957675\n",
      "Epoch: 17/20; Batch:296/468; Training loss:0.0983034\n",
      "Epoch: 17/20; Batch:297/468; Training loss:0.0957974\n",
      "Epoch: 17/20; Batch:298/468; Training loss:0.0963465\n",
      "Epoch: 17/20; Batch:299/468; Training loss:0.0971507\n",
      "Epoch: 17/20; Batch:300/468; Training loss:0.0963322\n",
      "Epoch: 17/20; Batch:301/468; Training loss:0.0930044\n",
      "Epoch: 17/20; Batch:302/468; Training loss:0.0955209\n",
      "Epoch: 17/20; Batch:303/468; Training loss:0.0972888\n",
      "Epoch: 17/20; Batch:304/468; Training loss:0.0946085\n",
      "Epoch: 17/20; Batch:305/468; Training loss:0.0959799\n",
      "Epoch: 17/20; Batch:306/468; Training loss:0.0987306\n",
      "Epoch: 17/20; Batch:307/468; Training loss:0.0958524\n",
      "Epoch: 17/20; Batch:308/468; Training loss:0.0970152\n",
      "Epoch: 17/20; Batch:309/468; Training loss:0.0939819\n",
      "Epoch: 17/20; Batch:310/468; Training loss:0.101158\n",
      "Epoch: 17/20; Batch:311/468; Training loss:0.097697\n",
      "Epoch: 17/20; Batch:312/468; Training loss:0.0958497\n",
      "Epoch: 17/20; Batch:313/468; Training loss:0.0967403\n",
      "Epoch: 17/20; Batch:314/468; Training loss:0.0937569\n",
      "Epoch: 17/20; Batch:315/468; Training loss:0.0959761\n",
      "Epoch: 17/20; Batch:316/468; Training loss:0.101833\n",
      "Epoch: 17/20; Batch:317/468; Training loss:0.0918677\n",
      "Epoch: 17/20; Batch:318/468; Training loss:0.0948278\n",
      "Epoch: 17/20; Batch:319/468; Training loss:0.0957\n",
      "Epoch: 17/20; Batch:320/468; Training loss:0.0958441\n",
      "Epoch: 17/20; Batch:321/468; Training loss:0.093848\n",
      "Epoch: 17/20; Batch:322/468; Training loss:0.0947562\n",
      "Epoch: 17/20; Batch:323/468; Training loss:0.0937883\n",
      "Epoch: 17/20; Batch:324/468; Training loss:0.100317\n",
      "Epoch: 17/20; Batch:325/468; Training loss:0.0963063\n",
      "Epoch: 17/20; Batch:326/468; Training loss:0.093488\n",
      "Epoch: 17/20; Batch:327/468; Training loss:0.0936575\n",
      "Epoch: 17/20; Batch:328/468; Training loss:0.0923218\n",
      "Epoch: 17/20; Batch:329/468; Training loss:0.101392\n",
      "Epoch: 17/20; Batch:330/468; Training loss:0.0925946\n",
      "Epoch: 17/20; Batch:331/468; Training loss:0.0990966\n",
      "Epoch: 17/20; Batch:332/468; Training loss:0.0922137\n",
      "Epoch: 17/20; Batch:333/468; Training loss:0.0945325\n",
      "Epoch: 17/20; Batch:334/468; Training loss:0.0985362\n",
      "Epoch: 17/20; Batch:335/468; Training loss:0.0980801\n",
      "Epoch: 17/20; Batch:336/468; Training loss:0.0955177\n",
      "Epoch: 17/20; Batch:337/468; Training loss:0.0914994\n",
      "Epoch: 17/20; Batch:338/468; Training loss:0.0972543\n",
      "Epoch: 17/20; Batch:339/468; Training loss:0.098788\n",
      "Epoch: 17/20; Batch:340/468; Training loss:0.0978655\n",
      "Epoch: 17/20; Batch:341/468; Training loss:0.0956035\n",
      "Epoch: 17/20; Batch:342/468; Training loss:0.0914459\n",
      "Epoch: 17/20; Batch:343/468; Training loss:0.0957133\n",
      "Epoch: 17/20; Batch:344/468; Training loss:0.0922738\n",
      "Epoch: 17/20; Batch:345/468; Training loss:0.0947\n",
      "Epoch: 17/20; Batch:346/468; Training loss:0.0967873\n",
      "Epoch: 17/20; Batch:347/468; Training loss:0.0973285\n",
      "Epoch: 17/20; Batch:348/468; Training loss:0.0951117\n",
      "Epoch: 17/20; Batch:349/468; Training loss:0.0968666\n",
      "Epoch: 17/20; Batch:350/468; Training loss:0.0924754\n",
      "Epoch: 17/20; Batch:351/468; Training loss:0.0992505\n",
      "Epoch: 17/20; Batch:352/468; Training loss:0.0983215\n",
      "Epoch: 17/20; Batch:353/468; Training loss:0.0986464\n",
      "Epoch: 17/20; Batch:354/468; Training loss:0.097129\n",
      "Epoch: 17/20; Batch:355/468; Training loss:0.0984993\n",
      "Epoch: 17/20; Batch:356/468; Training loss:0.0952766\n",
      "Epoch: 17/20; Batch:357/468; Training loss:0.0938554\n",
      "Epoch: 17/20; Batch:358/468; Training loss:0.0990636\n",
      "Epoch: 17/20; Batch:359/468; Training loss:0.0971713\n",
      "Epoch: 17/20; Batch:360/468; Training loss:0.0957144\n",
      "Epoch: 17/20; Batch:361/468; Training loss:0.0948686\n",
      "Epoch: 17/20; Batch:362/468; Training loss:0.0994502\n",
      "Epoch: 17/20; Batch:363/468; Training loss:0.102029\n",
      "Epoch: 17/20; Batch:364/468; Training loss:0.0949661\n",
      "Epoch: 17/20; Batch:365/468; Training loss:0.0936446\n",
      "Epoch: 17/20; Batch:366/468; Training loss:0.0925903\n",
      "Epoch: 17/20; Batch:367/468; Training loss:0.0968087\n",
      "Epoch: 17/20; Batch:368/468; Training loss:0.0980797\n",
      "Epoch: 17/20; Batch:369/468; Training loss:0.0984076\n",
      "Epoch: 17/20; Batch:370/468; Training loss:0.0913509\n",
      "Epoch: 17/20; Batch:371/468; Training loss:0.0932805\n",
      "Epoch: 17/20; Batch:372/468; Training loss:0.0981143\n",
      "Epoch: 17/20; Batch:373/468; Training loss:0.0995579\n",
      "Epoch: 17/20; Batch:374/468; Training loss:0.0984342\n",
      "Epoch: 17/20; Batch:375/468; Training loss:0.0978538\n",
      "Epoch: 17/20; Batch:376/468; Training loss:0.102459\n",
      "Epoch: 17/20; Batch:377/468; Training loss:0.0945648\n",
      "Epoch: 17/20; Batch:378/468; Training loss:0.0987491\n",
      "Epoch: 17/20; Batch:379/468; Training loss:0.098382\n",
      "Epoch: 17/20; Batch:380/468; Training loss:0.0980505\n",
      "Epoch: 17/20; Batch:381/468; Training loss:0.0983518\n",
      "Epoch: 17/20; Batch:382/468; Training loss:0.0943565\n",
      "Epoch: 17/20; Batch:383/468; Training loss:0.0926558\n",
      "Epoch: 17/20; Batch:384/468; Training loss:0.0980332\n",
      "Epoch: 17/20; Batch:385/468; Training loss:0.0949032\n",
      "Epoch: 17/20; Batch:386/468; Training loss:0.0978141\n",
      "Epoch: 17/20; Batch:387/468; Training loss:0.0969756\n",
      "Epoch: 17/20; Batch:388/468; Training loss:0.0987929\n",
      "Epoch: 17/20; Batch:389/468; Training loss:0.0950348\n",
      "Epoch: 17/20; Batch:390/468; Training loss:0.0977772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20; Batch:391/468; Training loss:0.0974527\n",
      "Epoch: 17/20; Batch:392/468; Training loss:0.0922137\n",
      "Epoch: 17/20; Batch:393/468; Training loss:0.0941765\n",
      "Epoch: 17/20; Batch:394/468; Training loss:0.0967117\n",
      "Epoch: 17/20; Batch:395/468; Training loss:0.09322\n",
      "Epoch: 17/20; Batch:396/468; Training loss:0.0922783\n",
      "Epoch: 17/20; Batch:397/468; Training loss:0.0954412\n",
      "Epoch: 17/20; Batch:398/468; Training loss:0.0943111\n",
      "Epoch: 17/20; Batch:399/468; Training loss:0.0978589\n",
      "Epoch: 17/20; Batch:400/468; Training loss:0.0973071\n",
      "Epoch: 17/20; Batch:401/468; Training loss:0.0964109\n",
      "Epoch: 17/20; Batch:402/468; Training loss:0.0977217\n",
      "Epoch: 17/20; Batch:403/468; Training loss:0.0992592\n",
      "Epoch: 17/20; Batch:404/468; Training loss:0.0959142\n",
      "Epoch: 17/20; Batch:405/468; Training loss:0.0942733\n",
      "Epoch: 17/20; Batch:406/468; Training loss:0.0925645\n",
      "Epoch: 17/20; Batch:407/468; Training loss:0.0932447\n",
      "Epoch: 17/20; Batch:408/468; Training loss:0.0964434\n",
      "Epoch: 17/20; Batch:409/468; Training loss:0.101251\n",
      "Epoch: 17/20; Batch:410/468; Training loss:0.0993597\n",
      "Epoch: 17/20; Batch:411/468; Training loss:0.0952033\n",
      "Epoch: 17/20; Batch:412/468; Training loss:0.0930926\n",
      "Epoch: 17/20; Batch:413/468; Training loss:0.0985056\n",
      "Epoch: 17/20; Batch:414/468; Training loss:0.0985141\n",
      "Epoch: 17/20; Batch:415/468; Training loss:0.101046\n",
      "Epoch: 17/20; Batch:416/468; Training loss:0.0975262\n",
      "Epoch: 17/20; Batch:417/468; Training loss:0.0974898\n",
      "Epoch: 17/20; Batch:418/468; Training loss:0.0935915\n",
      "Epoch: 17/20; Batch:419/468; Training loss:0.0952979\n",
      "Epoch: 17/20; Batch:420/468; Training loss:0.0974177\n",
      "Epoch: 17/20; Batch:421/468; Training loss:0.0975435\n",
      "Epoch: 17/20; Batch:422/468; Training loss:0.0942274\n",
      "Epoch: 17/20; Batch:423/468; Training loss:0.0967165\n",
      "Epoch: 17/20; Batch:424/468; Training loss:0.0994161\n",
      "Epoch: 17/20; Batch:425/468; Training loss:0.0956265\n",
      "Epoch: 17/20; Batch:426/468; Training loss:0.0940361\n",
      "Epoch: 17/20; Batch:427/468; Training loss:0.100878\n",
      "Epoch: 17/20; Batch:428/468; Training loss:0.0946468\n",
      "Epoch: 17/20; Batch:429/468; Training loss:0.0958538\n",
      "Epoch: 17/20; Batch:430/468; Training loss:0.0963129\n",
      "Epoch: 17/20; Batch:431/468; Training loss:0.094322\n",
      "Epoch: 17/20; Batch:432/468; Training loss:0.0959976\n",
      "Epoch: 17/20; Batch:433/468; Training loss:0.0956968\n",
      "Epoch: 17/20; Batch:434/468; Training loss:0.0975391\n",
      "Epoch: 17/20; Batch:435/468; Training loss:0.0968791\n",
      "Epoch: 17/20; Batch:436/468; Training loss:0.0994043\n",
      "Epoch: 17/20; Batch:437/468; Training loss:0.0972402\n",
      "Epoch: 17/20; Batch:438/468; Training loss:0.0998672\n",
      "Epoch: 17/20; Batch:439/468; Training loss:0.0953751\n",
      "Epoch: 17/20; Batch:440/468; Training loss:0.100105\n",
      "Epoch: 17/20; Batch:441/468; Training loss:0.0956542\n",
      "Epoch: 17/20; Batch:442/468; Training loss:0.0971035\n",
      "Epoch: 17/20; Batch:443/468; Training loss:0.0919515\n",
      "Epoch: 17/20; Batch:444/468; Training loss:0.0949767\n",
      "Epoch: 17/20; Batch:445/468; Training loss:0.0959837\n",
      "Epoch: 17/20; Batch:446/468; Training loss:0.0983201\n",
      "Epoch: 17/20; Batch:447/468; Training loss:0.0961115\n",
      "Epoch: 17/20; Batch:448/468; Training loss:0.0987137\n",
      "Epoch: 17/20; Batch:449/468; Training loss:0.0963177\n",
      "Epoch: 17/20; Batch:450/468; Training loss:0.0935113\n",
      "Epoch: 17/20; Batch:451/468; Training loss:0.0982716\n",
      "Epoch: 17/20; Batch:452/468; Training loss:0.0982536\n",
      "Epoch: 17/20; Batch:453/468; Training loss:0.0936597\n",
      "Epoch: 17/20; Batch:454/468; Training loss:0.092328\n",
      "Epoch: 17/20; Batch:455/468; Training loss:0.094731\n",
      "Epoch: 17/20; Batch:456/468; Training loss:0.0937659\n",
      "Epoch: 17/20; Batch:457/468; Training loss:0.0983227\n",
      "Epoch: 17/20; Batch:458/468; Training loss:0.0940062\n",
      "Epoch: 17/20; Batch:459/468; Training loss:0.0964126\n",
      "Epoch: 17/20; Batch:460/468; Training loss:0.0944834\n",
      "Epoch: 17/20; Batch:461/468; Training loss:0.0954039\n",
      "Epoch: 17/20; Batch:462/468; Training loss:0.0941358\n",
      "Epoch: 17/20; Batch:463/468; Training loss:0.0960124\n",
      "Epoch: 17/20; Batch:464/468; Training loss:0.0990156\n",
      "Epoch: 17/20; Batch:465/468; Training loss:0.0964972\n",
      "Epoch: 17/20; Batch:466/468; Training loss:0.0979141\n",
      "Epoch: 17/20; Batch:467/468; Training loss:0.0946155\n",
      "Epoch: 17/20; Batch:468/468; Training loss:0.0980935\n",
      "Epoch: 18/20; Batch:1/468; Training loss:0.0960091\n",
      "Epoch: 18/20; Batch:2/468; Training loss:0.102132\n",
      "Epoch: 18/20; Batch:3/468; Training loss:0.0970994\n",
      "Epoch: 18/20; Batch:4/468; Training loss:0.0971136\n",
      "Epoch: 18/20; Batch:5/468; Training loss:0.0963443\n",
      "Epoch: 18/20; Batch:6/468; Training loss:0.0975824\n",
      "Epoch: 18/20; Batch:7/468; Training loss:0.100015\n",
      "Epoch: 18/20; Batch:8/468; Training loss:0.0970593\n",
      "Epoch: 18/20; Batch:9/468; Training loss:0.0984543\n",
      "Epoch: 18/20; Batch:10/468; Training loss:0.09688\n",
      "Epoch: 18/20; Batch:11/468; Training loss:0.0953985\n",
      "Epoch: 18/20; Batch:12/468; Training loss:0.09526\n",
      "Epoch: 18/20; Batch:13/468; Training loss:0.099774\n",
      "Epoch: 18/20; Batch:14/468; Training loss:0.0971411\n",
      "Epoch: 18/20; Batch:15/468; Training loss:0.0957059\n",
      "Epoch: 18/20; Batch:16/468; Training loss:0.0967203\n",
      "Epoch: 18/20; Batch:17/468; Training loss:0.0957911\n",
      "Epoch: 18/20; Batch:18/468; Training loss:0.0987769\n",
      "Epoch: 18/20; Batch:19/468; Training loss:0.0902168\n",
      "Epoch: 18/20; Batch:20/468; Training loss:0.0939744\n",
      "Epoch: 18/20; Batch:21/468; Training loss:0.097737\n",
      "Epoch: 18/20; Batch:22/468; Training loss:0.0980694\n",
      "Epoch: 18/20; Batch:23/468; Training loss:0.0969441\n",
      "Epoch: 18/20; Batch:24/468; Training loss:0.0981285\n",
      "Epoch: 18/20; Batch:25/468; Training loss:0.0977212\n",
      "Epoch: 18/20; Batch:26/468; Training loss:0.095685\n",
      "Epoch: 18/20; Batch:27/468; Training loss:0.0947864\n",
      "Epoch: 18/20; Batch:28/468; Training loss:0.0950097\n",
      "Epoch: 18/20; Batch:29/468; Training loss:0.100647\n",
      "Epoch: 18/20; Batch:30/468; Training loss:0.0952756\n",
      "Epoch: 18/20; Batch:31/468; Training loss:0.0970097\n",
      "Epoch: 18/20; Batch:32/468; Training loss:0.0933496\n",
      "Epoch: 18/20; Batch:33/468; Training loss:0.0989857\n",
      "Epoch: 18/20; Batch:34/468; Training loss:0.0922497\n",
      "Epoch: 18/20; Batch:35/468; Training loss:0.0964641\n",
      "Epoch: 18/20; Batch:36/468; Training loss:0.0976634\n",
      "Epoch: 18/20; Batch:37/468; Training loss:0.0923977\n",
      "Epoch: 18/20; Batch:38/468; Training loss:0.0939801\n",
      "Epoch: 18/20; Batch:39/468; Training loss:0.0977238\n",
      "Epoch: 18/20; Batch:40/468; Training loss:0.0978397\n",
      "Epoch: 18/20; Batch:41/468; Training loss:0.0939975\n",
      "Epoch: 18/20; Batch:42/468; Training loss:0.0993826\n",
      "Epoch: 18/20; Batch:43/468; Training loss:0.0949043\n",
      "Epoch: 18/20; Batch:44/468; Training loss:0.0919887\n",
      "Epoch: 18/20; Batch:45/468; Training loss:0.0981154\n",
      "Epoch: 18/20; Batch:46/468; Training loss:0.0978203\n",
      "Epoch: 18/20; Batch:47/468; Training loss:0.0965478\n",
      "Epoch: 18/20; Batch:48/468; Training loss:0.0947217\n",
      "Epoch: 18/20; Batch:49/468; Training loss:0.09806\n",
      "Epoch: 18/20; Batch:50/468; Training loss:0.0973545\n",
      "Epoch: 18/20; Batch:51/468; Training loss:0.0935587\n",
      "Epoch: 18/20; Batch:52/468; Training loss:0.0969211\n",
      "Epoch: 18/20; Batch:53/468; Training loss:0.0954441\n",
      "Epoch: 18/20; Batch:54/468; Training loss:0.0967916\n",
      "Epoch: 18/20; Batch:55/468; Training loss:0.0976692\n",
      "Epoch: 18/20; Batch:56/468; Training loss:0.0976488\n",
      "Epoch: 18/20; Batch:57/468; Training loss:0.097626\n",
      "Epoch: 18/20; Batch:58/468; Training loss:0.100402\n",
      "Epoch: 18/20; Batch:59/468; Training loss:0.0937856\n",
      "Epoch: 18/20; Batch:60/468; Training loss:0.0984692\n",
      "Epoch: 18/20; Batch:61/468; Training loss:0.0933879\n",
      "Epoch: 18/20; Batch:62/468; Training loss:0.0959716\n",
      "Epoch: 18/20; Batch:63/468; Training loss:0.0983342\n",
      "Epoch: 18/20; Batch:64/468; Training loss:0.0979001\n",
      "Epoch: 18/20; Batch:65/468; Training loss:0.0949723\n",
      "Epoch: 18/20; Batch:66/468; Training loss:0.0951341\n",
      "Epoch: 18/20; Batch:67/468; Training loss:0.0977283\n",
      "Epoch: 18/20; Batch:68/468; Training loss:0.0971724\n",
      "Epoch: 18/20; Batch:69/468; Training loss:0.0995718\n",
      "Epoch: 18/20; Batch:70/468; Training loss:0.0990505\n",
      "Epoch: 18/20; Batch:71/468; Training loss:0.0969057\n",
      "Epoch: 18/20; Batch:72/468; Training loss:0.100381\n",
      "Epoch: 18/20; Batch:73/468; Training loss:0.092104\n",
      "Epoch: 18/20; Batch:74/468; Training loss:0.0958014\n",
      "Epoch: 18/20; Batch:75/468; Training loss:0.0952456\n",
      "Epoch: 18/20; Batch:76/468; Training loss:0.0971632\n",
      "Epoch: 18/20; Batch:77/468; Training loss:0.0995134\n",
      "Epoch: 18/20; Batch:78/468; Training loss:0.102094\n",
      "Epoch: 18/20; Batch:79/468; Training loss:0.0942892\n",
      "Epoch: 18/20; Batch:80/468; Training loss:0.0945747\n",
      "Epoch: 18/20; Batch:81/468; Training loss:0.0921476\n",
      "Epoch: 18/20; Batch:82/468; Training loss:0.0937883\n",
      "Epoch: 18/20; Batch:83/468; Training loss:0.0978851\n",
      "Epoch: 18/20; Batch:84/468; Training loss:0.0968109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20; Batch:85/468; Training loss:0.0980003\n",
      "Epoch: 18/20; Batch:86/468; Training loss:0.0957206\n",
      "Epoch: 18/20; Batch:87/468; Training loss:0.0965119\n",
      "Epoch: 18/20; Batch:88/468; Training loss:0.0954464\n",
      "Epoch: 18/20; Batch:89/468; Training loss:0.0947881\n",
      "Epoch: 18/20; Batch:90/468; Training loss:0.0946905\n",
      "Epoch: 18/20; Batch:91/468; Training loss:0.0996065\n",
      "Epoch: 18/20; Batch:92/468; Training loss:0.0930503\n",
      "Epoch: 18/20; Batch:93/468; Training loss:0.0931471\n",
      "Epoch: 18/20; Batch:94/468; Training loss:0.0977443\n",
      "Epoch: 18/20; Batch:95/468; Training loss:0.0982613\n",
      "Epoch: 18/20; Batch:96/468; Training loss:0.096148\n",
      "Epoch: 18/20; Batch:97/468; Training loss:0.0999388\n",
      "Epoch: 18/20; Batch:98/468; Training loss:0.0980974\n",
      "Epoch: 18/20; Batch:99/468; Training loss:0.0938176\n",
      "Epoch: 18/20; Batch:100/468; Training loss:0.0969909\n",
      "Epoch: 18/20; Batch:101/468; Training loss:0.0964339\n",
      "Epoch: 18/20; Batch:102/468; Training loss:0.0952632\n",
      "Epoch: 18/20; Batch:103/468; Training loss:0.0989062\n",
      "Epoch: 18/20; Batch:104/468; Training loss:0.0971442\n",
      "Epoch: 18/20; Batch:105/468; Training loss:0.097935\n",
      "Epoch: 18/20; Batch:106/468; Training loss:0.100683\n",
      "Epoch: 18/20; Batch:107/468; Training loss:0.094801\n",
      "Epoch: 18/20; Batch:108/468; Training loss:0.0936184\n",
      "Epoch: 18/20; Batch:109/468; Training loss:0.0920985\n",
      "Epoch: 18/20; Batch:110/468; Training loss:0.0923961\n",
      "Epoch: 18/20; Batch:111/468; Training loss:0.0944596\n",
      "Epoch: 18/20; Batch:112/468; Training loss:0.0943033\n",
      "Epoch: 18/20; Batch:113/468; Training loss:0.0932695\n",
      "Epoch: 18/20; Batch:114/468; Training loss:0.0965851\n",
      "Epoch: 18/20; Batch:115/468; Training loss:0.100204\n",
      "Epoch: 18/20; Batch:116/468; Training loss:0.0951046\n",
      "Epoch: 18/20; Batch:117/468; Training loss:0.0929547\n",
      "Epoch: 18/20; Batch:118/468; Training loss:0.0987252\n",
      "Epoch: 18/20; Batch:119/468; Training loss:0.0970935\n",
      "Epoch: 18/20; Batch:120/468; Training loss:0.094736\n",
      "Epoch: 18/20; Batch:121/468; Training loss:0.0943264\n",
      "Epoch: 18/20; Batch:122/468; Training loss:0.0941896\n",
      "Epoch: 18/20; Batch:123/468; Training loss:0.0987537\n",
      "Epoch: 18/20; Batch:124/468; Training loss:0.0931985\n",
      "Epoch: 18/20; Batch:125/468; Training loss:0.100829\n",
      "Epoch: 18/20; Batch:126/468; Training loss:0.0925045\n",
      "Epoch: 18/20; Batch:127/468; Training loss:0.0944208\n",
      "Epoch: 18/20; Batch:128/468; Training loss:0.0990489\n",
      "Epoch: 18/20; Batch:129/468; Training loss:0.100196\n",
      "Epoch: 18/20; Batch:130/468; Training loss:0.0953265\n",
      "Epoch: 18/20; Batch:131/468; Training loss:0.0969423\n",
      "Epoch: 18/20; Batch:132/468; Training loss:0.0931115\n",
      "Epoch: 18/20; Batch:133/468; Training loss:0.0959142\n",
      "Epoch: 18/20; Batch:134/468; Training loss:0.0954564\n",
      "Epoch: 18/20; Batch:135/468; Training loss:0.0956071\n",
      "Epoch: 18/20; Batch:136/468; Training loss:0.0945743\n",
      "Epoch: 18/20; Batch:137/468; Training loss:0.095822\n",
      "Epoch: 18/20; Batch:138/468; Training loss:0.0957129\n",
      "Epoch: 18/20; Batch:139/468; Training loss:0.0958493\n",
      "Epoch: 18/20; Batch:140/468; Training loss:0.093896\n",
      "Epoch: 18/20; Batch:141/468; Training loss:0.0941675\n",
      "Epoch: 18/20; Batch:142/468; Training loss:0.0974784\n",
      "Epoch: 18/20; Batch:143/468; Training loss:0.0940775\n",
      "Epoch: 18/20; Batch:144/468; Training loss:0.0957623\n",
      "Epoch: 18/20; Batch:145/468; Training loss:0.0972419\n",
      "Epoch: 18/20; Batch:146/468; Training loss:0.0979467\n",
      "Epoch: 18/20; Batch:147/468; Training loss:0.0946037\n",
      "Epoch: 18/20; Batch:148/468; Training loss:0.100224\n",
      "Epoch: 18/20; Batch:149/468; Training loss:0.0942743\n",
      "Epoch: 18/20; Batch:150/468; Training loss:0.0944204\n",
      "Epoch: 18/20; Batch:151/468; Training loss:0.096444\n",
      "Epoch: 18/20; Batch:152/468; Training loss:0.102294\n",
      "Epoch: 18/20; Batch:153/468; Training loss:0.091589\n",
      "Epoch: 18/20; Batch:154/468; Training loss:0.0986004\n",
      "Epoch: 18/20; Batch:155/468; Training loss:0.0963039\n",
      "Epoch: 18/20; Batch:156/468; Training loss:0.0971837\n",
      "Epoch: 18/20; Batch:157/468; Training loss:0.0980564\n",
      "Epoch: 18/20; Batch:158/468; Training loss:0.0954517\n",
      "Epoch: 18/20; Batch:159/468; Training loss:0.0962684\n",
      "Epoch: 18/20; Batch:160/468; Training loss:0.0988073\n",
      "Epoch: 18/20; Batch:161/468; Training loss:0.094454\n",
      "Epoch: 18/20; Batch:162/468; Training loss:0.0958673\n",
      "Epoch: 18/20; Batch:163/468; Training loss:0.0967224\n",
      "Epoch: 18/20; Batch:164/468; Training loss:0.10033\n",
      "Epoch: 18/20; Batch:165/468; Training loss:0.0954536\n",
      "Epoch: 18/20; Batch:166/468; Training loss:0.0949718\n",
      "Epoch: 18/20; Batch:167/468; Training loss:0.0964393\n",
      "Epoch: 18/20; Batch:168/468; Training loss:0.0933474\n",
      "Epoch: 18/20; Batch:169/468; Training loss:0.0965404\n",
      "Epoch: 18/20; Batch:170/468; Training loss:0.100704\n",
      "Epoch: 18/20; Batch:171/468; Training loss:0.0956622\n",
      "Epoch: 18/20; Batch:172/468; Training loss:0.0995499\n",
      "Epoch: 18/20; Batch:173/468; Training loss:0.0927279\n",
      "Epoch: 18/20; Batch:174/468; Training loss:0.0953706\n",
      "Epoch: 18/20; Batch:175/468; Training loss:0.097896\n",
      "Epoch: 18/20; Batch:176/468; Training loss:0.097694\n",
      "Epoch: 18/20; Batch:177/468; Training loss:0.0969792\n",
      "Epoch: 18/20; Batch:178/468; Training loss:0.0974676\n",
      "Epoch: 18/20; Batch:179/468; Training loss:0.100447\n",
      "Epoch: 18/20; Batch:180/468; Training loss:0.0939701\n",
      "Epoch: 18/20; Batch:181/468; Training loss:0.098352\n",
      "Epoch: 18/20; Batch:182/468; Training loss:0.0967834\n",
      "Epoch: 18/20; Batch:183/468; Training loss:0.09555\n",
      "Epoch: 18/20; Batch:184/468; Training loss:0.0984346\n",
      "Epoch: 18/20; Batch:185/468; Training loss:0.0952774\n",
      "Epoch: 18/20; Batch:186/468; Training loss:0.0960498\n",
      "Epoch: 18/20; Batch:187/468; Training loss:0.0976167\n",
      "Epoch: 18/20; Batch:188/468; Training loss:0.0961059\n",
      "Epoch: 18/20; Batch:189/468; Training loss:0.0999319\n",
      "Epoch: 18/20; Batch:190/468; Training loss:0.0973904\n",
      "Epoch: 18/20; Batch:191/468; Training loss:0.0949485\n",
      "Epoch: 18/20; Batch:192/468; Training loss:0.0981549\n",
      "Epoch: 18/20; Batch:193/468; Training loss:0.0961101\n",
      "Epoch: 18/20; Batch:194/468; Training loss:0.0955093\n",
      "Epoch: 18/20; Batch:195/468; Training loss:0.0964676\n",
      "Epoch: 18/20; Batch:196/468; Training loss:0.0969964\n",
      "Epoch: 18/20; Batch:197/468; Training loss:0.0965698\n",
      "Epoch: 18/20; Batch:198/468; Training loss:0.0966612\n",
      "Epoch: 18/20; Batch:199/468; Training loss:0.091494\n",
      "Epoch: 18/20; Batch:200/468; Training loss:0.0997835\n",
      "Epoch: 18/20; Batch:201/468; Training loss:0.0930544\n",
      "Epoch: 18/20; Batch:202/468; Training loss:0.0948504\n",
      "Epoch: 18/20; Batch:203/468; Training loss:0.0981862\n",
      "Epoch: 18/20; Batch:204/468; Training loss:0.0956167\n",
      "Epoch: 18/20; Batch:205/468; Training loss:0.0932587\n",
      "Epoch: 18/20; Batch:206/468; Training loss:0.0969288\n",
      "Epoch: 18/20; Batch:207/468; Training loss:0.0954556\n",
      "Epoch: 18/20; Batch:208/468; Training loss:0.0944351\n",
      "Epoch: 18/20; Batch:209/468; Training loss:0.0972687\n",
      "Epoch: 18/20; Batch:210/468; Training loss:0.0970384\n",
      "Epoch: 18/20; Batch:211/468; Training loss:0.0962719\n",
      "Epoch: 18/20; Batch:212/468; Training loss:0.0984781\n",
      "Epoch: 18/20; Batch:213/468; Training loss:0.0956329\n",
      "Epoch: 18/20; Batch:214/468; Training loss:0.0952629\n",
      "Epoch: 18/20; Batch:215/468; Training loss:0.0942226\n",
      "Epoch: 18/20; Batch:216/468; Training loss:0.097591\n",
      "Epoch: 18/20; Batch:217/468; Training loss:0.0974429\n",
      "Epoch: 18/20; Batch:218/468; Training loss:0.0935051\n",
      "Epoch: 18/20; Batch:219/468; Training loss:0.0956096\n",
      "Epoch: 18/20; Batch:220/468; Training loss:0.0898387\n",
      "Epoch: 18/20; Batch:221/468; Training loss:0.0953215\n",
      "Epoch: 18/20; Batch:222/468; Training loss:0.0974694\n",
      "Epoch: 18/20; Batch:223/468; Training loss:0.0959098\n",
      "Epoch: 18/20; Batch:224/468; Training loss:0.0980147\n",
      "Epoch: 18/20; Batch:225/468; Training loss:0.0940895\n",
      "Epoch: 18/20; Batch:226/468; Training loss:0.0914911\n",
      "Epoch: 18/20; Batch:227/468; Training loss:0.0961685\n",
      "Epoch: 18/20; Batch:228/468; Training loss:0.0957811\n",
      "Epoch: 18/20; Batch:229/468; Training loss:0.098726\n",
      "Epoch: 18/20; Batch:230/468; Training loss:0.0928998\n",
      "Epoch: 18/20; Batch:231/468; Training loss:0.0959195\n",
      "Epoch: 18/20; Batch:232/468; Training loss:0.095684\n",
      "Epoch: 18/20; Batch:233/468; Training loss:0.0945609\n",
      "Epoch: 18/20; Batch:234/468; Training loss:0.0949213\n",
      "Epoch: 18/20; Batch:235/468; Training loss:0.0957301\n",
      "Epoch: 18/20; Batch:236/468; Training loss:0.096761\n",
      "Epoch: 18/20; Batch:237/468; Training loss:0.0965386\n",
      "Epoch: 18/20; Batch:238/468; Training loss:0.0942485\n",
      "Epoch: 18/20; Batch:239/468; Training loss:0.0964509\n",
      "Epoch: 18/20; Batch:240/468; Training loss:0.0940538\n",
      "Epoch: 18/20; Batch:241/468; Training loss:0.0961686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20; Batch:242/468; Training loss:0.0996917\n",
      "Epoch: 18/20; Batch:243/468; Training loss:0.0961709\n",
      "Epoch: 18/20; Batch:244/468; Training loss:0.099762\n",
      "Epoch: 18/20; Batch:245/468; Training loss:0.0952375\n",
      "Epoch: 18/20; Batch:246/468; Training loss:0.0914484\n",
      "Epoch: 18/20; Batch:247/468; Training loss:0.0967465\n",
      "Epoch: 18/20; Batch:248/468; Training loss:0.0926574\n",
      "Epoch: 18/20; Batch:249/468; Training loss:0.0978232\n",
      "Epoch: 18/20; Batch:250/468; Training loss:0.0989755\n",
      "Epoch: 18/20; Batch:251/468; Training loss:0.0919638\n",
      "Epoch: 18/20; Batch:252/468; Training loss:0.0964133\n",
      "Epoch: 18/20; Batch:253/468; Training loss:0.0945872\n",
      "Epoch: 18/20; Batch:254/468; Training loss:0.0922919\n",
      "Epoch: 18/20; Batch:255/468; Training loss:0.0945532\n",
      "Epoch: 18/20; Batch:256/468; Training loss:0.099215\n",
      "Epoch: 18/20; Batch:257/468; Training loss:0.0951253\n",
      "Epoch: 18/20; Batch:258/468; Training loss:0.0974531\n",
      "Epoch: 18/20; Batch:259/468; Training loss:0.0942792\n",
      "Epoch: 18/20; Batch:260/468; Training loss:0.0963144\n",
      "Epoch: 18/20; Batch:261/468; Training loss:0.0946425\n",
      "Epoch: 18/20; Batch:262/468; Training loss:0.0934815\n",
      "Epoch: 18/20; Batch:263/468; Training loss:0.0935078\n",
      "Epoch: 18/20; Batch:264/468; Training loss:0.097186\n",
      "Epoch: 18/20; Batch:265/468; Training loss:0.0959182\n",
      "Epoch: 18/20; Batch:266/468; Training loss:0.0983701\n",
      "Epoch: 18/20; Batch:267/468; Training loss:0.0989593\n",
      "Epoch: 18/20; Batch:268/468; Training loss:0.102473\n",
      "Epoch: 18/20; Batch:269/468; Training loss:0.0927732\n",
      "Epoch: 18/20; Batch:270/468; Training loss:0.0960121\n",
      "Epoch: 18/20; Batch:271/468; Training loss:0.0950263\n",
      "Epoch: 18/20; Batch:272/468; Training loss:0.0967704\n",
      "Epoch: 18/20; Batch:273/468; Training loss:0.097301\n",
      "Epoch: 18/20; Batch:274/468; Training loss:0.0981227\n",
      "Epoch: 18/20; Batch:275/468; Training loss:0.0964673\n",
      "Epoch: 18/20; Batch:276/468; Training loss:0.0959131\n",
      "Epoch: 18/20; Batch:277/468; Training loss:0.0947848\n",
      "Epoch: 18/20; Batch:278/468; Training loss:0.0974206\n",
      "Epoch: 18/20; Batch:279/468; Training loss:0.0953165\n",
      "Epoch: 18/20; Batch:280/468; Training loss:0.0972836\n",
      "Epoch: 18/20; Batch:281/468; Training loss:0.0971552\n",
      "Epoch: 18/20; Batch:282/468; Training loss:0.0960745\n",
      "Epoch: 18/20; Batch:283/468; Training loss:0.0957222\n",
      "Epoch: 18/20; Batch:284/468; Training loss:0.0973318\n",
      "Epoch: 18/20; Batch:285/468; Training loss:0.100282\n",
      "Epoch: 18/20; Batch:286/468; Training loss:0.0985294\n",
      "Epoch: 18/20; Batch:287/468; Training loss:0.0972719\n",
      "Epoch: 18/20; Batch:288/468; Training loss:0.0922845\n",
      "Epoch: 18/20; Batch:289/468; Training loss:0.096398\n",
      "Epoch: 18/20; Batch:290/468; Training loss:0.0966715\n",
      "Epoch: 18/20; Batch:291/468; Training loss:0.0965598\n",
      "Epoch: 18/20; Batch:292/468; Training loss:0.0934068\n",
      "Epoch: 18/20; Batch:293/468; Training loss:0.089985\n",
      "Epoch: 18/20; Batch:294/468; Training loss:0.0975996\n",
      "Epoch: 18/20; Batch:295/468; Training loss:0.0967998\n",
      "Epoch: 18/20; Batch:296/468; Training loss:0.095796\n",
      "Epoch: 18/20; Batch:297/468; Training loss:0.0933276\n",
      "Epoch: 18/20; Batch:298/468; Training loss:0.0935755\n",
      "Epoch: 18/20; Batch:299/468; Training loss:0.100113\n",
      "Epoch: 18/20; Batch:300/468; Training loss:0.09211\n",
      "Epoch: 18/20; Batch:301/468; Training loss:0.099129\n",
      "Epoch: 18/20; Batch:302/468; Training loss:0.0971964\n",
      "Epoch: 18/20; Batch:303/468; Training loss:0.0983245\n",
      "Epoch: 18/20; Batch:304/468; Training loss:0.101536\n",
      "Epoch: 18/20; Batch:305/468; Training loss:0.0952286\n",
      "Epoch: 18/20; Batch:306/468; Training loss:0.0944009\n",
      "Epoch: 18/20; Batch:307/468; Training loss:0.0977702\n",
      "Epoch: 18/20; Batch:308/468; Training loss:0.0979646\n",
      "Epoch: 18/20; Batch:309/468; Training loss:0.0962366\n",
      "Epoch: 18/20; Batch:310/468; Training loss:0.0971244\n",
      "Epoch: 18/20; Batch:311/468; Training loss:0.0968263\n",
      "Epoch: 18/20; Batch:312/468; Training loss:0.0954574\n",
      "Epoch: 18/20; Batch:313/468; Training loss:0.0979861\n",
      "Epoch: 18/20; Batch:314/468; Training loss:0.0969717\n",
      "Epoch: 18/20; Batch:315/468; Training loss:0.0975391\n",
      "Epoch: 18/20; Batch:316/468; Training loss:0.0989935\n",
      "Epoch: 18/20; Batch:317/468; Training loss:0.0958369\n",
      "Epoch: 18/20; Batch:318/468; Training loss:0.0935543\n",
      "Epoch: 18/20; Batch:319/468; Training loss:0.0959379\n",
      "Epoch: 18/20; Batch:320/468; Training loss:0.0951331\n",
      "Epoch: 18/20; Batch:321/468; Training loss:0.0941357\n",
      "Epoch: 18/20; Batch:322/468; Training loss:0.0992174\n",
      "Epoch: 18/20; Batch:323/468; Training loss:0.0996185\n",
      "Epoch: 18/20; Batch:324/468; Training loss:0.100401\n",
      "Epoch: 18/20; Batch:325/468; Training loss:0.093466\n",
      "Epoch: 18/20; Batch:326/468; Training loss:0.0969117\n",
      "Epoch: 18/20; Batch:327/468; Training loss:0.0988413\n",
      "Epoch: 18/20; Batch:328/468; Training loss:0.0962357\n",
      "Epoch: 18/20; Batch:329/468; Training loss:0.0926864\n",
      "Epoch: 18/20; Batch:330/468; Training loss:0.0960055\n",
      "Epoch: 18/20; Batch:331/468; Training loss:0.0917773\n",
      "Epoch: 18/20; Batch:332/468; Training loss:0.101856\n",
      "Epoch: 18/20; Batch:333/468; Training loss:0.0942907\n",
      "Epoch: 18/20; Batch:334/468; Training loss:0.0919635\n",
      "Epoch: 18/20; Batch:335/468; Training loss:0.0941118\n",
      "Epoch: 18/20; Batch:336/468; Training loss:0.0946694\n",
      "Epoch: 18/20; Batch:337/468; Training loss:0.0936005\n",
      "Epoch: 18/20; Batch:338/468; Training loss:0.0932527\n",
      "Epoch: 18/20; Batch:339/468; Training loss:0.0976509\n",
      "Epoch: 18/20; Batch:340/468; Training loss:0.0994965\n",
      "Epoch: 18/20; Batch:341/468; Training loss:0.0953707\n",
      "Epoch: 18/20; Batch:342/468; Training loss:0.0947403\n",
      "Epoch: 18/20; Batch:343/468; Training loss:0.0967933\n",
      "Epoch: 18/20; Batch:344/468; Training loss:0.0986622\n",
      "Epoch: 18/20; Batch:345/468; Training loss:0.09306\n",
      "Epoch: 18/20; Batch:346/468; Training loss:0.0938464\n",
      "Epoch: 18/20; Batch:347/468; Training loss:0.0966842\n",
      "Epoch: 18/20; Batch:348/468; Training loss:0.0980061\n",
      "Epoch: 18/20; Batch:349/468; Training loss:0.0986999\n",
      "Epoch: 18/20; Batch:350/468; Training loss:0.0983243\n",
      "Epoch: 18/20; Batch:351/468; Training loss:0.0984122\n",
      "Epoch: 18/20; Batch:352/468; Training loss:0.0978662\n",
      "Epoch: 18/20; Batch:353/468; Training loss:0.0950186\n",
      "Epoch: 18/20; Batch:354/468; Training loss:0.0977215\n",
      "Epoch: 18/20; Batch:355/468; Training loss:0.0955592\n",
      "Epoch: 18/20; Batch:356/468; Training loss:0.0960684\n",
      "Epoch: 18/20; Batch:357/468; Training loss:0.0979408\n",
      "Epoch: 18/20; Batch:358/468; Training loss:0.0984404\n",
      "Epoch: 18/20; Batch:359/468; Training loss:0.0989566\n",
      "Epoch: 18/20; Batch:360/468; Training loss:0.0954865\n",
      "Epoch: 18/20; Batch:361/468; Training loss:0.0968037\n",
      "Epoch: 18/20; Batch:362/468; Training loss:0.0934226\n",
      "Epoch: 18/20; Batch:363/468; Training loss:0.100141\n",
      "Epoch: 18/20; Batch:364/468; Training loss:0.0960151\n",
      "Epoch: 18/20; Batch:365/468; Training loss:0.0973593\n",
      "Epoch: 18/20; Batch:366/468; Training loss:0.0954065\n",
      "Epoch: 18/20; Batch:367/468; Training loss:0.0953534\n",
      "Epoch: 18/20; Batch:368/468; Training loss:0.0937717\n",
      "Epoch: 18/20; Batch:369/468; Training loss:0.0939389\n",
      "Epoch: 18/20; Batch:370/468; Training loss:0.0912988\n",
      "Epoch: 18/20; Batch:371/468; Training loss:0.100309\n",
      "Epoch: 18/20; Batch:372/468; Training loss:0.0991407\n",
      "Epoch: 18/20; Batch:373/468; Training loss:0.0947651\n",
      "Epoch: 18/20; Batch:374/468; Training loss:0.094523\n",
      "Epoch: 18/20; Batch:375/468; Training loss:0.0959772\n",
      "Epoch: 18/20; Batch:376/468; Training loss:0.096319\n",
      "Epoch: 18/20; Batch:377/468; Training loss:0.0957321\n",
      "Epoch: 18/20; Batch:378/468; Training loss:0.0982004\n",
      "Epoch: 18/20; Batch:379/468; Training loss:0.0955842\n",
      "Epoch: 18/20; Batch:380/468; Training loss:0.0916008\n",
      "Epoch: 18/20; Batch:381/468; Training loss:0.096282\n",
      "Epoch: 18/20; Batch:382/468; Training loss:0.0957393\n",
      "Epoch: 18/20; Batch:383/468; Training loss:0.0946768\n",
      "Epoch: 18/20; Batch:384/468; Training loss:0.0950902\n",
      "Epoch: 18/20; Batch:385/468; Training loss:0.0928512\n",
      "Epoch: 18/20; Batch:386/468; Training loss:0.0936999\n",
      "Epoch: 18/20; Batch:387/468; Training loss:0.0913157\n",
      "Epoch: 18/20; Batch:388/468; Training loss:0.0959634\n",
      "Epoch: 18/20; Batch:389/468; Training loss:0.0953299\n",
      "Epoch: 18/20; Batch:390/468; Training loss:0.0984928\n",
      "Epoch: 18/20; Batch:391/468; Training loss:0.0961778\n",
      "Epoch: 18/20; Batch:392/468; Training loss:0.0979419\n",
      "Epoch: 18/20; Batch:393/468; Training loss:0.0980463\n",
      "Epoch: 18/20; Batch:394/468; Training loss:0.0931666\n",
      "Epoch: 18/20; Batch:395/468; Training loss:0.0973498\n",
      "Epoch: 18/20; Batch:396/468; Training loss:0.0964485\n",
      "Epoch: 18/20; Batch:397/468; Training loss:0.0972843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20; Batch:398/468; Training loss:0.100816\n",
      "Epoch: 18/20; Batch:399/468; Training loss:0.10024\n",
      "Epoch: 18/20; Batch:400/468; Training loss:0.0953127\n",
      "Epoch: 18/20; Batch:401/468; Training loss:0.0971207\n",
      "Epoch: 18/20; Batch:402/468; Training loss:0.0960507\n",
      "Epoch: 18/20; Batch:403/468; Training loss:0.0984026\n",
      "Epoch: 18/20; Batch:404/468; Training loss:0.0962108\n",
      "Epoch: 18/20; Batch:405/468; Training loss:0.0974141\n",
      "Epoch: 18/20; Batch:406/468; Training loss:0.0924387\n",
      "Epoch: 18/20; Batch:407/468; Training loss:0.0920492\n",
      "Epoch: 18/20; Batch:408/468; Training loss:0.0974653\n",
      "Epoch: 18/20; Batch:409/468; Training loss:0.095888\n",
      "Epoch: 18/20; Batch:410/468; Training loss:0.0977235\n",
      "Epoch: 18/20; Batch:411/468; Training loss:0.0973094\n",
      "Epoch: 18/20; Batch:412/468; Training loss:0.0954176\n",
      "Epoch: 18/20; Batch:413/468; Training loss:0.0958902\n",
      "Epoch: 18/20; Batch:414/468; Training loss:0.0933571\n",
      "Epoch: 18/20; Batch:415/468; Training loss:0.102526\n",
      "Epoch: 18/20; Batch:416/468; Training loss:0.0961317\n",
      "Epoch: 18/20; Batch:417/468; Training loss:0.0949373\n",
      "Epoch: 18/20; Batch:418/468; Training loss:0.0969553\n",
      "Epoch: 18/20; Batch:419/468; Training loss:0.0978651\n",
      "Epoch: 18/20; Batch:420/468; Training loss:0.0964148\n",
      "Epoch: 18/20; Batch:421/468; Training loss:0.0975547\n",
      "Epoch: 18/20; Batch:422/468; Training loss:0.097659\n",
      "Epoch: 18/20; Batch:423/468; Training loss:0.0935646\n",
      "Epoch: 18/20; Batch:424/468; Training loss:0.0985481\n",
      "Epoch: 18/20; Batch:425/468; Training loss:0.0939163\n",
      "Epoch: 18/20; Batch:426/468; Training loss:0.0937032\n",
      "Epoch: 18/20; Batch:427/468; Training loss:0.0967083\n",
      "Epoch: 18/20; Batch:428/468; Training loss:0.0961219\n",
      "Epoch: 18/20; Batch:429/468; Training loss:0.0959815\n",
      "Epoch: 18/20; Batch:430/468; Training loss:0.0959835\n",
      "Epoch: 18/20; Batch:431/468; Training loss:0.0965702\n",
      "Epoch: 18/20; Batch:432/468; Training loss:0.0956779\n",
      "Epoch: 18/20; Batch:433/468; Training loss:0.0933165\n",
      "Epoch: 18/20; Batch:434/468; Training loss:0.0952299\n",
      "Epoch: 18/20; Batch:435/468; Training loss:0.0952331\n",
      "Epoch: 18/20; Batch:436/468; Training loss:0.0924127\n",
      "Epoch: 18/20; Batch:437/468; Training loss:0.0950561\n",
      "Epoch: 18/20; Batch:438/468; Training loss:0.100462\n",
      "Epoch: 18/20; Batch:439/468; Training loss:0.0934048\n",
      "Epoch: 18/20; Batch:440/468; Training loss:0.0973454\n",
      "Epoch: 18/20; Batch:441/468; Training loss:0.0916825\n",
      "Epoch: 18/20; Batch:442/468; Training loss:0.0986423\n",
      "Epoch: 18/20; Batch:443/468; Training loss:0.0959791\n",
      "Epoch: 18/20; Batch:444/468; Training loss:0.0961601\n",
      "Epoch: 18/20; Batch:445/468; Training loss:0.0936885\n",
      "Epoch: 18/20; Batch:446/468; Training loss:0.0964286\n",
      "Epoch: 18/20; Batch:447/468; Training loss:0.0961323\n",
      "Epoch: 18/20; Batch:448/468; Training loss:0.0965148\n",
      "Epoch: 18/20; Batch:449/468; Training loss:0.0977839\n",
      "Epoch: 18/20; Batch:450/468; Training loss:0.0939384\n",
      "Epoch: 18/20; Batch:451/468; Training loss:0.0961587\n",
      "Epoch: 18/20; Batch:452/468; Training loss:0.0974713\n",
      "Epoch: 18/20; Batch:453/468; Training loss:0.096214\n",
      "Epoch: 18/20; Batch:454/468; Training loss:0.091486\n",
      "Epoch: 18/20; Batch:455/468; Training loss:0.0933394\n",
      "Epoch: 18/20; Batch:456/468; Training loss:0.0964994\n",
      "Epoch: 18/20; Batch:457/468; Training loss:0.0971164\n",
      "Epoch: 18/20; Batch:458/468; Training loss:0.0956632\n",
      "Epoch: 18/20; Batch:459/468; Training loss:0.100142\n",
      "Epoch: 18/20; Batch:460/468; Training loss:0.0988155\n",
      "Epoch: 18/20; Batch:461/468; Training loss:0.0943708\n",
      "Epoch: 18/20; Batch:462/468; Training loss:0.095171\n",
      "Epoch: 18/20; Batch:463/468; Training loss:0.102466\n",
      "Epoch: 18/20; Batch:464/468; Training loss:0.0993074\n",
      "Epoch: 18/20; Batch:465/468; Training loss:0.0947678\n",
      "Epoch: 18/20; Batch:466/468; Training loss:0.0977041\n",
      "Epoch: 18/20; Batch:467/468; Training loss:0.0990725\n",
      "Epoch: 18/20; Batch:468/468; Training loss:0.0970897\n",
      "Epoch: 19/20; Batch:1/468; Training loss:0.0996683\n",
      "Epoch: 19/20; Batch:2/468; Training loss:0.0954559\n",
      "Epoch: 19/20; Batch:3/468; Training loss:0.0988452\n",
      "Epoch: 19/20; Batch:4/468; Training loss:0.0965429\n",
      "Epoch: 19/20; Batch:5/468; Training loss:0.0941831\n",
      "Epoch: 19/20; Batch:6/468; Training loss:0.096101\n",
      "Epoch: 19/20; Batch:7/468; Training loss:0.0954801\n",
      "Epoch: 19/20; Batch:8/468; Training loss:0.0941629\n",
      "Epoch: 19/20; Batch:9/468; Training loss:0.094352\n",
      "Epoch: 19/20; Batch:10/468; Training loss:0.0949315\n",
      "Epoch: 19/20; Batch:11/468; Training loss:0.0958489\n",
      "Epoch: 19/20; Batch:12/468; Training loss:0.0977798\n",
      "Epoch: 19/20; Batch:13/468; Training loss:0.0929636\n",
      "Epoch: 19/20; Batch:14/468; Training loss:0.0973828\n",
      "Epoch: 19/20; Batch:15/468; Training loss:0.10138\n",
      "Epoch: 19/20; Batch:16/468; Training loss:0.0977143\n",
      "Epoch: 19/20; Batch:17/468; Training loss:0.0957149\n",
      "Epoch: 19/20; Batch:18/468; Training loss:0.0962225\n",
      "Epoch: 19/20; Batch:19/468; Training loss:0.0978167\n",
      "Epoch: 19/20; Batch:20/468; Training loss:0.0989235\n",
      "Epoch: 19/20; Batch:21/468; Training loss:0.0968494\n",
      "Epoch: 19/20; Batch:22/468; Training loss:0.0883515\n",
      "Epoch: 19/20; Batch:23/468; Training loss:0.0998307\n",
      "Epoch: 19/20; Batch:24/468; Training loss:0.0955508\n",
      "Epoch: 19/20; Batch:25/468; Training loss:0.10035\n",
      "Epoch: 19/20; Batch:26/468; Training loss:0.0964728\n",
      "Epoch: 19/20; Batch:27/468; Training loss:0.0971497\n",
      "Epoch: 19/20; Batch:28/468; Training loss:0.0946738\n",
      "Epoch: 19/20; Batch:29/468; Training loss:0.0954455\n",
      "Epoch: 19/20; Batch:30/468; Training loss:0.0946199\n",
      "Epoch: 19/20; Batch:31/468; Training loss:0.0974689\n",
      "Epoch: 19/20; Batch:32/468; Training loss:0.0964668\n",
      "Epoch: 19/20; Batch:33/468; Training loss:0.0946725\n",
      "Epoch: 19/20; Batch:34/468; Training loss:0.0948503\n",
      "Epoch: 19/20; Batch:35/468; Training loss:0.0932021\n",
      "Epoch: 19/20; Batch:36/468; Training loss:0.0953852\n",
      "Epoch: 19/20; Batch:37/468; Training loss:0.0962618\n",
      "Epoch: 19/20; Batch:38/468; Training loss:0.0965411\n",
      "Epoch: 19/20; Batch:39/468; Training loss:0.0974963\n",
      "Epoch: 19/20; Batch:40/468; Training loss:0.0966839\n",
      "Epoch: 19/20; Batch:41/468; Training loss:0.0990641\n",
      "Epoch: 19/20; Batch:42/468; Training loss:0.0971021\n",
      "Epoch: 19/20; Batch:43/468; Training loss:0.0923957\n",
      "Epoch: 19/20; Batch:44/468; Training loss:0.0984021\n",
      "Epoch: 19/20; Batch:45/468; Training loss:0.0966128\n",
      "Epoch: 19/20; Batch:46/468; Training loss:0.0926054\n",
      "Epoch: 19/20; Batch:47/468; Training loss:0.0955145\n",
      "Epoch: 19/20; Batch:48/468; Training loss:0.096124\n",
      "Epoch: 19/20; Batch:49/468; Training loss:0.0970742\n",
      "Epoch: 19/20; Batch:50/468; Training loss:0.0944984\n",
      "Epoch: 19/20; Batch:51/468; Training loss:0.0962443\n",
      "Epoch: 19/20; Batch:52/468; Training loss:0.0950217\n",
      "Epoch: 19/20; Batch:53/468; Training loss:0.0956398\n",
      "Epoch: 19/20; Batch:54/468; Training loss:0.0975057\n",
      "Epoch: 19/20; Batch:55/468; Training loss:0.0976686\n",
      "Epoch: 19/20; Batch:56/468; Training loss:0.0962055\n",
      "Epoch: 19/20; Batch:57/468; Training loss:0.096633\n",
      "Epoch: 19/20; Batch:58/468; Training loss:0.0960955\n",
      "Epoch: 19/20; Batch:59/468; Training loss:0.0948757\n",
      "Epoch: 19/20; Batch:60/468; Training loss:0.0947373\n",
      "Epoch: 19/20; Batch:61/468; Training loss:0.0963873\n",
      "Epoch: 19/20; Batch:62/468; Training loss:0.0983358\n",
      "Epoch: 19/20; Batch:63/468; Training loss:0.0939941\n",
      "Epoch: 19/20; Batch:64/468; Training loss:0.0952131\n",
      "Epoch: 19/20; Batch:65/468; Training loss:0.0931426\n",
      "Epoch: 19/20; Batch:66/468; Training loss:0.0962821\n",
      "Epoch: 19/20; Batch:67/468; Training loss:0.0924307\n",
      "Epoch: 19/20; Batch:68/468; Training loss:0.0945588\n",
      "Epoch: 19/20; Batch:69/468; Training loss:0.0928625\n",
      "Epoch: 19/20; Batch:70/468; Training loss:0.0959649\n",
      "Epoch: 19/20; Batch:71/468; Training loss:0.0979136\n",
      "Epoch: 19/20; Batch:72/468; Training loss:0.0943292\n",
      "Epoch: 19/20; Batch:73/468; Training loss:0.0928408\n",
      "Epoch: 19/20; Batch:74/468; Training loss:0.0932133\n",
      "Epoch: 19/20; Batch:75/468; Training loss:0.0979725\n",
      "Epoch: 19/20; Batch:76/468; Training loss:0.0956092\n",
      "Epoch: 19/20; Batch:77/468; Training loss:0.0956623\n",
      "Epoch: 19/20; Batch:78/468; Training loss:0.0942889\n",
      "Epoch: 19/20; Batch:79/468; Training loss:0.097404\n",
      "Epoch: 19/20; Batch:80/468; Training loss:0.0942896\n",
      "Epoch: 19/20; Batch:81/468; Training loss:0.096075\n",
      "Epoch: 19/20; Batch:82/468; Training loss:0.0956211\n",
      "Epoch: 19/20; Batch:83/468; Training loss:0.095004\n",
      "Epoch: 19/20; Batch:84/468; Training loss:0.0942993\n",
      "Epoch: 19/20; Batch:85/468; Training loss:0.0965061\n",
      "Epoch: 19/20; Batch:86/468; Training loss:0.093611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20; Batch:87/468; Training loss:0.0953497\n",
      "Epoch: 19/20; Batch:88/468; Training loss:0.0979092\n",
      "Epoch: 19/20; Batch:89/468; Training loss:0.0944496\n",
      "Epoch: 19/20; Batch:90/468; Training loss:0.099497\n",
      "Epoch: 19/20; Batch:91/468; Training loss:0.0984857\n",
      "Epoch: 19/20; Batch:92/468; Training loss:0.0962294\n",
      "Epoch: 19/20; Batch:93/468; Training loss:0.0954819\n",
      "Epoch: 19/20; Batch:94/468; Training loss:0.0976521\n",
      "Epoch: 19/20; Batch:95/468; Training loss:0.0981518\n",
      "Epoch: 19/20; Batch:96/468; Training loss:0.0942127\n",
      "Epoch: 19/20; Batch:97/468; Training loss:0.0966908\n",
      "Epoch: 19/20; Batch:98/468; Training loss:0.0984218\n",
      "Epoch: 19/20; Batch:99/468; Training loss:0.0947911\n",
      "Epoch: 19/20; Batch:100/468; Training loss:0.0991985\n",
      "Epoch: 19/20; Batch:101/468; Training loss:0.0957656\n",
      "Epoch: 19/20; Batch:102/468; Training loss:0.0944454\n",
      "Epoch: 19/20; Batch:103/468; Training loss:0.0977843\n",
      "Epoch: 19/20; Batch:104/468; Training loss:0.0959465\n",
      "Epoch: 19/20; Batch:105/468; Training loss:0.0937303\n",
      "Epoch: 19/20; Batch:106/468; Training loss:0.0930806\n",
      "Epoch: 19/20; Batch:107/468; Training loss:0.098183\n",
      "Epoch: 19/20; Batch:108/468; Training loss:0.095884\n",
      "Epoch: 19/20; Batch:109/468; Training loss:0.100259\n",
      "Epoch: 19/20; Batch:110/468; Training loss:0.0940049\n",
      "Epoch: 19/20; Batch:111/468; Training loss:0.0995093\n",
      "Epoch: 19/20; Batch:112/468; Training loss:0.09413\n",
      "Epoch: 19/20; Batch:113/468; Training loss:0.0960335\n",
      "Epoch: 19/20; Batch:114/468; Training loss:0.0959097\n",
      "Epoch: 19/20; Batch:115/468; Training loss:0.097071\n",
      "Epoch: 19/20; Batch:116/468; Training loss:0.0966637\n",
      "Epoch: 19/20; Batch:117/468; Training loss:0.0950896\n",
      "Epoch: 19/20; Batch:118/468; Training loss:0.0994829\n",
      "Epoch: 19/20; Batch:119/468; Training loss:0.0949986\n",
      "Epoch: 19/20; Batch:120/468; Training loss:0.0936818\n",
      "Epoch: 19/20; Batch:121/468; Training loss:0.100917\n",
      "Epoch: 19/20; Batch:122/468; Training loss:0.0955667\n",
      "Epoch: 19/20; Batch:123/468; Training loss:0.0942658\n",
      "Epoch: 19/20; Batch:124/468; Training loss:0.0954913\n",
      "Epoch: 19/20; Batch:125/468; Training loss:0.0966906\n",
      "Epoch: 19/20; Batch:126/468; Training loss:0.0967811\n",
      "Epoch: 19/20; Batch:127/468; Training loss:0.0980994\n",
      "Epoch: 19/20; Batch:128/468; Training loss:0.0940345\n",
      "Epoch: 19/20; Batch:129/468; Training loss:0.102444\n",
      "Epoch: 19/20; Batch:130/468; Training loss:0.0984728\n",
      "Epoch: 19/20; Batch:131/468; Training loss:0.0938668\n",
      "Epoch: 19/20; Batch:132/468; Training loss:0.0986167\n",
      "Epoch: 19/20; Batch:133/468; Training loss:0.0958597\n",
      "Epoch: 19/20; Batch:134/468; Training loss:0.0930499\n",
      "Epoch: 19/20; Batch:135/468; Training loss:0.094399\n",
      "Epoch: 19/20; Batch:136/468; Training loss:0.100427\n",
      "Epoch: 19/20; Batch:137/468; Training loss:0.0935176\n",
      "Epoch: 19/20; Batch:138/468; Training loss:0.0923037\n",
      "Epoch: 19/20; Batch:139/468; Training loss:0.0951276\n",
      "Epoch: 19/20; Batch:140/468; Training loss:0.0931611\n",
      "Epoch: 19/20; Batch:141/468; Training loss:0.0954701\n",
      "Epoch: 19/20; Batch:142/468; Training loss:0.0949956\n",
      "Epoch: 19/20; Batch:143/468; Training loss:0.0955242\n",
      "Epoch: 19/20; Batch:144/468; Training loss:0.096913\n",
      "Epoch: 19/20; Batch:145/468; Training loss:0.0959564\n",
      "Epoch: 19/20; Batch:146/468; Training loss:0.0996811\n",
      "Epoch: 19/20; Batch:147/468; Training loss:0.0950977\n",
      "Epoch: 19/20; Batch:148/468; Training loss:0.0970986\n",
      "Epoch: 19/20; Batch:149/468; Training loss:0.0974976\n",
      "Epoch: 19/20; Batch:150/468; Training loss:0.0963513\n",
      "Epoch: 19/20; Batch:151/468; Training loss:0.0962577\n",
      "Epoch: 19/20; Batch:152/468; Training loss:0.0973802\n",
      "Epoch: 19/20; Batch:153/468; Training loss:0.0949696\n",
      "Epoch: 19/20; Batch:154/468; Training loss:0.0962294\n",
      "Epoch: 19/20; Batch:155/468; Training loss:0.0970832\n",
      "Epoch: 19/20; Batch:156/468; Training loss:0.0968308\n",
      "Epoch: 19/20; Batch:157/468; Training loss:0.0953295\n",
      "Epoch: 19/20; Batch:158/468; Training loss:0.0961425\n",
      "Epoch: 19/20; Batch:159/468; Training loss:0.0967486\n",
      "Epoch: 19/20; Batch:160/468; Training loss:0.0967212\n",
      "Epoch: 19/20; Batch:161/468; Training loss:0.0964362\n",
      "Epoch: 19/20; Batch:162/468; Training loss:0.096043\n",
      "Epoch: 19/20; Batch:163/468; Training loss:0.09693\n",
      "Epoch: 19/20; Batch:164/468; Training loss:0.0965546\n",
      "Epoch: 19/20; Batch:165/468; Training loss:0.0983429\n",
      "Epoch: 19/20; Batch:166/468; Training loss:0.0950196\n",
      "Epoch: 19/20; Batch:167/468; Training loss:0.0965031\n",
      "Epoch: 19/20; Batch:168/468; Training loss:0.0930879\n",
      "Epoch: 19/20; Batch:169/468; Training loss:0.0951317\n",
      "Epoch: 19/20; Batch:170/468; Training loss:0.0958872\n",
      "Epoch: 19/20; Batch:171/468; Training loss:0.0969823\n",
      "Epoch: 19/20; Batch:172/468; Training loss:0.0964347\n",
      "Epoch: 19/20; Batch:173/468; Training loss:0.0964683\n",
      "Epoch: 19/20; Batch:174/468; Training loss:0.0980581\n",
      "Epoch: 19/20; Batch:175/468; Training loss:0.0976024\n",
      "Epoch: 19/20; Batch:176/468; Training loss:0.0967284\n",
      "Epoch: 19/20; Batch:177/468; Training loss:0.0975179\n",
      "Epoch: 19/20; Batch:178/468; Training loss:0.0934221\n",
      "Epoch: 19/20; Batch:179/468; Training loss:0.0975581\n",
      "Epoch: 19/20; Batch:180/468; Training loss:0.0985321\n",
      "Epoch: 19/20; Batch:181/468; Training loss:0.0961239\n",
      "Epoch: 19/20; Batch:182/468; Training loss:0.0950071\n",
      "Epoch: 19/20; Batch:183/468; Training loss:0.0969924\n",
      "Epoch: 19/20; Batch:184/468; Training loss:0.0929948\n",
      "Epoch: 19/20; Batch:185/468; Training loss:0.0946878\n",
      "Epoch: 19/20; Batch:186/468; Training loss:0.0970752\n",
      "Epoch: 19/20; Batch:187/468; Training loss:0.0946626\n",
      "Epoch: 19/20; Batch:188/468; Training loss:0.09689\n",
      "Epoch: 19/20; Batch:189/468; Training loss:0.0972577\n",
      "Epoch: 19/20; Batch:190/468; Training loss:0.0946364\n",
      "Epoch: 19/20; Batch:191/468; Training loss:0.0975316\n",
      "Epoch: 19/20; Batch:192/468; Training loss:0.0928617\n",
      "Epoch: 19/20; Batch:193/468; Training loss:0.0968276\n",
      "Epoch: 19/20; Batch:194/468; Training loss:0.0991151\n",
      "Epoch: 19/20; Batch:195/468; Training loss:0.097235\n",
      "Epoch: 19/20; Batch:196/468; Training loss:0.0952066\n",
      "Epoch: 19/20; Batch:197/468; Training loss:0.0962323\n",
      "Epoch: 19/20; Batch:198/468; Training loss:0.0982196\n",
      "Epoch: 19/20; Batch:199/468; Training loss:0.0989763\n",
      "Epoch: 19/20; Batch:200/468; Training loss:0.0964848\n",
      "Epoch: 19/20; Batch:201/468; Training loss:0.0951386\n",
      "Epoch: 19/20; Batch:202/468; Training loss:0.0945772\n",
      "Epoch: 19/20; Batch:203/468; Training loss:0.094408\n",
      "Epoch: 19/20; Batch:204/468; Training loss:0.0941828\n",
      "Epoch: 19/20; Batch:205/468; Training loss:0.0967412\n",
      "Epoch: 19/20; Batch:206/468; Training loss:0.0934857\n",
      "Epoch: 19/20; Batch:207/468; Training loss:0.0949854\n",
      "Epoch: 19/20; Batch:208/468; Training loss:0.0974395\n",
      "Epoch: 19/20; Batch:209/468; Training loss:0.0968553\n",
      "Epoch: 19/20; Batch:210/468; Training loss:0.101541\n",
      "Epoch: 19/20; Batch:211/468; Training loss:0.0949124\n",
      "Epoch: 19/20; Batch:212/468; Training loss:0.0940053\n",
      "Epoch: 19/20; Batch:213/468; Training loss:0.0926061\n",
      "Epoch: 19/20; Batch:214/468; Training loss:0.0948638\n",
      "Epoch: 19/20; Batch:215/468; Training loss:0.0982451\n",
      "Epoch: 19/20; Batch:216/468; Training loss:0.0950028\n",
      "Epoch: 19/20; Batch:217/468; Training loss:0.0920146\n",
      "Epoch: 19/20; Batch:218/468; Training loss:0.0963509\n",
      "Epoch: 19/20; Batch:219/468; Training loss:0.0928942\n",
      "Epoch: 19/20; Batch:220/468; Training loss:0.0939393\n",
      "Epoch: 19/20; Batch:221/468; Training loss:0.0953485\n",
      "Epoch: 19/20; Batch:222/468; Training loss:0.099682\n",
      "Epoch: 19/20; Batch:223/468; Training loss:0.0969787\n",
      "Epoch: 19/20; Batch:224/468; Training loss:0.0957364\n",
      "Epoch: 19/20; Batch:225/468; Training loss:0.0989189\n",
      "Epoch: 19/20; Batch:226/468; Training loss:0.096526\n",
      "Epoch: 19/20; Batch:227/468; Training loss:0.097696\n",
      "Epoch: 19/20; Batch:228/468; Training loss:0.0915898\n",
      "Epoch: 19/20; Batch:229/468; Training loss:0.095961\n",
      "Epoch: 19/20; Batch:230/468; Training loss:0.0948777\n",
      "Epoch: 19/20; Batch:231/468; Training loss:0.09542\n",
      "Epoch: 19/20; Batch:232/468; Training loss:0.0981773\n",
      "Epoch: 19/20; Batch:233/468; Training loss:0.0978779\n",
      "Epoch: 19/20; Batch:234/468; Training loss:0.0991677\n",
      "Epoch: 19/20; Batch:235/468; Training loss:0.0933273\n",
      "Epoch: 19/20; Batch:236/468; Training loss:0.0944015\n",
      "Epoch: 19/20; Batch:237/468; Training loss:0.0956967\n",
      "Epoch: 19/20; Batch:238/468; Training loss:0.0966508\n",
      "Epoch: 19/20; Batch:239/468; Training loss:0.0973588\n",
      "Epoch: 19/20; Batch:240/468; Training loss:0.0957844\n",
      "Epoch: 19/20; Batch:241/468; Training loss:0.0954676\n",
      "Epoch: 19/20; Batch:242/468; Training loss:0.0969124\n",
      "Epoch: 19/20; Batch:243/468; Training loss:0.0965182\n",
      "Epoch: 19/20; Batch:244/468; Training loss:0.0937119\n",
      "Epoch: 19/20; Batch:245/468; Training loss:0.097004\n",
      "Epoch: 19/20; Batch:246/468; Training loss:0.0982697\n",
      "Epoch: 19/20; Batch:247/468; Training loss:0.0947965\n",
      "Epoch: 19/20; Batch:248/468; Training loss:0.0960798\n",
      "Epoch: 19/20; Batch:249/468; Training loss:0.093817\n",
      "Epoch: 19/20; Batch:250/468; Training loss:0.0942869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20; Batch:251/468; Training loss:0.093042\n",
      "Epoch: 19/20; Batch:252/468; Training loss:0.0994352\n",
      "Epoch: 19/20; Batch:253/468; Training loss:0.0973923\n",
      "Epoch: 19/20; Batch:254/468; Training loss:0.0965694\n",
      "Epoch: 19/20; Batch:255/468; Training loss:0.0944531\n",
      "Epoch: 19/20; Batch:256/468; Training loss:0.0990346\n",
      "Epoch: 19/20; Batch:257/468; Training loss:0.094406\n",
      "Epoch: 19/20; Batch:258/468; Training loss:0.0929632\n",
      "Epoch: 19/20; Batch:259/468; Training loss:0.0930438\n",
      "Epoch: 19/20; Batch:260/468; Training loss:0.0989727\n",
      "Epoch: 19/20; Batch:261/468; Training loss:0.0965398\n",
      "Epoch: 19/20; Batch:262/468; Training loss:0.0994008\n",
      "Epoch: 19/20; Batch:263/468; Training loss:0.0987745\n",
      "Epoch: 19/20; Batch:264/468; Training loss:0.0979435\n",
      "Epoch: 19/20; Batch:265/468; Training loss:0.0986899\n",
      "Epoch: 19/20; Batch:266/468; Training loss:0.0944289\n",
      "Epoch: 19/20; Batch:267/468; Training loss:0.0957033\n",
      "Epoch: 19/20; Batch:268/468; Training loss:0.0984518\n",
      "Epoch: 19/20; Batch:269/468; Training loss:0.0980976\n",
      "Epoch: 19/20; Batch:270/468; Training loss:0.0980105\n",
      "Epoch: 19/20; Batch:271/468; Training loss:0.0982217\n",
      "Epoch: 19/20; Batch:272/468; Training loss:0.0997799\n",
      "Epoch: 19/20; Batch:273/468; Training loss:0.0969579\n",
      "Epoch: 19/20; Batch:274/468; Training loss:0.0974413\n",
      "Epoch: 19/20; Batch:275/468; Training loss:0.0950354\n",
      "Epoch: 19/20; Batch:276/468; Training loss:0.0920488\n",
      "Epoch: 19/20; Batch:277/468; Training loss:0.092346\n",
      "Epoch: 19/20; Batch:278/468; Training loss:0.0969711\n",
      "Epoch: 19/20; Batch:279/468; Training loss:0.102019\n",
      "Epoch: 19/20; Batch:280/468; Training loss:0.0986747\n",
      "Epoch: 19/20; Batch:281/468; Training loss:0.0978863\n",
      "Epoch: 19/20; Batch:282/468; Training loss:0.0938429\n",
      "Epoch: 19/20; Batch:283/468; Training loss:0.097895\n",
      "Epoch: 19/20; Batch:284/468; Training loss:0.0955416\n",
      "Epoch: 19/20; Batch:285/468; Training loss:0.101433\n",
      "Epoch: 19/20; Batch:286/468; Training loss:0.0939934\n",
      "Epoch: 19/20; Batch:287/468; Training loss:0.0982184\n",
      "Epoch: 19/20; Batch:288/468; Training loss:0.0951494\n",
      "Epoch: 19/20; Batch:289/468; Training loss:0.0891651\n",
      "Epoch: 19/20; Batch:290/468; Training loss:0.0948951\n",
      "Epoch: 19/20; Batch:291/468; Training loss:0.0950661\n",
      "Epoch: 19/20; Batch:292/468; Training loss:0.0960753\n",
      "Epoch: 19/20; Batch:293/468; Training loss:0.0983112\n",
      "Epoch: 19/20; Batch:294/468; Training loss:0.0964285\n",
      "Epoch: 19/20; Batch:295/468; Training loss:0.0954233\n",
      "Epoch: 19/20; Batch:296/468; Training loss:0.0937037\n",
      "Epoch: 19/20; Batch:297/468; Training loss:0.0961368\n",
      "Epoch: 19/20; Batch:298/468; Training loss:0.0949954\n",
      "Epoch: 19/20; Batch:299/468; Training loss:0.0980233\n",
      "Epoch: 19/20; Batch:300/468; Training loss:0.0927578\n",
      "Epoch: 19/20; Batch:301/468; Training loss:0.0937089\n",
      "Epoch: 19/20; Batch:302/468; Training loss:0.0924381\n",
      "Epoch: 19/20; Batch:303/468; Training loss:0.0928735\n",
      "Epoch: 19/20; Batch:304/468; Training loss:0.0950253\n",
      "Epoch: 19/20; Batch:305/468; Training loss:0.095941\n",
      "Epoch: 19/20; Batch:306/468; Training loss:0.0959549\n",
      "Epoch: 19/20; Batch:307/468; Training loss:0.0933655\n",
      "Epoch: 19/20; Batch:308/468; Training loss:0.0944909\n",
      "Epoch: 19/20; Batch:309/468; Training loss:0.0980026\n",
      "Epoch: 19/20; Batch:310/468; Training loss:0.0992607\n",
      "Epoch: 19/20; Batch:311/468; Training loss:0.0959407\n",
      "Epoch: 19/20; Batch:312/468; Training loss:0.0992275\n",
      "Epoch: 19/20; Batch:313/468; Training loss:0.0980568\n",
      "Epoch: 19/20; Batch:314/468; Training loss:0.0913655\n",
      "Epoch: 19/20; Batch:315/468; Training loss:0.092713\n",
      "Epoch: 19/20; Batch:316/468; Training loss:0.0948593\n",
      "Epoch: 19/20; Batch:317/468; Training loss:0.0940271\n",
      "Epoch: 19/20; Batch:318/468; Training loss:0.0967245\n",
      "Epoch: 19/20; Batch:319/468; Training loss:0.0947768\n",
      "Epoch: 19/20; Batch:320/468; Training loss:0.094757\n",
      "Epoch: 19/20; Batch:321/468; Training loss:0.0992217\n",
      "Epoch: 19/20; Batch:322/468; Training loss:0.0936736\n",
      "Epoch: 19/20; Batch:323/468; Training loss:0.0968718\n",
      "Epoch: 19/20; Batch:324/468; Training loss:0.0924845\n",
      "Epoch: 19/20; Batch:325/468; Training loss:0.0971998\n",
      "Epoch: 19/20; Batch:326/468; Training loss:0.0945319\n",
      "Epoch: 19/20; Batch:327/468; Training loss:0.0939455\n",
      "Epoch: 19/20; Batch:328/468; Training loss:0.0932262\n",
      "Epoch: 19/20; Batch:329/468; Training loss:0.0997422\n",
      "Epoch: 19/20; Batch:330/468; Training loss:0.0947397\n",
      "Epoch: 19/20; Batch:331/468; Training loss:0.0942894\n",
      "Epoch: 19/20; Batch:332/468; Training loss:0.0999521\n",
      "Epoch: 19/20; Batch:333/468; Training loss:0.0928068\n",
      "Epoch: 19/20; Batch:334/468; Training loss:0.0967326\n",
      "Epoch: 19/20; Batch:335/468; Training loss:0.0957889\n",
      "Epoch: 19/20; Batch:336/468; Training loss:0.0970815\n",
      "Epoch: 19/20; Batch:337/468; Training loss:0.0967206\n",
      "Epoch: 19/20; Batch:338/468; Training loss:0.0946355\n",
      "Epoch: 19/20; Batch:339/468; Training loss:0.096038\n",
      "Epoch: 19/20; Batch:340/468; Training loss:0.0953519\n",
      "Epoch: 19/20; Batch:341/468; Training loss:0.0928822\n",
      "Epoch: 19/20; Batch:342/468; Training loss:0.0953741\n",
      "Epoch: 19/20; Batch:343/468; Training loss:0.0945902\n",
      "Epoch: 19/20; Batch:344/468; Training loss:0.0962823\n",
      "Epoch: 19/20; Batch:345/468; Training loss:0.0960164\n",
      "Epoch: 19/20; Batch:346/468; Training loss:0.0940596\n",
      "Epoch: 19/20; Batch:347/468; Training loss:0.0970799\n",
      "Epoch: 19/20; Batch:348/468; Training loss:0.0985191\n",
      "Epoch: 19/20; Batch:349/468; Training loss:0.0975075\n",
      "Epoch: 19/20; Batch:350/468; Training loss:0.097784\n",
      "Epoch: 19/20; Batch:351/468; Training loss:0.0971584\n",
      "Epoch: 19/20; Batch:352/468; Training loss:0.0971213\n",
      "Epoch: 19/20; Batch:353/468; Training loss:0.0961987\n",
      "Epoch: 19/20; Batch:354/468; Training loss:0.0942644\n",
      "Epoch: 19/20; Batch:355/468; Training loss:0.0955695\n",
      "Epoch: 19/20; Batch:356/468; Training loss:0.0961536\n",
      "Epoch: 19/20; Batch:357/468; Training loss:0.0911926\n",
      "Epoch: 19/20; Batch:358/468; Training loss:0.0953873\n",
      "Epoch: 19/20; Batch:359/468; Training loss:0.0976116\n",
      "Epoch: 19/20; Batch:360/468; Training loss:0.0943905\n",
      "Epoch: 19/20; Batch:361/468; Training loss:0.0957128\n",
      "Epoch: 19/20; Batch:362/468; Training loss:0.0960793\n",
      "Epoch: 19/20; Batch:363/468; Training loss:0.0950337\n",
      "Epoch: 19/20; Batch:364/468; Training loss:0.0974776\n",
      "Epoch: 19/20; Batch:365/468; Training loss:0.0974448\n",
      "Epoch: 19/20; Batch:366/468; Training loss:0.0935742\n",
      "Epoch: 19/20; Batch:367/468; Training loss:0.0960429\n",
      "Epoch: 19/20; Batch:368/468; Training loss:0.0967514\n",
      "Epoch: 19/20; Batch:369/468; Training loss:0.0962303\n",
      "Epoch: 19/20; Batch:370/468; Training loss:0.0914603\n",
      "Epoch: 19/20; Batch:371/468; Training loss:0.095718\n",
      "Epoch: 19/20; Batch:372/468; Training loss:0.0990719\n",
      "Epoch: 19/20; Batch:373/468; Training loss:0.0943617\n",
      "Epoch: 19/20; Batch:374/468; Training loss:0.0943365\n",
      "Epoch: 19/20; Batch:375/468; Training loss:0.0941579\n",
      "Epoch: 19/20; Batch:376/468; Training loss:0.0956239\n",
      "Epoch: 19/20; Batch:377/468; Training loss:0.0942\n",
      "Epoch: 19/20; Batch:378/468; Training loss:0.0951292\n",
      "Epoch: 19/20; Batch:379/468; Training loss:0.0931881\n",
      "Epoch: 19/20; Batch:380/468; Training loss:0.0962751\n",
      "Epoch: 19/20; Batch:381/468; Training loss:0.0958012\n",
      "Epoch: 19/20; Batch:382/468; Training loss:0.0947664\n",
      "Epoch: 19/20; Batch:383/468; Training loss:0.094869\n",
      "Epoch: 19/20; Batch:384/468; Training loss:0.0963779\n",
      "Epoch: 19/20; Batch:385/468; Training loss:0.0959691\n",
      "Epoch: 19/20; Batch:386/468; Training loss:0.0935932\n",
      "Epoch: 19/20; Batch:387/468; Training loss:0.093857\n",
      "Epoch: 19/20; Batch:388/468; Training loss:0.0936632\n",
      "Epoch: 19/20; Batch:389/468; Training loss:0.0950901\n",
      "Epoch: 19/20; Batch:390/468; Training loss:0.0951694\n",
      "Epoch: 19/20; Batch:391/468; Training loss:0.0968256\n",
      "Epoch: 19/20; Batch:392/468; Training loss:0.0938801\n",
      "Epoch: 19/20; Batch:393/468; Training loss:0.0964438\n",
      "Epoch: 19/20; Batch:394/468; Training loss:0.0957075\n",
      "Epoch: 19/20; Batch:395/468; Training loss:0.0945619\n",
      "Epoch: 19/20; Batch:396/468; Training loss:0.0985508\n",
      "Epoch: 19/20; Batch:397/468; Training loss:0.0949329\n",
      "Epoch: 19/20; Batch:398/468; Training loss:0.0965973\n",
      "Epoch: 19/20; Batch:399/468; Training loss:0.0978496\n",
      "Epoch: 19/20; Batch:400/468; Training loss:0.0972123\n",
      "Epoch: 19/20; Batch:401/468; Training loss:0.0989739\n",
      "Epoch: 19/20; Batch:402/468; Training loss:0.0949904\n",
      "Epoch: 19/20; Batch:403/468; Training loss:0.0952066\n",
      "Epoch: 19/20; Batch:404/468; Training loss:0.0966833\n",
      "Epoch: 19/20; Batch:405/468; Training loss:0.0933168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20; Batch:406/468; Training loss:0.0972547\n",
      "Epoch: 19/20; Batch:407/468; Training loss:0.0950007\n",
      "Epoch: 19/20; Batch:408/468; Training loss:0.0976907\n",
      "Epoch: 19/20; Batch:409/468; Training loss:0.0923829\n",
      "Epoch: 19/20; Batch:410/468; Training loss:0.0953776\n",
      "Epoch: 19/20; Batch:411/468; Training loss:0.0958169\n",
      "Epoch: 19/20; Batch:412/468; Training loss:0.096231\n",
      "Epoch: 19/20; Batch:413/468; Training loss:0.0956642\n",
      "Epoch: 19/20; Batch:414/468; Training loss:0.0946803\n",
      "Epoch: 19/20; Batch:415/468; Training loss:0.0920698\n",
      "Epoch: 19/20; Batch:416/468; Training loss:0.0976646\n",
      "Epoch: 19/20; Batch:417/468; Training loss:0.0923232\n",
      "Epoch: 19/20; Batch:418/468; Training loss:0.0905707\n",
      "Epoch: 19/20; Batch:419/468; Training loss:0.096014\n",
      "Epoch: 19/20; Batch:420/468; Training loss:0.0939256\n",
      "Epoch: 19/20; Batch:421/468; Training loss:0.0968136\n",
      "Epoch: 19/20; Batch:422/468; Training loss:0.0949175\n",
      "Epoch: 19/20; Batch:423/468; Training loss:0.0977403\n",
      "Epoch: 19/20; Batch:424/468; Training loss:0.0936261\n",
      "Epoch: 19/20; Batch:425/468; Training loss:0.0953215\n",
      "Epoch: 19/20; Batch:426/468; Training loss:0.0996537\n",
      "Epoch: 19/20; Batch:427/468; Training loss:0.0985123\n",
      "Epoch: 19/20; Batch:428/468; Training loss:0.0997904\n",
      "Epoch: 19/20; Batch:429/468; Training loss:0.0954387\n",
      "Epoch: 19/20; Batch:430/468; Training loss:0.0965532\n",
      "Epoch: 19/20; Batch:431/468; Training loss:0.0933974\n",
      "Epoch: 19/20; Batch:432/468; Training loss:0.0966271\n",
      "Epoch: 19/20; Batch:433/468; Training loss:0.0946283\n",
      "Epoch: 19/20; Batch:434/468; Training loss:0.0953318\n",
      "Epoch: 19/20; Batch:435/468; Training loss:0.0983079\n",
      "Epoch: 19/20; Batch:436/468; Training loss:0.0914673\n",
      "Epoch: 19/20; Batch:437/468; Training loss:0.0944218\n",
      "Epoch: 19/20; Batch:438/468; Training loss:0.0972751\n",
      "Epoch: 19/20; Batch:439/468; Training loss:0.0972448\n",
      "Epoch: 19/20; Batch:440/468; Training loss:0.0975855\n",
      "Epoch: 19/20; Batch:441/468; Training loss:0.0963026\n",
      "Epoch: 19/20; Batch:442/468; Training loss:0.097684\n",
      "Epoch: 19/20; Batch:443/468; Training loss:0.0983345\n",
      "Epoch: 19/20; Batch:444/468; Training loss:0.0946895\n",
      "Epoch: 19/20; Batch:445/468; Training loss:0.0957736\n",
      "Epoch: 19/20; Batch:446/468; Training loss:0.0962745\n",
      "Epoch: 19/20; Batch:447/468; Training loss:0.0971925\n",
      "Epoch: 19/20; Batch:448/468; Training loss:0.0960355\n",
      "Epoch: 19/20; Batch:449/468; Training loss:0.0949019\n",
      "Epoch: 19/20; Batch:450/468; Training loss:0.0961259\n",
      "Epoch: 19/20; Batch:451/468; Training loss:0.0943177\n",
      "Epoch: 19/20; Batch:452/468; Training loss:0.0941541\n",
      "Epoch: 19/20; Batch:453/468; Training loss:0.0998899\n",
      "Epoch: 19/20; Batch:454/468; Training loss:0.0940819\n",
      "Epoch: 19/20; Batch:455/468; Training loss:0.0971971\n",
      "Epoch: 19/20; Batch:456/468; Training loss:0.095668\n",
      "Epoch: 19/20; Batch:457/468; Training loss:0.0936098\n",
      "Epoch: 19/20; Batch:458/468; Training loss:0.0934455\n",
      "Epoch: 19/20; Batch:459/468; Training loss:0.0942139\n",
      "Epoch: 19/20; Batch:460/468; Training loss:0.0967407\n",
      "Epoch: 19/20; Batch:461/468; Training loss:0.093641\n",
      "Epoch: 19/20; Batch:462/468; Training loss:0.0981881\n",
      "Epoch: 19/20; Batch:463/468; Training loss:0.0973019\n",
      "Epoch: 19/20; Batch:464/468; Training loss:0.09604\n",
      "Epoch: 19/20; Batch:465/468; Training loss:0.0977768\n",
      "Epoch: 19/20; Batch:466/468; Training loss:0.0914613\n",
      "Epoch: 19/20; Batch:467/468; Training loss:0.099793\n",
      "Epoch: 19/20; Batch:468/468; Training loss:0.0987939\n",
      "Epoch: 20/20; Batch:1/468; Training loss:0.0992977\n",
      "Epoch: 20/20; Batch:2/468; Training loss:0.0962107\n",
      "Epoch: 20/20; Batch:3/468; Training loss:0.0925886\n",
      "Epoch: 20/20; Batch:4/468; Training loss:0.0964495\n",
      "Epoch: 20/20; Batch:5/468; Training loss:0.0976501\n",
      "Epoch: 20/20; Batch:6/468; Training loss:0.096723\n",
      "Epoch: 20/20; Batch:7/468; Training loss:0.0949447\n",
      "Epoch: 20/20; Batch:8/468; Training loss:0.0958274\n",
      "Epoch: 20/20; Batch:9/468; Training loss:0.0990963\n",
      "Epoch: 20/20; Batch:10/468; Training loss:0.0899606\n",
      "Epoch: 20/20; Batch:11/468; Training loss:0.0998736\n",
      "Epoch: 20/20; Batch:12/468; Training loss:0.0900558\n",
      "Epoch: 20/20; Batch:13/468; Training loss:0.0965496\n",
      "Epoch: 20/20; Batch:14/468; Training loss:0.0972928\n",
      "Epoch: 20/20; Batch:15/468; Training loss:0.0914261\n",
      "Epoch: 20/20; Batch:16/468; Training loss:0.095064\n",
      "Epoch: 20/20; Batch:17/468; Training loss:0.0957726\n",
      "Epoch: 20/20; Batch:18/468; Training loss:0.0936415\n",
      "Epoch: 20/20; Batch:19/468; Training loss:0.0970886\n",
      "Epoch: 20/20; Batch:20/468; Training loss:0.0946009\n",
      "Epoch: 20/20; Batch:21/468; Training loss:0.0950855\n",
      "Epoch: 20/20; Batch:22/468; Training loss:0.0997918\n",
      "Epoch: 20/20; Batch:23/468; Training loss:0.0974029\n",
      "Epoch: 20/20; Batch:24/468; Training loss:0.0980321\n",
      "Epoch: 20/20; Batch:25/468; Training loss:0.0961763\n",
      "Epoch: 20/20; Batch:26/468; Training loss:0.101217\n",
      "Epoch: 20/20; Batch:27/468; Training loss:0.0980494\n",
      "Epoch: 20/20; Batch:28/468; Training loss:0.0986835\n",
      "Epoch: 20/20; Batch:29/468; Training loss:0.0968296\n",
      "Epoch: 20/20; Batch:30/468; Training loss:0.0946946\n",
      "Epoch: 20/20; Batch:31/468; Training loss:0.0958524\n",
      "Epoch: 20/20; Batch:32/468; Training loss:0.0958648\n",
      "Epoch: 20/20; Batch:33/468; Training loss:0.0968088\n",
      "Epoch: 20/20; Batch:34/468; Training loss:0.0959474\n",
      "Epoch: 20/20; Batch:35/468; Training loss:0.0981217\n",
      "Epoch: 20/20; Batch:36/468; Training loss:0.094257\n",
      "Epoch: 20/20; Batch:37/468; Training loss:0.0968258\n",
      "Epoch: 20/20; Batch:38/468; Training loss:0.097414\n",
      "Epoch: 20/20; Batch:39/468; Training loss:0.0945386\n",
      "Epoch: 20/20; Batch:40/468; Training loss:0.0935147\n",
      "Epoch: 20/20; Batch:41/468; Training loss:0.0980207\n",
      "Epoch: 20/20; Batch:42/468; Training loss:0.0963988\n",
      "Epoch: 20/20; Batch:43/468; Training loss:0.0963653\n",
      "Epoch: 20/20; Batch:44/468; Training loss:0.0949553\n",
      "Epoch: 20/20; Batch:45/468; Training loss:0.0926472\n",
      "Epoch: 20/20; Batch:46/468; Training loss:0.0964814\n",
      "Epoch: 20/20; Batch:47/468; Training loss:0.0922143\n",
      "Epoch: 20/20; Batch:48/468; Training loss:0.096213\n",
      "Epoch: 20/20; Batch:49/468; Training loss:0.094737\n",
      "Epoch: 20/20; Batch:50/468; Training loss:0.0939993\n",
      "Epoch: 20/20; Batch:51/468; Training loss:0.0956535\n",
      "Epoch: 20/20; Batch:52/468; Training loss:0.0967495\n",
      "Epoch: 20/20; Batch:53/468; Training loss:0.0947678\n",
      "Epoch: 20/20; Batch:54/468; Training loss:0.0967516\n",
      "Epoch: 20/20; Batch:55/468; Training loss:0.0899938\n",
      "Epoch: 20/20; Batch:56/468; Training loss:0.095368\n",
      "Epoch: 20/20; Batch:57/468; Training loss:0.0943763\n",
      "Epoch: 20/20; Batch:58/468; Training loss:0.097518\n",
      "Epoch: 20/20; Batch:59/468; Training loss:0.0978227\n",
      "Epoch: 20/20; Batch:60/468; Training loss:0.0974799\n",
      "Epoch: 20/20; Batch:61/468; Training loss:0.0990609\n",
      "Epoch: 20/20; Batch:62/468; Training loss:0.095908\n",
      "Epoch: 20/20; Batch:63/468; Training loss:0.0938901\n",
      "Epoch: 20/20; Batch:64/468; Training loss:0.0948976\n",
      "Epoch: 20/20; Batch:65/468; Training loss:0.094988\n",
      "Epoch: 20/20; Batch:66/468; Training loss:0.0968831\n",
      "Epoch: 20/20; Batch:67/468; Training loss:0.0961711\n",
      "Epoch: 20/20; Batch:68/468; Training loss:0.0963941\n",
      "Epoch: 20/20; Batch:69/468; Training loss:0.0947409\n",
      "Epoch: 20/20; Batch:70/468; Training loss:0.0979608\n",
      "Epoch: 20/20; Batch:71/468; Training loss:0.095556\n",
      "Epoch: 20/20; Batch:72/468; Training loss:0.0982542\n",
      "Epoch: 20/20; Batch:73/468; Training loss:0.0974131\n",
      "Epoch: 20/20; Batch:74/468; Training loss:0.0952891\n",
      "Epoch: 20/20; Batch:75/468; Training loss:0.0961672\n",
      "Epoch: 20/20; Batch:76/468; Training loss:0.0955858\n",
      "Epoch: 20/20; Batch:77/468; Training loss:0.0936178\n",
      "Epoch: 20/20; Batch:78/468; Training loss:0.0967007\n",
      "Epoch: 20/20; Batch:79/468; Training loss:0.0924796\n",
      "Epoch: 20/20; Batch:80/468; Training loss:0.0952229\n",
      "Epoch: 20/20; Batch:81/468; Training loss:0.0991554\n",
      "Epoch: 20/20; Batch:82/468; Training loss:0.0971103\n",
      "Epoch: 20/20; Batch:83/468; Training loss:0.0972755\n",
      "Epoch: 20/20; Batch:84/468; Training loss:0.0924029\n",
      "Epoch: 20/20; Batch:85/468; Training loss:0.095257\n",
      "Epoch: 20/20; Batch:86/468; Training loss:0.0946765\n",
      "Epoch: 20/20; Batch:87/468; Training loss:0.0966942\n",
      "Epoch: 20/20; Batch:88/468; Training loss:0.0953769\n",
      "Epoch: 20/20; Batch:89/468; Training loss:0.0979772\n",
      "Epoch: 20/20; Batch:90/468; Training loss:0.0976923\n",
      "Epoch: 20/20; Batch:91/468; Training loss:0.0947646\n",
      "Epoch: 20/20; Batch:92/468; Training loss:0.0956776\n",
      "Epoch: 20/20; Batch:93/468; Training loss:0.0950013\n",
      "Epoch: 20/20; Batch:94/468; Training loss:0.0932938\n",
      "Epoch: 20/20; Batch:95/468; Training loss:0.0919407\n",
      "Epoch: 20/20; Batch:96/468; Training loss:0.0964383\n",
      "Epoch: 20/20; Batch:97/468; Training loss:0.0960358\n",
      "Epoch: 20/20; Batch:98/468; Training loss:0.0939941\n",
      "Epoch: 20/20; Batch:99/468; Training loss:0.098316\n",
      "Epoch: 20/20; Batch:100/468; Training loss:0.0952912\n",
      "Epoch: 20/20; Batch:101/468; Training loss:0.0969575\n",
      "Epoch: 20/20; Batch:102/468; Training loss:0.0970404\n",
      "Epoch: 20/20; Batch:103/468; Training loss:0.0989642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20; Batch:104/468; Training loss:0.0948324\n",
      "Epoch: 20/20; Batch:105/468; Training loss:0.0928932\n",
      "Epoch: 20/20; Batch:106/468; Training loss:0.0978381\n",
      "Epoch: 20/20; Batch:107/468; Training loss:0.093667\n",
      "Epoch: 20/20; Batch:108/468; Training loss:0.0995155\n",
      "Epoch: 20/20; Batch:109/468; Training loss:0.0978176\n",
      "Epoch: 20/20; Batch:110/468; Training loss:0.0946085\n",
      "Epoch: 20/20; Batch:111/468; Training loss:0.0915617\n",
      "Epoch: 20/20; Batch:112/468; Training loss:0.0940763\n",
      "Epoch: 20/20; Batch:113/468; Training loss:0.0945919\n",
      "Epoch: 20/20; Batch:114/468; Training loss:0.0924509\n",
      "Epoch: 20/20; Batch:115/468; Training loss:0.0975019\n",
      "Epoch: 20/20; Batch:116/468; Training loss:0.0942881\n",
      "Epoch: 20/20; Batch:117/468; Training loss:0.0931052\n",
      "Epoch: 20/20; Batch:118/468; Training loss:0.0957453\n",
      "Epoch: 20/20; Batch:119/468; Training loss:0.0921179\n",
      "Epoch: 20/20; Batch:120/468; Training loss:0.0944602\n",
      "Epoch: 20/20; Batch:121/468; Training loss:0.0959525\n",
      "Epoch: 20/20; Batch:122/468; Training loss:0.0969139\n",
      "Epoch: 20/20; Batch:123/468; Training loss:0.0951487\n",
      "Epoch: 20/20; Batch:124/468; Training loss:0.0960369\n",
      "Epoch: 20/20; Batch:125/468; Training loss:0.0984867\n",
      "Epoch: 20/20; Batch:126/468; Training loss:0.0992175\n",
      "Epoch: 20/20; Batch:127/468; Training loss:0.0960837\n",
      "Epoch: 20/20; Batch:128/468; Training loss:0.0950484\n",
      "Epoch: 20/20; Batch:129/468; Training loss:0.0962907\n",
      "Epoch: 20/20; Batch:130/468; Training loss:0.0956302\n",
      "Epoch: 20/20; Batch:131/468; Training loss:0.0962852\n",
      "Epoch: 20/20; Batch:132/468; Training loss:0.0931896\n",
      "Epoch: 20/20; Batch:133/468; Training loss:0.0963715\n",
      "Epoch: 20/20; Batch:134/468; Training loss:0.0915933\n",
      "Epoch: 20/20; Batch:135/468; Training loss:0.0950565\n",
      "Epoch: 20/20; Batch:136/468; Training loss:0.0955528\n",
      "Epoch: 20/20; Batch:137/468; Training loss:0.0933638\n",
      "Epoch: 20/20; Batch:138/468; Training loss:0.0971477\n",
      "Epoch: 20/20; Batch:139/468; Training loss:0.0984298\n",
      "Epoch: 20/20; Batch:140/468; Training loss:0.0909784\n",
      "Epoch: 20/20; Batch:141/468; Training loss:0.0990586\n",
      "Epoch: 20/20; Batch:142/468; Training loss:0.0913968\n",
      "Epoch: 20/20; Batch:143/468; Training loss:0.0970362\n",
      "Epoch: 20/20; Batch:144/468; Training loss:0.0972531\n",
      "Epoch: 20/20; Batch:145/468; Training loss:0.0941659\n",
      "Epoch: 20/20; Batch:146/468; Training loss:0.098334\n",
      "Epoch: 20/20; Batch:147/468; Training loss:0.0943115\n",
      "Epoch: 20/20; Batch:148/468; Training loss:0.0972008\n",
      "Epoch: 20/20; Batch:149/468; Training loss:0.0959082\n",
      "Epoch: 20/20; Batch:150/468; Training loss:0.0957531\n",
      "Epoch: 20/20; Batch:151/468; Training loss:0.0978922\n",
      "Epoch: 20/20; Batch:152/468; Training loss:0.099704\n",
      "Epoch: 20/20; Batch:153/468; Training loss:0.0967264\n",
      "Epoch: 20/20; Batch:154/468; Training loss:0.0967732\n",
      "Epoch: 20/20; Batch:155/468; Training loss:0.0950407\n",
      "Epoch: 20/20; Batch:156/468; Training loss:0.0961953\n",
      "Epoch: 20/20; Batch:157/468; Training loss:0.0966369\n",
      "Epoch: 20/20; Batch:158/468; Training loss:0.0972094\n",
      "Epoch: 20/20; Batch:159/468; Training loss:0.094954\n",
      "Epoch: 20/20; Batch:160/468; Training loss:0.0960943\n",
      "Epoch: 20/20; Batch:161/468; Training loss:0.0944737\n",
      "Epoch: 20/20; Batch:162/468; Training loss:0.096603\n",
      "Epoch: 20/20; Batch:163/468; Training loss:0.0928186\n",
      "Epoch: 20/20; Batch:164/468; Training loss:0.0929575\n",
      "Epoch: 20/20; Batch:165/468; Training loss:0.098228\n",
      "Epoch: 20/20; Batch:166/468; Training loss:0.0932297\n",
      "Epoch: 20/20; Batch:167/468; Training loss:0.0933037\n",
      "Epoch: 20/20; Batch:168/468; Training loss:0.0948178\n",
      "Epoch: 20/20; Batch:169/468; Training loss:0.093357\n",
      "Epoch: 20/20; Batch:170/468; Training loss:0.0932694\n",
      "Epoch: 20/20; Batch:171/468; Training loss:0.0937674\n",
      "Epoch: 20/20; Batch:172/468; Training loss:0.0937003\n",
      "Epoch: 20/20; Batch:173/468; Training loss:0.0909826\n",
      "Epoch: 20/20; Batch:174/468; Training loss:0.0940182\n",
      "Epoch: 20/20; Batch:175/468; Training loss:0.0952891\n",
      "Epoch: 20/20; Batch:176/468; Training loss:0.0981342\n",
      "Epoch: 20/20; Batch:177/468; Training loss:0.0956327\n",
      "Epoch: 20/20; Batch:178/468; Training loss:0.0919953\n",
      "Epoch: 20/20; Batch:179/468; Training loss:0.0948424\n",
      "Epoch: 20/20; Batch:180/468; Training loss:0.097251\n",
      "Epoch: 20/20; Batch:181/468; Training loss:0.0985411\n",
      "Epoch: 20/20; Batch:182/468; Training loss:0.0931567\n",
      "Epoch: 20/20; Batch:183/468; Training loss:0.0978697\n",
      "Epoch: 20/20; Batch:184/468; Training loss:0.0983867\n",
      "Epoch: 20/20; Batch:185/468; Training loss:0.0972861\n",
      "Epoch: 20/20; Batch:186/468; Training loss:0.095637\n",
      "Epoch: 20/20; Batch:187/468; Training loss:0.093284\n",
      "Epoch: 20/20; Batch:188/468; Training loss:0.0957073\n",
      "Epoch: 20/20; Batch:189/468; Training loss:0.0942518\n",
      "Epoch: 20/20; Batch:190/468; Training loss:0.0932789\n",
      "Epoch: 20/20; Batch:191/468; Training loss:0.0991298\n",
      "Epoch: 20/20; Batch:192/468; Training loss:0.0929541\n",
      "Epoch: 20/20; Batch:193/468; Training loss:0.0925996\n",
      "Epoch: 20/20; Batch:194/468; Training loss:0.0978783\n",
      "Epoch: 20/20; Batch:195/468; Training loss:0.0974493\n",
      "Epoch: 20/20; Batch:196/468; Training loss:0.0963917\n",
      "Epoch: 20/20; Batch:197/468; Training loss:0.0938172\n",
      "Epoch: 20/20; Batch:198/468; Training loss:0.0933063\n",
      "Epoch: 20/20; Batch:199/468; Training loss:0.0958977\n",
      "Epoch: 20/20; Batch:200/468; Training loss:0.0978045\n",
      "Epoch: 20/20; Batch:201/468; Training loss:0.0974401\n",
      "Epoch: 20/20; Batch:202/468; Training loss:0.0936331\n",
      "Epoch: 20/20; Batch:203/468; Training loss:0.100184\n",
      "Epoch: 20/20; Batch:204/468; Training loss:0.0962264\n",
      "Epoch: 20/20; Batch:205/468; Training loss:0.0944503\n",
      "Epoch: 20/20; Batch:206/468; Training loss:0.0980436\n",
      "Epoch: 20/20; Batch:207/468; Training loss:0.101324\n",
      "Epoch: 20/20; Batch:208/468; Training loss:0.0959017\n",
      "Epoch: 20/20; Batch:209/468; Training loss:0.091116\n",
      "Epoch: 20/20; Batch:210/468; Training loss:0.0991046\n",
      "Epoch: 20/20; Batch:211/468; Training loss:0.0994383\n",
      "Epoch: 20/20; Batch:212/468; Training loss:0.097189\n",
      "Epoch: 20/20; Batch:213/468; Training loss:0.0999593\n",
      "Epoch: 20/20; Batch:214/468; Training loss:0.0945136\n",
      "Epoch: 20/20; Batch:215/468; Training loss:0.0969011\n",
      "Epoch: 20/20; Batch:216/468; Training loss:0.0974537\n",
      "Epoch: 20/20; Batch:217/468; Training loss:0.0939733\n",
      "Epoch: 20/20; Batch:218/468; Training loss:0.0964119\n",
      "Epoch: 20/20; Batch:219/468; Training loss:0.0973412\n",
      "Epoch: 20/20; Batch:220/468; Training loss:0.0907655\n",
      "Epoch: 20/20; Batch:221/468; Training loss:0.0913807\n",
      "Epoch: 20/20; Batch:222/468; Training loss:0.0955041\n",
      "Epoch: 20/20; Batch:223/468; Training loss:0.0968354\n",
      "Epoch: 20/20; Batch:224/468; Training loss:0.0956748\n",
      "Epoch: 20/20; Batch:225/468; Training loss:0.0968806\n",
      "Epoch: 20/20; Batch:226/468; Training loss:0.0965427\n",
      "Epoch: 20/20; Batch:227/468; Training loss:0.0946593\n",
      "Epoch: 20/20; Batch:228/468; Training loss:0.0984551\n",
      "Epoch: 20/20; Batch:229/468; Training loss:0.0943237\n",
      "Epoch: 20/20; Batch:230/468; Training loss:0.0962596\n",
      "Epoch: 20/20; Batch:231/468; Training loss:0.0922521\n",
      "Epoch: 20/20; Batch:232/468; Training loss:0.0953433\n",
      "Epoch: 20/20; Batch:233/468; Training loss:0.094886\n",
      "Epoch: 20/20; Batch:234/468; Training loss:0.0972831\n",
      "Epoch: 20/20; Batch:235/468; Training loss:0.0972298\n",
      "Epoch: 20/20; Batch:236/468; Training loss:0.0971734\n",
      "Epoch: 20/20; Batch:237/468; Training loss:0.0973862\n",
      "Epoch: 20/20; Batch:238/468; Training loss:0.0971678\n",
      "Epoch: 20/20; Batch:239/468; Training loss:0.097089\n",
      "Epoch: 20/20; Batch:240/468; Training loss:0.0924527\n",
      "Epoch: 20/20; Batch:241/468; Training loss:0.101059\n",
      "Epoch: 20/20; Batch:242/468; Training loss:0.095474\n",
      "Epoch: 20/20; Batch:243/468; Training loss:0.0982683\n",
      "Epoch: 20/20; Batch:244/468; Training loss:0.0961047\n",
      "Epoch: 20/20; Batch:245/468; Training loss:0.0972615\n",
      "Epoch: 20/20; Batch:246/468; Training loss:0.0971432\n",
      "Epoch: 20/20; Batch:247/468; Training loss:0.091733\n",
      "Epoch: 20/20; Batch:248/468; Training loss:0.095221\n",
      "Epoch: 20/20; Batch:249/468; Training loss:0.0981212\n",
      "Epoch: 20/20; Batch:250/468; Training loss:0.0960934\n",
      "Epoch: 20/20; Batch:251/468; Training loss:0.0972684\n",
      "Epoch: 20/20; Batch:252/468; Training loss:0.0959366\n",
      "Epoch: 20/20; Batch:253/468; Training loss:0.0954778\n",
      "Epoch: 20/20; Batch:254/468; Training loss:0.0973955\n",
      "Epoch: 20/20; Batch:255/468; Training loss:0.0946079\n",
      "Epoch: 20/20; Batch:256/468; Training loss:0.0966923\n",
      "Epoch: 20/20; Batch:257/468; Training loss:0.0949587\n",
      "Epoch: 20/20; Batch:258/468; Training loss:0.0947766\n",
      "Epoch: 20/20; Batch:259/468; Training loss:0.0934568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20; Batch:260/468; Training loss:0.0981477\n",
      "Epoch: 20/20; Batch:261/468; Training loss:0.0919587\n",
      "Epoch: 20/20; Batch:262/468; Training loss:0.0936971\n",
      "Epoch: 20/20; Batch:263/468; Training loss:0.091324\n",
      "Epoch: 20/20; Batch:264/468; Training loss:0.0950537\n",
      "Epoch: 20/20; Batch:265/468; Training loss:0.0937096\n",
      "Epoch: 20/20; Batch:266/468; Training loss:0.0993214\n",
      "Epoch: 20/20; Batch:267/468; Training loss:0.0942981\n",
      "Epoch: 20/20; Batch:268/468; Training loss:0.0976209\n",
      "Epoch: 20/20; Batch:269/468; Training loss:0.093726\n",
      "Epoch: 20/20; Batch:270/468; Training loss:0.0937592\n",
      "Epoch: 20/20; Batch:271/468; Training loss:0.0985978\n",
      "Epoch: 20/20; Batch:272/468; Training loss:0.0953669\n",
      "Epoch: 20/20; Batch:273/468; Training loss:0.0942757\n",
      "Epoch: 20/20; Batch:274/468; Training loss:0.0970866\n",
      "Epoch: 20/20; Batch:275/468; Training loss:0.0955169\n",
      "Epoch: 20/20; Batch:276/468; Training loss:0.0954722\n",
      "Epoch: 20/20; Batch:277/468; Training loss:0.0909469\n",
      "Epoch: 20/20; Batch:278/468; Training loss:0.096321\n",
      "Epoch: 20/20; Batch:279/468; Training loss:0.0915543\n",
      "Epoch: 20/20; Batch:280/468; Training loss:0.0927548\n",
      "Epoch: 20/20; Batch:281/468; Training loss:0.0940871\n",
      "Epoch: 20/20; Batch:282/468; Training loss:0.0965994\n",
      "Epoch: 20/20; Batch:283/468; Training loss:0.0970601\n",
      "Epoch: 20/20; Batch:284/468; Training loss:0.0952063\n",
      "Epoch: 20/20; Batch:285/468; Training loss:0.0948378\n",
      "Epoch: 20/20; Batch:286/468; Training loss:0.0981454\n",
      "Epoch: 20/20; Batch:287/468; Training loss:0.0982922\n",
      "Epoch: 20/20; Batch:288/468; Training loss:0.0984318\n",
      "Epoch: 20/20; Batch:289/468; Training loss:0.096667\n",
      "Epoch: 20/20; Batch:290/468; Training loss:0.0978501\n",
      "Epoch: 20/20; Batch:291/468; Training loss:0.0973549\n",
      "Epoch: 20/20; Batch:292/468; Training loss:0.097878\n",
      "Epoch: 20/20; Batch:293/468; Training loss:0.0989917\n",
      "Epoch: 20/20; Batch:294/468; Training loss:0.0929352\n",
      "Epoch: 20/20; Batch:295/468; Training loss:0.0929399\n",
      "Epoch: 20/20; Batch:296/468; Training loss:0.0976679\n",
      "Epoch: 20/20; Batch:297/468; Training loss:0.0912206\n",
      "Epoch: 20/20; Batch:298/468; Training loss:0.09149\n",
      "Epoch: 20/20; Batch:299/468; Training loss:0.0989959\n",
      "Epoch: 20/20; Batch:300/468; Training loss:0.0957012\n",
      "Epoch: 20/20; Batch:301/468; Training loss:0.0932211\n",
      "Epoch: 20/20; Batch:302/468; Training loss:0.0963921\n",
      "Epoch: 20/20; Batch:303/468; Training loss:0.0947057\n",
      "Epoch: 20/20; Batch:304/468; Training loss:0.0950216\n",
      "Epoch: 20/20; Batch:305/468; Training loss:0.0946772\n",
      "Epoch: 20/20; Batch:306/468; Training loss:0.0919098\n",
      "Epoch: 20/20; Batch:307/468; Training loss:0.0974981\n",
      "Epoch: 20/20; Batch:308/468; Training loss:0.0951369\n",
      "Epoch: 20/20; Batch:309/468; Training loss:0.0958119\n",
      "Epoch: 20/20; Batch:310/468; Training loss:0.102021\n",
      "Epoch: 20/20; Batch:311/468; Training loss:0.0957084\n",
      "Epoch: 20/20; Batch:312/468; Training loss:0.0954405\n",
      "Epoch: 20/20; Batch:313/468; Training loss:0.0924299\n",
      "Epoch: 20/20; Batch:314/468; Training loss:0.094757\n",
      "Epoch: 20/20; Batch:315/468; Training loss:0.0949132\n",
      "Epoch: 20/20; Batch:316/468; Training loss:0.0967719\n",
      "Epoch: 20/20; Batch:317/468; Training loss:0.0969412\n",
      "Epoch: 20/20; Batch:318/468; Training loss:0.0947054\n",
      "Epoch: 20/20; Batch:319/468; Training loss:0.0931695\n",
      "Epoch: 20/20; Batch:320/468; Training loss:0.0969823\n",
      "Epoch: 20/20; Batch:321/468; Training loss:0.0972873\n",
      "Epoch: 20/20; Batch:322/468; Training loss:0.0969856\n",
      "Epoch: 20/20; Batch:323/468; Training loss:0.0973854\n",
      "Epoch: 20/20; Batch:324/468; Training loss:0.0983361\n",
      "Epoch: 20/20; Batch:325/468; Training loss:0.095892\n",
      "Epoch: 20/20; Batch:326/468; Training loss:0.0932674\n",
      "Epoch: 20/20; Batch:327/468; Training loss:0.0952227\n",
      "Epoch: 20/20; Batch:328/468; Training loss:0.0939152\n",
      "Epoch: 20/20; Batch:329/468; Training loss:0.0968582\n",
      "Epoch: 20/20; Batch:330/468; Training loss:0.0901891\n",
      "Epoch: 20/20; Batch:331/468; Training loss:0.0944804\n",
      "Epoch: 20/20; Batch:332/468; Training loss:0.0946798\n",
      "Epoch: 20/20; Batch:333/468; Training loss:0.0962797\n",
      "Epoch: 20/20; Batch:334/468; Training loss:0.0955619\n",
      "Epoch: 20/20; Batch:335/468; Training loss:0.0956104\n",
      "Epoch: 20/20; Batch:336/468; Training loss:0.0979762\n",
      "Epoch: 20/20; Batch:337/468; Training loss:0.0953832\n",
      "Epoch: 20/20; Batch:338/468; Training loss:0.100757\n",
      "Epoch: 20/20; Batch:339/468; Training loss:0.0955302\n",
      "Epoch: 20/20; Batch:340/468; Training loss:0.0941399\n",
      "Epoch: 20/20; Batch:341/468; Training loss:0.0953689\n",
      "Epoch: 20/20; Batch:342/468; Training loss:0.0925752\n",
      "Epoch: 20/20; Batch:343/468; Training loss:0.0949457\n",
      "Epoch: 20/20; Batch:344/468; Training loss:0.0993921\n",
      "Epoch: 20/20; Batch:345/468; Training loss:0.0921265\n",
      "Epoch: 20/20; Batch:346/468; Training loss:0.0949232\n",
      "Epoch: 20/20; Batch:347/468; Training loss:0.0981499\n",
      "Epoch: 20/20; Batch:348/468; Training loss:0.0970253\n",
      "Epoch: 20/20; Batch:349/468; Training loss:0.0920434\n",
      "Epoch: 20/20; Batch:350/468; Training loss:0.0954727\n",
      "Epoch: 20/20; Batch:351/468; Training loss:0.0964775\n",
      "Epoch: 20/20; Batch:352/468; Training loss:0.0936544\n",
      "Epoch: 20/20; Batch:353/468; Training loss:0.0937364\n",
      "Epoch: 20/20; Batch:354/468; Training loss:0.0955806\n",
      "Epoch: 20/20; Batch:355/468; Training loss:0.0977645\n",
      "Epoch: 20/20; Batch:356/468; Training loss:0.0920756\n",
      "Epoch: 20/20; Batch:357/468; Training loss:0.0956463\n",
      "Epoch: 20/20; Batch:358/468; Training loss:0.095472\n",
      "Epoch: 20/20; Batch:359/468; Training loss:0.0964472\n",
      "Epoch: 20/20; Batch:360/468; Training loss:0.0934103\n",
      "Epoch: 20/20; Batch:361/468; Training loss:0.0996676\n",
      "Epoch: 20/20; Batch:362/468; Training loss:0.0953125\n",
      "Epoch: 20/20; Batch:363/468; Training loss:0.0916319\n",
      "Epoch: 20/20; Batch:364/468; Training loss:0.0948801\n",
      "Epoch: 20/20; Batch:365/468; Training loss:0.0891557\n",
      "Epoch: 20/20; Batch:366/468; Training loss:0.0947141\n",
      "Epoch: 20/20; Batch:367/468; Training loss:0.0941364\n",
      "Epoch: 20/20; Batch:368/468; Training loss:0.0952146\n",
      "Epoch: 20/20; Batch:369/468; Training loss:0.0961379\n",
      "Epoch: 20/20; Batch:370/468; Training loss:0.0927196\n",
      "Epoch: 20/20; Batch:371/468; Training loss:0.0968453\n",
      "Epoch: 20/20; Batch:372/468; Training loss:0.0925093\n",
      "Epoch: 20/20; Batch:373/468; Training loss:0.0919107\n",
      "Epoch: 20/20; Batch:374/468; Training loss:0.0942973\n",
      "Epoch: 20/20; Batch:375/468; Training loss:0.096846\n",
      "Epoch: 20/20; Batch:376/468; Training loss:0.0950107\n",
      "Epoch: 20/20; Batch:377/468; Training loss:0.0946433\n",
      "Epoch: 20/20; Batch:378/468; Training loss:0.0977399\n",
      "Epoch: 20/20; Batch:379/468; Training loss:0.0922117\n",
      "Epoch: 20/20; Batch:380/468; Training loss:0.0955924\n",
      "Epoch: 20/20; Batch:381/468; Training loss:0.0955893\n",
      "Epoch: 20/20; Batch:382/468; Training loss:0.0931459\n",
      "Epoch: 20/20; Batch:383/468; Training loss:0.101278\n",
      "Epoch: 20/20; Batch:384/468; Training loss:0.0937093\n",
      "Epoch: 20/20; Batch:385/468; Training loss:0.0979848\n",
      "Epoch: 20/20; Batch:386/468; Training loss:0.0992222\n",
      "Epoch: 20/20; Batch:387/468; Training loss:0.0976318\n",
      "Epoch: 20/20; Batch:388/468; Training loss:0.0974229\n",
      "Epoch: 20/20; Batch:389/468; Training loss:0.0977219\n",
      "Epoch: 20/20; Batch:390/468; Training loss:0.0978497\n",
      "Epoch: 20/20; Batch:391/468; Training loss:0.092552\n",
      "Epoch: 20/20; Batch:392/468; Training loss:0.0964794\n",
      "Epoch: 20/20; Batch:393/468; Training loss:0.0949803\n",
      "Epoch: 20/20; Batch:394/468; Training loss:0.0949006\n",
      "Epoch: 20/20; Batch:395/468; Training loss:0.0966357\n",
      "Epoch: 20/20; Batch:396/468; Training loss:0.0969356\n",
      "Epoch: 20/20; Batch:397/468; Training loss:0.0969965\n",
      "Epoch: 20/20; Batch:398/468; Training loss:0.0955264\n",
      "Epoch: 20/20; Batch:399/468; Training loss:0.099337\n",
      "Epoch: 20/20; Batch:400/468; Training loss:0.0925984\n",
      "Epoch: 20/20; Batch:401/468; Training loss:0.0976133\n",
      "Epoch: 20/20; Batch:402/468; Training loss:0.0963541\n",
      "Epoch: 20/20; Batch:403/468; Training loss:0.096408\n",
      "Epoch: 20/20; Batch:404/468; Training loss:0.0986633\n",
      "Epoch: 20/20; Batch:405/468; Training loss:0.0951201\n",
      "Epoch: 20/20; Batch:406/468; Training loss:0.097016\n",
      "Epoch: 20/20; Batch:407/468; Training loss:0.0973109\n",
      "Epoch: 20/20; Batch:408/468; Training loss:0.0975656\n",
      "Epoch: 20/20; Batch:409/468; Training loss:0.0973578\n",
      "Epoch: 20/20; Batch:410/468; Training loss:0.0930224\n",
      "Epoch: 20/20; Batch:411/468; Training loss:0.0983535\n",
      "Epoch: 20/20; Batch:412/468; Training loss:0.0949029\n",
      "Epoch: 20/20; Batch:413/468; Training loss:0.0962533\n",
      "Epoch: 20/20; Batch:414/468; Training loss:0.0952611\n",
      "Epoch: 20/20; Batch:415/468; Training loss:0.0986702\n",
      "Epoch: 20/20; Batch:416/468; Training loss:0.0983101\n",
      "Epoch: 20/20; Batch:417/468; Training loss:0.0955068\n",
      "Epoch: 20/20; Batch:418/468; Training loss:0.0969282\n",
      "Epoch: 20/20; Batch:419/468; Training loss:0.0964615\n",
      "Epoch: 20/20; Batch:420/468; Training loss:0.0962472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20; Batch:421/468; Training loss:0.0985662\n",
      "Epoch: 20/20; Batch:422/468; Training loss:0.0935474\n",
      "Epoch: 20/20; Batch:423/468; Training loss:0.0972575\n",
      "Epoch: 20/20; Batch:424/468; Training loss:0.0961083\n",
      "Epoch: 20/20; Batch:425/468; Training loss:0.0999377\n",
      "Epoch: 20/20; Batch:426/468; Training loss:0.094645\n",
      "Epoch: 20/20; Batch:427/468; Training loss:0.0931635\n",
      "Epoch: 20/20; Batch:428/468; Training loss:0.0963347\n",
      "Epoch: 20/20; Batch:429/468; Training loss:0.0974406\n",
      "Epoch: 20/20; Batch:430/468; Training loss:0.0940084\n",
      "Epoch: 20/20; Batch:431/468; Training loss:0.0996806\n",
      "Epoch: 20/20; Batch:432/468; Training loss:0.0929649\n",
      "Epoch: 20/20; Batch:433/468; Training loss:0.0956433\n",
      "Epoch: 20/20; Batch:434/468; Training loss:0.0944869\n",
      "Epoch: 20/20; Batch:435/468; Training loss:0.093383\n",
      "Epoch: 20/20; Batch:436/468; Training loss:0.0970212\n",
      "Epoch: 20/20; Batch:437/468; Training loss:0.0986034\n",
      "Epoch: 20/20; Batch:438/468; Training loss:0.0979093\n",
      "Epoch: 20/20; Batch:439/468; Training loss:0.0963924\n",
      "Epoch: 20/20; Batch:440/468; Training loss:0.0944891\n",
      "Epoch: 20/20; Batch:441/468; Training loss:0.0980334\n",
      "Epoch: 20/20; Batch:442/468; Training loss:0.0975162\n",
      "Epoch: 20/20; Batch:443/468; Training loss:0.0956246\n",
      "Epoch: 20/20; Batch:444/468; Training loss:0.0962106\n",
      "Epoch: 20/20; Batch:445/468; Training loss:0.0984408\n",
      "Epoch: 20/20; Batch:446/468; Training loss:0.0937386\n",
      "Epoch: 20/20; Batch:447/468; Training loss:0.0959096\n",
      "Epoch: 20/20; Batch:448/468; Training loss:0.0952739\n",
      "Epoch: 20/20; Batch:449/468; Training loss:0.100269\n",
      "Epoch: 20/20; Batch:450/468; Training loss:0.0979301\n",
      "Epoch: 20/20; Batch:451/468; Training loss:0.096745\n",
      "Epoch: 20/20; Batch:452/468; Training loss:0.0962111\n",
      "Epoch: 20/20; Batch:453/468; Training loss:0.0962306\n",
      "Epoch: 20/20; Batch:454/468; Training loss:0.0951712\n",
      "Epoch: 20/20; Batch:455/468; Training loss:0.0983147\n",
      "Epoch: 20/20; Batch:456/468; Training loss:0.0959217\n",
      "Epoch: 20/20; Batch:457/468; Training loss:0.0943883\n",
      "Epoch: 20/20; Batch:458/468; Training loss:0.0903596\n",
      "Epoch: 20/20; Batch:459/468; Training loss:0.0969186\n",
      "Epoch: 20/20; Batch:460/468; Training loss:0.0942433\n",
      "Epoch: 20/20; Batch:461/468; Training loss:0.0925774\n",
      "Epoch: 20/20; Batch:462/468; Training loss:0.0950187\n",
      "Epoch: 20/20; Batch:463/468; Training loss:0.0981477\n",
      "Epoch: 20/20; Batch:464/468; Training loss:0.0928545\n",
      "Epoch: 20/20; Batch:465/468; Training loss:0.0952794\n",
      "Epoch: 20/20; Batch:466/468; Training loss:0.0937867\n",
      "Epoch: 20/20; Batch:467/468; Training loss:0.0965076\n",
      "Epoch: 20/20; Batch:468/468; Training loss:0.0958598\n"
     ]
    }
   ],
   "source": [
    "noise_factor = 0.5\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "for ep in range(epochs):\n",
    "    for batch in range(total_batch):\n",
    "        batch_xs, _ = mnist.train.next_batch(batch_size=batch_size)\n",
    "        batch_xs = batch_xs.reshape((-1, 28, 28, 1))\n",
    "        \n",
    "        # 加入噪声\n",
    "        noisy_imgs = batch_xs + noise_factor * np.random.randn(*batch_xs.shape)\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        batch_cost, _ = sess.run([cost, optimizer], \n",
    "                                 feed_dict={inputs_:noisy_imgs,\n",
    "                                            targets_: batch_xs})\n",
    "        \n",
    "        print('Epoch: %s/%s; Batch:%s/%s; Training loss:%s'%(ep+1, epochs, batch+1, total_batch, batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABawAAAEsCAYAAAAvofT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAQFrghjUAAAAAAAAAIC38D3b3nfYuHqmAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1bd2fe2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "imgs = mnist.test.images[10:20]\n",
    "noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n",
    "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "\n",
    "reconstructed = sess.run(outputs_,\n",
    "                         feed_dict={inputs_:noisy_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "for images, row in zip([imgs, noisy_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28,28)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
